\documentclass[charterfonts,lotsofwhite]{patmorin}
\usepackage{graphicx}
\usepackage{amsopn}
\input{pat}

\newcommand{\defequals}{\stackrel{\mathrm{def}}{=}}
\newcommand{\boundary}{\partial}
\DeclareMathOperator{\depth}{depth}

\title{\MakeUppercase{Distribution-Sensitive Planar Point Location}}
\author{ULB Algorithms \and John Iacono \and Pat Morin}
\date{}

\begin{document}
\maketitle

\section{Introduction}

The planar point location problem is one of the first problems to have
been considered by the field of computational geometry. Given a planar
straight line graph $G$, the planar point location problems asks us to
construct a data structure so that, for any query point $p$, we can
quickly determine which face of $G$ contains $p$.  The history of the
planar point location problem parallels, in many ways, the study of
binary search trees.

After a few initial attempts \cite{dl76,lp77,p81}, asymptotically
optimal (and quite different) linear-space $O(\log n)$ query time
solutions to the planar point location problem were obtained by
Kirkpatrick \cite{28}, Sarnak and Tarjan \cite{46}, Edelsbrunner
\etal\ \cite{24} in the mid 1980s.  An elegant, and optimal,
randomized solution was later given by Mulmuley \cite{34}.  Preparata
\cite{p} gives a comprehensive survey of the results of this era.

In the 1990s, several authors became interested in determining the
exact constant achievable in the query time.  Goodrich \etal\
\cite{26} gave an $O(n^2)$ size data structure that, for any query,
required the evaluation of at most $2\log n$ point-line
comparisons.\footnote{Here and throughout, logarithms are implicitly
taken modulo 2 unless otherwise specified.} Later, Adamy and Seidel
\cite{1} gave a linear space data structure that answers queries using
$\log n + 2\sqrt{\log n} + O(\log\log n)$ point-line comparisons and
showed that this result is optimal up to the third term.

Still not done with the problem, several authors considered the point
location problem under various assumptions about the query
distribution.  All these solutions compare the expected query time to
\emph{entropy bound};  if the query point $p$ is chosen randomly such
that $p_i$ is the probability that $p$ is contained in face $i$ of
$G$, then no algorithm that makes only binary decisions can answer
queries using fewer than 
\[
    H = \sum_i p_i\log(1/p_i)
\]
decisions.

At this point, the problem becomes more complicated and the results
become more specific.  Arya \etal\ \cite{XXX} showed that, for planar
graphs whose faces are convex polygons, and for distributions where
$x$ and $y$ coordinates are drawn independently, a data structure
exists in which the expected number of point-line comparisons is at
most $4H+O(1)$ using linear space or at most $2H+O(1)$ using quadratic
space.

Iacono \cite{iXX} showed that, for triangulations, a simple variant of
Kirkpatrick's original point location structure gives a linear space,
$O(H+1)$ query time.  Another result by Arya \etal\ \cite{ammXX} gives
a data structure for planar graphs whose faces have constant
complexity (e.g., triangulations) that uses $H + O(H^{2/3}+1)$
expected number of comparisons per query and $O(n\log n)$ space.   The
same authors \cite{ammXX} gave a data structure that, for planar
graphs whose faces have constant complexity, uses $O(n\log^* n)$ space
and for which the expected number of comparisons is $H+O(H^{2/3}+1)$.

The same three authors \cite{ammXX} also showed that a variant of
Mulmuley's randomized algorithm gives, for planar graphs whose faces
have constant complexity, a simple $O(H+1)$ query time, linear space
data structure. 

In this paper we consider, yet again, distribution-sensitive point
location. However, we begin our investigation with one of the most
basic versions of the problem:  Testing if a query point is contained
in a convex polygon. The original, $O(\log n)$, time solution to this
problem is folklore.

\section{Preliminaries}

For a set $X$, itself containing sets, we denote the union of its
elements by $\cup X=\bigcup_{x\in X} x$.

For a distribution $D$ and an event $X$, we denote by $D_{|X}$ the
distribution $D$ conditioned on $X$.  That is, the distribution where 
the probability of an even $Y$ is
$\Pr(Y|X)=\Pr(Y\cap X)/\Pr(X)$.

A \emph{classification problem} over a domain $\mathcal{D}$ is a function
$\mathcal{P}:\mathcal{D}\mapsto \{0,\ldots,k-1\}$.  The special case
in which $k=2$ is called a \emph{decision problem}.

A \emph{classification tree} is a full binary tree\footnote{A full
binary tree is a rooted ordered binary tree in which each non-leaf
node has exactly two children.} in which each internal node $v$ is
labelled with a predicate $P_v(\cdot)$ and for which each leaf $\ell$
is labelled with a value $d(\ell)\in\{0,\ldots,k-1\}$. The
\emph{search path} of an input $p$ in a comparison tree $T$ starts at
the root of $T$ and, at each internal node $v$, proceeds to the left
child of $v$ if $P_v(p)$ is false and proceeds to the right child of
$v$ otherwise.  We denote by $T(p)$ the label of the final (leaf) node
in the search path for $p$.  We say that the classification tree $T$
\emph{solves} the classification problem $\mathcal{P}$ over the domain
$\mathcal{D}$ if, for every $p\in \mathcal{D}$, $\mathcal{P}(p)=T(p)$.

When there is a probability measure $D$ over $\mathcal{D}$, we define
the \emph{expected search time} of a classification tree $T$ as the
expected length of the search path for $p$ when $p$ is drawn at random
from $\mathcal{D}$ according to the distribution $D$.  Note that, for
each leaf $\ell$ of $T$ there is a subset of $r(\ell)\subseteq
\mathcal{D}$ such that the search path for any $p\in r(\ell)$ ends at
$\ell$.  Thus, the expected search time of $T$ can be written as
\[
     \mu_D(T) = \sum_{\ell\in L(T)} \Pr(r(\ell))\times \depth(\ell)
	\enspace ,
\]
where $L(T)$ denotes the leaves of $T$ and $\depth(\ell)$ denotes the
length of the path from the root of $T$ to $\ell$.

The folloing theorem, which is a restatement of (part of) Shannon's
coding Theorem \cite{sXX}, is what all existing results use to
establish their optimality:

\begin{thm}\thmlabel{shannon}
Let $\mathcal{P}:D\mapsto \{0,\ldots,k-1\}$ be a classification
problem and let $p\in \mathcal{D}$ be selected from a distibution $D$ such
that $\Pr\{\mathcal{P}(x)= i\}=p_i$, for $0\le i< k$.  Then, any
classification tree $T$ that solves $\mathcal{P}$ has
\begin{equation}
     \mu_D(T) \ge \sum_{i=0}^{k-1} p_i\log(1/p_i) \enspace .
	\eqlabel{shannon}
\end{equation}
\end{thm}

\thmref{shannon} can provide very powerful lower bounds since it makes
no assumptions about the predicates used in the nodes of the decision
tree $T$.  However, it is useless for decision problems since the
right hand side of \eqref{shannon} is never greater than 1 in this
case.  For instance, in all existing results on distribution-sensitive
planar point location, the set $\{0,\ldots,k-1\}$ are the set of faces
in the subdivision.  However, for one of the simplest planar point
location problems, namely testing whether a point is contained in a
convex polygon, there are only two faces, so \thmref{shannon} can not
give a lower bound greater than 1 regardless of the input distribution
or the number of vertices of the polygon.  However, we know 
that testing if a point is contained in a convex $n$-gon has an
$\Omega(\log n)$ lower bound in fairly strong models of computation,
including algebraic decision trees \cite{bo}.

\section{Collections of Intervals}

Before considering point-in-polygon testing we warm up by studying a
related one-dimensional problem on a set of intervals.  Our motivation
for studying this problem is that it gives intuition into how to
characterize the complexity of point-in-polygon testing under random
distributions.

Let $I=\{[a_i,b_i]: i=1,\ldots,n\}$ be a set of $n$ disjoint closed
real intervals, with $b_i < a_{i+1}$ for all $1\le i\le n$.  We are
interested in the problem of constructing a decision tree for $I$ that
can answer the decision problem: Given $p\in\R$, is $p\in \cup I$?
The type of decision trees we consider are \emph{comparison trees}.
These are decision trees in which each internal node $v$ is labelled
with a comparison of the form $P_v(x) = [x > n(v)]$ where $n(v)$ is
some real number called the \emph{label} of $v$.\footnote{Here and
throughout, we use Iverson's notation \cite{kXX} where, for a
predicate $X$, $[X]$ evaluates to 1 if $X$ is true and 0 otherwise.}
In particular, we are interested in the case when the query point $p$
is drawn according to some probability distribution $D$.   Throughout
the remainder of this section, all references to probabilities are
implicitly with reference to $D$ and the query domain is the support
of $D$.  We call such a decision tree a membership tester for $(I,D)$.

\comment{
To study the expected search time of comparison trees for this problem
we need to define a slightly different set of intervals than $I$.  Let
$J$ be the set of intervals obtained after the following two
operations:

\begin{enumerate}

\item Remove from $I$ all intervals $\ell$ such that $\Pr(\ell) = 0$.

\item Repeat the following operation as many times as possible:  If
$\Pr([b_i,a_{i+1}])=0$ then replace the two intervals $(a_i,b_i)$ and
$(a_{i+1},b_{i+1})$ with the single interval $(a_i,b_{i+1})$
\end{enumerate}
}

\begin{lem}\lemlabel{intervals}
Let $\overline{I}$ be the set of maximal open intervals in
$\R\setminus\cup I$ and let $S=I\cup \overline{I}$.  Then,
for any decision tree that tests if a point $p$ drawn from $D$
is in $I$,
\[
    \mu_D(T) \ge H(I,D) \defequals 
	\sum_{x\in S} \Pr(x)\log(1/\Pr(x)) \enspace .
\]
\end{lem}

\begin{proof}
Consider two distinct intervals $i,j\in S$ and suppose $p\in i$ and
$q\in j$.  Note that the search paths for $p$ and $q$ must finish in
different leaves of $T$ since, otherwise, there is some point
$r\in[p,q]$ that is incorrectly classified by $T$, i.e., such
that $T(p)\neq [p\in\cup I]$.  Now, for every interval $i\in S$,
select a point $p_i\in i$ such that the length of the search path for
$p$ is minimum.   Using the standard encoding of paths in a binary
search tree, this gives a code for the intervals of $S$ where the
length of the codeword for $i$ is the depth of $p_i$ in $T$.  In this
way we obtain

\[
   \mu_D(T)
    = \sum_{\ell\in L(T)} \Pr(\ell)\depth(\ell)
   \ge \sum_{i \in S} \Pr(i)\depth(p_i)
   \ge H(I,D)
   \enspace , 
\]
where the second inequality follows from Shannon's source coding
theorem \cite{X}.
\end{proof}

Note that, to prove the above result, we required some assumptions
about the predicates used at the nodes of $T$, namely that they are
comparisons.  Without some such assumption a lower bound that holds
for every set $I$ of intervals is impossible to prove.\footnote{For
example, if the intervals are defined by $a_i=2(i-1)$ and
$b_i=2(i-1)+1$ then the predicate $P(x)=[(x\ge 0)\wedge(x\le
2n)\wedge((\floor{x}\bmod 2=0)\vee(x=\floor{x}))]$ determines if $x\in
S$ in constant time.} This is in stark contrast to the very closely
related classification problem: Given a partitioning of $R$ into a set
$S$ of $n$ intervals, construct a classification tree whose output is
$i$ if its input point is contained in the $i$th interval.  For the
latter problem, \thmref{shannon} implies that any decision tree has an
expected search time at least equal to the entropy of the distribution
of the queries over the intervals. 

Though not the focus of the current paper, it is worth noting that the
lower bound in \lemref{intervals} can be matched to within an additive
constant by storing the intervals of $I$ and $\overline{I}$ at the
leaves of a biased binary search tree \cite{kXX,mXX}.  In particular,
for every $I$ and $D$ there exists a membership tester for $(I,D)$
whose expected search time is $H(I,D)+O(1)$.

\section{Point In Convex Polygon Testing}

Let $P$ be a convex $n$-gon whose boundary is denoted by $\partial P$
and consider a probability measure $D$ over $\R^2$.  We use the
convention that $P$ contains it boundary.  In this section we are
interested in preprocessing $P$ and $D$ so that we can quickly solve the
decision problem of testing whether a query point $p$, drawn according
to $D$, is contained in $P$. 

In particular, we are interested in algorithms that can be described
as \emph{linear decision trees}.  These are decision trees such that
each internal node $v$ contains a linear inequality $P_v(x,y)=[ax+by
\ge c]$.  Again, we require that, for every $p$ in the support of $D$,
the leaf reached in the search path for $p$ is labelled with a 1 if
and only if $p\in P$.  Geometrically, each internal node of $T$ is
labelled with a directed line and the decision to go to the left or
right child depends on whether p is to the left or right (or on) this
line.  

Our exposition is broken up into three sections.  We begin by
describing a data structure (in fact, a decision tree) that solves
this problem.  Next, we give an (easy) analysis of the expected search
time of this data structure.  Finally, we give a (more difficult)
proof that this expected search time is optimal.


\subsection{Triangle Trees}

We begin by describing a data structure that works by creating a
sequence of successively finer approximations $A_0,\ldots,A_k$ to
$\boundary P$.  Each approximation $A_i$ consists of two convex
polygons; an \emph{outer approximation} that contains $P$ and an
\emph{inner approximation} that is contained in $P$.

Each approximation $A_i$ is completely defined by a set $S_i$ of
$O(2^i)$ points from $\boundary P$.  The inner approximation is simply
the convex hull of $S_i$.  The outer approximation has an edge tangent
to $P$ at each of the points of $S_i$.  More precisely, for each point
$x\in S_i$ there is an edge $e$ of the outer approximation that
contains $x$.  If $x$ is in the interior of an edge of $P$ then $e$ is
contained in the same line that contains that edge. Otherwise ($x$ is
a vertex of $P$) $e$ is supported by the line containing the edge
incident to $x$ that precedes it in counterclockwise order.  We ensure
that successive approximations have a containment relationship, i.e.,
$A_i\supseteq A_{i+1}$, by choosing our boundary points so that
$S_i\subseteq S_{i+1}$.

\notice{say something about degenerate triangles.} Next we define the
sets $S_0,\ldots,S_k$ that define our approximations.  The set $S_0$
is empty, and we use the convention that the outer approximation in
this case is the entire plane and the inner approximation is the empty
set. The set $S_1$ consists of two points, $x$ and $y$ on $\boundary
P$ such that $\Pr(h_1)\le 1/2$ and $\Pr(h_2)\le 1/2$, for each of the
two open halfspaces $h_1$ and $h_2$ bounded by the line containing $x$
and $y$.

Next, we show how to obtain the set $S_i$ from the set $S_{i+1}$.  Let
$p_0,\ldots,p_{m_i-1}$ be the points in $S_i$ as they occur in
conterclockwise order around the boundary of $P$.  The approximation
$A_i$ thus consists of $m$ open triangles
$\Delta_0,\ldots,\Delta_{m-1}$ where $\Delta_j$ has one side joining
$p_j$ and $p_{j+1}$ and a third vertex $t_j$ that lies at the
intersection of two lines tangent to $P$ at $p_j$ and
$p_{j+1}$.\footnote{Throughout this discussion, subscripts are
implicitly taken modulo $k$.}  Refer to \figref{split}. For each
triangle $\Delta_j$ that has positive area we add a new point to
$S_{i+1}$  by partitioning $\Delta_j$ into two open triangles
$\Delta_{j,0}$ and $\Delta_{j,1}$ by a line $\ell$ through $t_j$ and
such that 
\[  
     \Pr(\Delta_{j,b}) \le \Pr(\Delta_{j})/2 \enspace ,
\]
for $b=0,1$.
We then select a new point to add to $S_{i+1}$ at the intersection of
$\ell$ and $\boundary P$ that occurs in $\Delta_j$.

\begin{figure}
\begin{center}
\begin{tabular}{cc}
\includegraphics{split-a} & \includegraphics{split-b}
\end{tabular}
\end{center}
\caption{Splitting the triangle $\Delta_j$ into two triangles
$\Delta_{j,0}$ and $\Delta_{j,1}$ with a line through $t_j$.}
\figlabel{split}
\end{figure}

The entire process terminates at the first value of $k$ for which all
triangles have 0 area.  The approximations $A_0,\ldots,A_k$ are stored
as a binary tree $T$ that we call a \emph{triangle tree}.  Each node
$v$ of $T$ at level $i$ in $T$ corresponds to an open triangle
$\Delta(v)$ in approximation $A_{i}$ and the two children of $v$
correspond to the two triangles into which $\Delta(v)$ is split.  The
leaves of $T$ correspond to 0 area triangles that are actually line
segments in $\boundary P$. 

To use the tree $T$ to determine if a point $p$ is contained in $P$ we
proceed top-down, starting at the root.  For a point $p$ contained in
$\Delta(v)$ one of two things can happen: (1) $p$ is contained in one
of the two open triangles $\Delta(v_r)$ or $\Delta(v_\ell)$ where
$v_r$ and $v_\ell$ are the right and left child of $v$, respectively,
in which which we recurse on the right or left child of $v$,
respectively, or (2) otherwise we can determine in constant time if
$p\in P$.

\subsection{Analysis of the Triangle Tree}

Let $T$ be the triangle tree for a convex $n$-gon $P$ and a
distribution $D$.  For each internal node $v$ of $T$, define
$\Pr(v)=\Pr(\Delta(v))-\Pr(\Delta(v_r))-\Pr(\Delta(v_\ell))$ where
$v_\ell$ and $v_r$ are the two children of $v$ in $T$.  Notice that
$\Pr(v)$ is the probability that a search terminates at node $v$.

\begin{thm}\lemlabel{upper-bound}
Using the triangle tree $T$, the expected number of linear inequalities
required to check if $p\in P$ for a point $p$ drawn from $D$ is at
most
\[
     \mu_D(T) = O(1)\times \sum_{v\in T}\Pr(v)\times\depth (v)
      \le O(1)\times \sum_{v\in T}\Pr(v)\log(1/\Pr(v))
\]
\end{thm}

\begin{proof}
The first equality is simply by the definition of expectation.  The
second inequality follows immediately from the fact that, for any node
$v\in T$ whose parent is $u$, $\Pr(v)\le \Pr(\Delta(v))\le (1/2)\Pr(\Delta(u))$.
\end{proof}


\subsection{Optimality of the Triangle Tree}

Next we will show that the performance bound given by
\lemref{upper-bound} is optimal.  That is, that there is no linear
decision tree whose expected search time (on distribution $D$) is
asymptotically better than that of the triangle tree.
The key ingredient in our argument is the following lemma:

\begin{lem}\lemlabel{lower-bound}
Let $V$ be a subset of the vertices of the triangle tree $T$ such that
no vertex in $V$ is the descendent of any other vertex in $V$.  Let
$R=\bigcup_{v\in V} \Delta(v)$ and let $D_{|R}$ denote the
distribution $D$ conditioned on $R$.  Then, for any linear decision
tree $T'$,
\[
    \mu_{D_{|R}}(T) 
	\ge \sum_{v\in V}\Pr(\Delta(v)|R)\log(1/\Pr(\Delta(v)|R) \enspace .
\]
\end{lem}

\begin{proof}
Easy.
\end{proof}

The remainder of our argument involves constructing a lower-bound
based on \lemref{lower-bound} that matches the upper bound in
\thmref{upper-bound}.  Let 
\[
   H = \sum_{v\in V(T)} \Pr(v)\log (1/\Pr(v))
\] 
be the entropy of the distribution of search paths in $T$.  Note that
the upper bound of \thmref{upper-bound} is within a constant factor of
$H$. Thus, our goal is to show that no linear decision tree has an
expected search time in $o(H)$.

We start our analysis by partitioning the internal nodes of $T$ in
\emph{groups} $G_1,G_2,\ldots$ where
\[
	G_i = \{v\in T : 1/2^{i} \le \Pr(\Delta(v)) < 1/2^i \} \enspace .
\]
In what follows, we fix some constant $0< \alpha < 1$.  Our first
result shows that, in our lower bound, we can discard a fairly large
number of elements from each group without having much effect on the
overall entropy:

\begin{lem}\lemlabel{garbage}
For each $i$, let $G_i'$ be obtained by deleting at most $2^{\alpha
i}$ elements.  Then
\[
    \sum_{i=1}^\infty \sum_{v\in G_i'} \Pr(v)\log(1/\Pr(v)) \ge H-O(1)
	\enspace .
\]
\end{lem}

\begin{proof}
\begin{eqnarray*}
   H & = & \sum_{j=1}^n p_j\log(1/p_j) \\
   & = & \sum_{i=1}^{\infty}\sum_{v\in G_i'} \Pr(v)\log (1/\Pr(v)) +
         \sum_{i=1}^{\infty}\sum_{v\in G_i\setminus G_i'} \Pr(v)\log (1/\Pr(v)) \\
   & \le & \sum_{i=1}^{\infty}\sum_{v\in G_i'} \Pr(v)\log (1/\Pr(v)) +
         \sum_{i=1}^{\infty}\sum_{v\in G_i\setminus G_i'} (1/2^{i-1})\log (2^i) \\
   & \le & \sum_{i=1}^{\infty}\sum_{v\in G_i'} \Pr(v)\log (1/\Pr(v)) +
         \sum_{i=1}^{\infty}\sum_{v\in G_i\setminus G_i'} 2i(1/2^{i}) \\
   & \le & \sum_{i=1}^{\infty}\sum_{v\in G_i'} \Pr(v)\log (1/\Pr(v)) +
         \sum_{i=1}^{\infty} 2i(1/2^{i-\alpha}) \\
   & \le & \sum_{i=1}^{\infty}\sum_{v\in G_i'} \Pr(v)\log (1/\Pr(v)) + O(1)
\end{eqnarray*}
\end{proof}

In order to use \lemref{lower-bound} we must partition the vertices of
$T$ into sets that are compatible with the conditions of the lemma.

\begin{lem}\lemlabel{partition}
Each group $G_i$ can be partitioned into $t_i$ subgroups
$G_{i,1},\ldots,G_{i,t_i}$ such that all but one subgroup, $G_{i,t_i}$
contains at least $2^{\alpha i}/i$ elements and within any subgroup
$G_{i,j}$ with $j < t_i$ there is no pair of nodes $u,v\in G_{i,j}$
with $u$ an ancestor of $v$ in $T$.
\end{lem}

\begin{proof}
Assume that $|G_i|\ge 2^{\alpha i}$, otherwise there is nothing to
prove.  Observe that all vertices in $G_i$ appear within the first $i$
levels of $T$.  Thus, any node in $G_i$ has at most $i-1$ ancestors in
$T$.  

We can obtain the first group $G_{i,1}$ by first defining all nodes of
$G_i$ to be \emph{unmarked} and \emph{unselected}.  To obtain
$G_{i,1}$ we repeatedly \emph{select} any unselected and unmarked
node $v\in G_i$ that does not have any descendants in $G_i$ and
\emph{mark} the (at most $i-1$) ancestors of $v$ in $T$.  This
process selects at least
\[
   |G_i|/i \ge 2^{\alpha i}/i
\] 
elements to take part in $G_{i,1}$.  We can then apply this process
recursively on $G_i\setminus G_{i,1}$ to obtain the sets
$G_{i,2},\ldots,G_{i,t_i}$.
\end{proof}


\begin{thm}
Any linear decision tree $T'$ that determines whether any query point 
$p\in\R^2$ is contained in $P$ has
\[
   \mu_D(T') = \Omega(H) - O(1) \enspace .
\]
\end{thm}

\begin{proof}
Our proof is an application of the little-birdie principle.  Partition
the nodes of $T$ into the sets $G_{i,j}$ as described in
\lemref{partition}.  We work in a slightly stronger model of
computation in which we are given the tree $T$ and the partitioning of
the vertices of $T$ into the sets $G_{i,j}$ and which allows a whole
collection of decision trees $T_{i,j}$, one for each group $G_{i,j}$.
Now, when the point $p$ is selected according to $D$, a little birdie
indicates to the computer which group $G_{i,j}$ contains the vertex
$v$ of $T$ at which a search for $p$ would end.  The computer then
uses this to select the decision tree $T_{i,j}$ and uses it to
determine if $p\in P$.  The cost of this is the cost of searching in
$T_{i,j}$. \marginpar{Is assuming that $T_{i,j}$ is correct for
all inputs in $\R^2$ a problem?} Thus, the expected cost of a search 
in this model of computation is
\[
     \mu = \sum_{i=1}^\infty \sum_{j=1}^{t_i}
	\Pr\{G_{i,j}\}\mu_{D_{i,j}}(T_{i,j}) \enspace ,
\]
where $\Pr\{G_{i,j}\}$ denotes the probability of a search, in $T$,
ending at node $v\in G_{i,j}$ and $D_{i,j}$ denotes the distribution
$D$ conditioned on this event.  Clearly this model of computation is
at least as strong as the linear decision tree model since, in this
model, there is always the option of ignoring the birdie's advice by
creating a single linear decision tree $T'$ and setting $T_{i,j}=T'$
for all $i$ and $j$.

Now, applying \lemref{lower-bound} to each group $G_{i,j}$ we obtain
\begin{eqnarray*}
\mu & \ge & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i}\Pr\{G_{i,j}\}\times
	\sum_{v\in G_{i,j}}\Pr(v\mid G_{i,j})\log(1/\Pr(v\mid G_{i,j})) \\
& \ge & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i-1}\Pr\{G_{i,j}\}\times
	\sum_{v\in G_{i,j}}\Pr(v\mid G_{i,j})\log(1/\Pr(v\mid G_{i,j})) \\
& = & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i-1}
	\sum_{v\in G_{i,j}}\Pr(v)\log(\Pr\{G_{i,j}\}/\Pr(v)) \\
& \ge & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i-1}
	\sum_{v\in G_{i,j}}\Pr(v)(\log(1/\Pr\{v\})+ \log(\Pr\{G_{i,j}\}) \\
& \ge & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i-1}
	\sum_{v\in G_{i,j}}\Pr(v)(\log(2^{i-1}) + \log(2^{\alpha i}/i2^{i}) \\
& \ge & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i-1}
	\sum_{v\in G_{i,j}}\Pr(v)(i-1 + \alpha i -i -2\log i) \\
& \ge & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i-1}
	\sum_{v\in G_{i,j}}\Pr(v)\times \Omega(i) \\
& \ge & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i-1}
	\sum_{v\in G_{i,j}}\Omega(\Pr(v)\log (1/\Pr(v)) \\
& \ge & \Omega(H) -O(1)
\end{eqnarray*}
\end{proof}

\section{Point Location}




\end{document}
