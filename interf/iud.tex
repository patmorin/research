\documentclass{patmorin}
%\usepackage{charter}
\usepackage{textcase}
\usepackage{amsmath,amsthm}
\usepackage{pat}

\newcommand{\mst}{\mathit{MST}}

\title{\MakeTextUppercase{A Note on Interference in Random Point Sets}}
\author{BIRS 12w5004}

\begin{document}
\maketitle

\begin{abstract}
  We show that, in any constant dimension $d$, given a set of $n$
  i.u.d. points in the $d$ dimensional cube, it is possible, with high
  probability, to construct a connected graph on these points whose
  maximum interference is $O((\log n)^{1/3})$.
\end{abstract}

\section{Introduction}


Let $V=\{x_1,\ldots,x_n\}$ be a set of $n$ points in $\R^d$ and let
$G=(V,E)$ be a graph with vertex set $V$.  The graph $G$ defines a set,
$B(G)$, of closed balls $B_1,\ldots,B_n$, where $B_i$ has center $x_i$
and radius
\[
   r_i = \max\{\|x_ix_j\| : j\in\{1,\ldots,n\}\} \enspace .
\]
In words, $B_i$ is just large enough to enclose all of $x_i$'s neighbours
in $G$.  We define the \emph{interference} at a point, $p$, as the
number of
these balls that contain $p$, i.e.,
\[
    I(p,G) = |\{B\in B(G) : p\in B\}| \enspace .
\]
We define the \emph{(maximum receiver-centric) interference} of $G$ as the
maximum inteference at any vertex of $G$, i.e.,
\[
   I(G) = \max\{I(x,G) : x\in V\} \enspace .
\]
One of the goals of network design is to build, given $V$, a connected
graph $G=(V,E)$ such that $I(G)$ is minimized.  Thus, it is natural to consider the interference as a property of a given point set, $V$, defined as
\[
  I(V) = \min\{I(G) : \mbox{$G=(V,E)$ is connected}\} \enspace .
\]
The purpose of current paper is to prove the following results:
\begin{thm}\thmlabel{main}
  Let $V$ be a set of $n$ points independently and uniformly distributed
  in $[0,1]^d$.  Then, with high probability, $I(V)\in O((\log n)^{1/3})$.
\end{thm}

\begin{thm}\thmlabel{lower-bound}
  Let $V$ be a set of $n$ points independently and uniformly distributed
  in $[0,1]^d$.  Then, with high probability, $I(\mst(V))\in\Omega((\log
  n)^{1/2})$ and $I(V)\in \Omega((\log n)^{1/4})$.
\end{thm}



\subsection{Related Work}

In this section, we survey previous work on the problem of bounding
the interference of worst-case and random point sets.  A summary of the
results described in this section is given in \figref{related}.  In the statements of all results in this section, $|V|=n$.

\begin{figure}
\begin{center}
  \begin{tabular}{|l|l|r@{, }l|}\hline
    Ref. & Dimension & \multicolumn{2}{c|}{Statement} \\ \hline
    \cite{vR05} & $d=1$ & for all $V$ & $I(V)\in O(n^{1/2})$ \\
    \cite{vR05} & $d\ge 1$ & there exists $V$ & $I(V)\in \Omega(n^{1/2})$ \\
    \cite{ht08} & $d=2$ & for all $V$ & $I(V)\in O(n^{1/2})$ \\
    \cite{ht08} & $d\ge 3$ & for all $V$ & $I(V)\in O((n\log n)^{1/2})$ \\
    \cite{kkmns10} & $d= 1$ & for $V$ i.u.d. in $[0,1]$ & $I(\mst(V))\in \Theta((\log n)^{1/2})$ w.h.p. \\
    \cite{kkmns10,vR05} & $d = 1$ & for $V$ i.u.d. in $[0,1]$  & $I(V)\in\Omega((\log n)^{1/4})$ w.h.p.  \\
    \cite{kdh11} & $d\ge 2$ & for $V$ i.u.d. in $[0,1]^d$ & $I(\mst(V))\in O(\log n)$ w.h.p.  \\
    Here & $d\ge 1$ & for $V$ i.u.d. in $[0,1]^d$  & $I(V)\in O((\log n)^{1/3})$ w.h.p.  \\ \hline
  \end{tabular}
\end{center}
\caption{Previous and new results on interference in geometric networks.}
\figlabel{related}
\end{figure}

The definition of interference used in this paper was introduced by
von~Rickenbach \etal\ \cite{vR05} who proved upper and
lower bounds on the interference of one dimensional point sets:
\begin{thm}\thmlabel{sqrtnlower}
For any integer $d\ge 1$, there exists $V\subset\R^{d}$, such
that $I(V)=\Omega(n^{1/2})$.
\end{thm}
The point set, $V$, in this lower-bound consists of any sequence of
points $x_1,\ldots,x_n$, all on a line, such that $\|x_{i+1}x_i\| \le (1/2)\|x_{i}x_{i-1}\|$,
for all $i\in\{2,\ldots,n-1\}$.  That is, the gaps between consecutive
points decrease exponentially.

This lower bound is matched by an upper-bound:
\begin{thm}\thmlabel{twod-upper}
For all $V\subset\R$, $I(V)\in O(n^{1/2})$.
\end{thm}
The upper bound in \thmref{twod-upper} is obtained by selecting $n^{1/2}$
vertices to act as \emph{hubs}, connecting the hubs into any connected
network and then having each of the remaining nodes connect to its
nearest hub.  This idea was extended to two and higher dimensions
by Halld\'orsson and Tokuyama \cite{ht08}, by using a special type of
$(n^{-1/2})$-net as the set of hubs:
\begin{thm}\thmlabel{sqrtn2d}
For all $V\subset\R^2$, $I(V)\in O(n^{1/2})$.  
\end{thm}
\begin{thm}\thmlabel{sqrtndd}
  For any constant $d\ge 3$, and for all $V\subset\R^d$, $I(V)=O((n\log
  n)^{1/2})$.
\end{thm}

Several authors have shown that the interference of a point set is
related to the (logarithm of) the ratio between the longest and shortest
distance defined by the point set.  In particular, different versions
of the following theorem have been proven by Halld\'orsson and Tokuyama
\cite{ht08}, and Khabbazian, Durocher, and Haghnegahdar
\cite{kdh11}:
\begin{thm}\thmlabel{log}
  For any constant integer $d\ge 1$, and for all $V\subset\R^d$
  $I(V)=O(\log D)$, where $D=\max\{\|xy\|: \{x,y\}\subseteq V\}/\min\{\|xy\|:
  \{x,y\}\subseteq V\}$.
\end{thm}
(A version of this theorem even applies when $V$ is a set of elements
in a metric space of bounded doubling dimension \cite{msz11}.)  The
\emph{minimum spanning tree} of $V$ is the connected graph, $\mst(V)$,
of minimum total edge length.  At least two of the proofs of \thmref{log}
proceed by showing that $I(\mst(V))=O(\log D)$.  A strengthening of
this theorem, that is implict in a proof due to Maheshwari, Smid, and
Zeh \cite{msz11} is that the numerator in the definition of $D$ can be
replaced with the length of the longest edge in $\mst(V)$.

\thmref{log} suggests that point sets with very high interference are
unlikely to occur in practice.  This intuition is born out by the results
of Kranakis \etal\ \cite{kkmns10}, who show that high interference is
unlikely to occur in random point sets in one dimension:
\begin{thm}\thmlabel{sqrtlogn}
  Let $V$ be a set of $n$ points independently and uniformly distributed
  in $[0,1]$.  Then, with high probability, $I(\mst(V))\in \Theta((\log
  n)^{1/2})$.
\end{thm}
Note that, in this one-dimensional case, the minimum spanning tree,
$\mst(V)$, is simply a path that connects the points of $V$ in order,
from left to right.  The lower-bound in \thmref{sqrtlogn} is proven
by showing that, with high probability, $V$ contains a subsequence
of points like that used in the proof of \thmref{sqrtnlower},
whose length is $\Omega((\log n)^{1/2})$.  Combining the proofs of
Theorems~\ref{thm:sqrtnlower} and \ref{thm:sqrtlogn} one obtains the
following result:
\begin{thm}
  Let $V$ be a set of $n$ points independently and uniformly distributed
  in $[0,1]$.  Then, with high probability, $I(V)\in \Omega((\log
  n)^{1/4})$.
\end{thm}
Our \thmref{lower-bound} is a generalization of the previous two results to
any constant dimension $d>1$.

In higher dimensions, Khabbazian, Durocher, and Haghnegahdar use their
version of \thmref{log} to show:
\begin{thm}
  Let $V$ be a set of $n$ points independently and uniformly distributed
  in $[0,1]^d$.  Then, with high probability, $I(\mst(V))\in O(\log n)$.
\end{thm}
This result follows from two observations: (1)~no pair of points in $V$
is at distance greater than $\sqrt{d}$, and (2)~with high probability,
the minimum distance between any pair of points is $\Omega(n^{-2})$.  Therefore,
with high probability
\[
   D \le \sqrt{d}/\Omega(n^{-2}) = O(n^2)
\]
and applying \thmref{log} completes the proof.

In the remainder of this paper, we prove \thmref{main}.

\section{Proof of \thmref{main}}

In this section, we prove \thmref{main}.  However, before we do this,
we state a slightly modified version of \thmref{log} that is needed in
our proof.

\begin{lem}\lemlabel{log}
  Let $V\subset\R^d$ be a set of points such that no ball of unit
  diameter contains more than $c$ points of $V$, for some constant $c$
  and suppose that $I(MST(V))=r$ for some integer $r>1$.  Then $MST(V)$
  contains an edge of length $\Omega(c^r)$, for some constant $c=c(d)>1$
  depending only on $d$.
\end{lem}

\begin{proof}[Proof Sketch]
This proof is similar to Lemma~3 of
\cite{msz11}. Let $x$ be any point in $\R^d$ and let $D$ be the length
of the longest edge in $\mst(V)$.  Partition the set
of balls $B_i\in B(\mst(V))$ that contain $x$ into
$O(\log D)$ sets $S_0,\ldots,S_{r'}$. The set $S_0$ contains all balls of radius at most $1/2$.  The set $S_i$, $i\in\{1,\ldots,k\}$ contains all balls whose radius is in the range $(2^{i-1},2^i]$.  It is sufficient to show that each set $S_i$ contains $O(1)$ edges.

The assumption that no unit disk contains more than $c$ points of $V$
implies that $|S_0|\le c$.  For any $S_i$, $i\in\{1,\ldots,k\}$, we argue
as follows.  All the balls in $S_i$ %TODO: continue
are contained in a ball of radius $2^{i+1}$ centered at $x$.  Therefore, a simple packing argument implies that there exists a ball of radius $2^{i-1}$ that contains $c_d$ centers of balls in $S_i$.  The center of each ball is the endpoint of an edge of length 


where the $i$th set contains balls with radii in
$[2^i,2^{i+1})$.  Each of these sets contains only a constant number of
balls so, if $x$ is contained in $r$ balls, there must be some edge of
length $2^{\Omega(r)}$.

\noindent\ldots finish for the cases where $c>0$ \ldots
\end{proof}


\begin{proof}[Proof of \thmref{main}]
For ease of exposition, we will prove the theorem for $d=2$, and mention,
later, the modifications required for the cases $d=1$ and $d\ge 3$.
Partition $[0,1]^2$ into square \emph{cells} of area $1/nt$ for some
value $t$ to be specified later.  Let $N_i$ denote the number of points
that are contained in the $i$th cell.  Then $N_i$ is binomial
with mean $\mu=1/t$.  Recall Chernoff's Bounds \cite{c52} on the tails
of binomial random variables:
\[
  \Pr\{N_i \ge (1+\delta)\mu\} 
    \le \left(\frac{e^\delta}{(1+\delta)^{1+\delta}}\right)^\mu \enspace .
\]
In our setting, we have, 
\begin{align*}
  \Pr\{N_i \ge k\} 
    & = \Pr\{N_i \ge kt\mu\} \\
    & \le \left(\frac{e^{kt}}{(kt)^{kt}}\right)^{1/t} \\
    & = \frac{e^{k}}{(kt)^{k}} \\
    & \le \frac{1}{t^{k}} & \text{for $k\ge e$} \\
    & \le \frac{1}{n^{c+2}} & \text{for $t=2^{(\log n)^{1/3}}$ and $k=(c+2)(\log n)^{2/3}$.} \\
\end{align*}
Note that the number of cells is no more than $nt\le
n^2$, for sufficiently large $n$.  Therefore, by the union bound, the
probability that there exists any cell containing more than $k$ points
is at most $n^{-c}$.

Within each non-empty cell, we apply \thmref{sqrtn2d} to
connect the vertices in the $i$th cell into a connected graph $G_i$
with $I(G_i)=O(\sqrt{N_i})$.  In fact, a somewhat stronger result holds,
namely that $\max\{I(x,G_i) : x\in\R^2\}=O(\sqrt{N_i})$.  Notice that
each edge in $G_i$ has length at most $\sqrt{2/nt}$.  Stated another
way, in $\bigcup_i G_i$, any point, $x$, receives interference only
from cells within distance $\sqrt{2/nt}$ of the cell containing $x$.
There are only 13 such cells, so
\[
  \max\left\{I\left(x,\bigcup_iG_i\right) : x\in\R^2\right\}=O(\sqrt{k}) 
    = O((\log n)^{1/3})
\]
with high probability.

Thus far, the points within each cell are connected to each other and
the maximum interference, over all points in $\R^2$, is $O(\sqrt{k})$.
To connect the cells to each other, we select one point from each
non-empty cell and connect these using a minimum spanning tree, $T$.
What remains is to show that the additional interference caused by the
addition of the edges in $T$ does not exceed $O((\log n)^{1/3})$.

Suppose that $I(x,T)=r$, for some point $x\in\R^2$.  There are at most
4 vertices in $T$ whose distance to $x$ is less than $1/\sqrt{nt}$.
Therefore, by \lemref{log}, $T$ must contain an edge of length at least
$c^r/\sqrt{nt}$, for some $c>1$.  

A well-known property of minimum spanning trees is that, for any edge
$uv$ in $T$, the open disk with diameter $uv$ does not contain any
vertices of $T$.  In our setting, this means that there is an open disk,
$B$, of radius $c^r/2\sqrt{nt}$ such that every cell contained in $B$
contains no point of $V$.  Inside of $B$ is another empty disk $B'$ of
radius $c^r/(2\sqrt{nt})-\sqrt{2/nt}$ whose center is also the center
of some cell.

At least one quarter of the area of $B'$ is contained in $[0,1]^2$,
so the number of cells completely contained in $B'$ is $\pi c^{2r}/16 -
O(c^{r}/\sqrt{nt})$.  By decreasing $c$ slightly, and only considering
$r$ larger than a sufficiently large constant, $r_0$, we can simplify
this number of cells to $\pi c^{2r}/16$.

For a fixed disk $B'$, the probability that the $\pi c^{2r}/16$ cells
defined by $B'$ are empty of points in $V$ is at most
\begin{align*}
 p 
  & \le (1-\pi c^{2r}/{16nt})^{n} \\
  & \le \exp(-\pi c^{2r}/16t) \\
 & \le 1/n^{2+c'} \enspace ,
\end{align*}
for $r\ge(\log_c2)(\log(16/\pi)+\log t + \log(2+c')+\log\ln n)$.  By the union bound, the
probability that there exists any such $B'$ is at most
$pnt\le 1/n^{c'}$.  Since we can choose $r\in O(\log t+\log\log n) = O((\log
n)^{1/3})$, this completes the proof.
\end{proof}

\section{Proof of \thmref{lower-bound}}

In this section, we prove the lower-bound of \thmref{X}.  We define
a \emph{Zeno configuration} as follows:  A Zeno
configuration of size $k$, centered at $x$, is defined by a set of $k+1$ disks.
The construction starts with disjoint disks $D_0,\ldots,D_{k-1}$, each
having radius $u$.  The disk $D_0$ is centered at the $x$.  The center
of $D_i$, $i\in\{1,\ldots,k-1\}$ is at $x+(u3^i, 0)$.  A final large disk,
$D$, of radius $r=u3^k$ is centered at $x$ and contains all
other disks.  A Zeno configuration occurs in a point set $V$ when $D$
contains exactly $k$ points of $V$ and these occur with exactly one
point in each disk $D_i$.

The following lemma shows that a Zeno configuration in $V$ causes high
interference in $\mst(V)$.

\begin{lem}\lemlabel{zeno-mst}
If $V$ contains a Zeno configuration of size $k$, $I(\mst(V))\ge k-1$.
\end{lem}

\begin{proof}
Let $x_i$, $i\in\{0,\ldots,k-1\}$, denote the point of $V$ contained in
$D_i$.  Note that, for $i\in\{1,\ldots,k-1\}$ the closest point to $x_i$ in
$V$ is $x_{i-1}$.  Since $\mst(V)$ contains the nearest-neighbour graph, this
implies that $\mst(V)$ contains the edges $x_ix_{i+1}$ for all
$i\in\{0,\ldots,k-2\}$.  We claim that, for all $i\in\{0,\ldots,k-2\}$, the disk $B_i$ centered at $x_i$
that contains $x_{i+1}$ also contains $x_0$.  This is clearly true for
$i=0$ and $i=1$.  Next, note that
\[
  \|x_ix_0\| \le u(3^i+2) \enspace .
\]
On the other hand, for $i\ge 2$,
\[
  \|x_ix_{i+1}\| \ge u(3^{i+1}-3^i-2) = 2u3^i-2u \ge u(3^i + 7) > 
\|x_ix_0\| \enspace .
\]
Therefore, $I(x_0,\mst(V)) \ge k-1$.
\end{proof}

The next lemma shows that a Zeno configuration causes high interference on
any connected graph on vertex set $V$.

\begin{lem}
If $V$ contains a Zeno configuration of size $k$, then $I(V)\ge\sqrt{k-1}$.
\end{lem}

\begin{proof}
Let $G$ be any connected graph on $V$.  Using the same notation as
in the proof of \lemref{zeno-mst}, call a vertex, $x_i$, a \emph{big
one} if $x_i$ is adjacent to any vertex $x_j$, with $j>i$, or $x_i$ is
adjacent to any vertex $x$ not in $D$.  The proof of \lemref{zeno-mst}
shows that every big one contributes to the interference at $x_0$.
Therefore, if the Zeno configuration contains $\sqrt{k-1}$ or more big
ones, then $I(x_0,G)\ge\sqrt{k-1}$ and there is nothing left to prove.
Otherwise, note that each of $x_0,\ldots,x_{k-2}$ is either a big one
or adjacent to a big one. Therefore, there must be a big one, $x_i$,
with degree at least $\sqrt{k-1}-1$, so $I(x_i,G)\ge\sqrt{k-1}$.
\end{proof}

To prove \thmref{lower-bound}, all that remains is to show a Zeno
configuration of size $\Omega((\log n)^{1/2})$ occurs in $V$ with high
probability.

\begin{proof}[Proof of \thmref{lower-bound}]
Choose the parameter $u$ in the Zeno configuration so that $\pi r^2=1/n$,
i.e., $u=1/(\sqrt{\pi n}3^k)$.  Then the area of the small disks is
$1/(n3^{2k})$.
We analyze the probability that a Zeno configuration of length $k=c(\log
n)^{1/3}$ centered at $x_i$ occurs in a set, $V$, of $n$ i.u.d. points
$\{x_1,\ldots,x_n\}$ in
$[0,1]^2$.  Let $\mathcal{Z}_i$ denote the event ``$V$
contains a Zeno configuration centered at $x_i$.''  Then we have
\begin{align*}
 \Pr\{\mathcal{Z}_i\mid x_i\in[r,1-r]^2\} 
  & = \left(\frac{(n-1)!}{(n-k)!}\right) % ways of choosing remaining points (order matters)
      \left(\frac{1}{n3^{2k}}\right)^{k-1}   % ith point lands in D_i 
      \left(1-\frac{1}{n}\right)^{n-k}   \\ % remaining points avoid D 
  & \ge
      \left((n-k)^{k-1}\right) 
      \left(\frac{1}{n3^{2k(k-1)}}\right)^{k-1}  
      \left(1-\frac{1}{n}\right)^{n-k}   \\ 
  & \ge 
      (1-k/n)^{k-1} % 
      \left(\frac{1}{3^{2k(k-1)}}\right)
      \left(1-\frac{1}{n}\right)^{n-k} \\
  & \ge (1/e-o(1)) \cdot \left(\frac{1}{3^{2k(k-1)}}\right) \\
  & = \Omega(1/n^{\alpha})
\end{align*}
for $k=((\alpha/2)(\log_3 n))^{1/2}$, where $\alpha$ is a free parameter in
the range $[0,1]$.  Since $\Pr\{x_i\in[r,1-r]^2\} > 1-4r$, we now
uncondition
\[
 \Pr\{\mathcal{Z}_i\} \ge  
    (1-4r)\cdot\Pr\{\mathcal{Z}_i\mid x_i\in[r,1-r]^2\}
    = \Omega(1/n^{\alpha})
\]
Let $Y_i$ be the indicator variable defined as
\[
   Y_i = \begin{cases} 1 & \text{if $\mathcal{Z}_i$} \\
                       0 & \text{otherwise} 
         \end{cases}
\]
and let $N=\sum_{i=1}^n Y_i$ count the number of Zeno configurations.  We
have just shown that 
\[
   \E[N] = n\E[Y_i] = n\Pr\{\mathcal{Z}_i\}=\Omega(n^{1-\alpha}) \enspace .
\]
Unfortunately, this is not quite enough to prove that $N>0$ with high
probability.  Instead, we finish the proof using the second moment method
(c.f., Alon and Spencer \cite[Chapter~4]{asXX}).  For this, we need only 
show that, for any $\{i,j\}\subset\{1,\ldots,n\}$,
\[
   \limsup_{n\rightarrow\infty}\frac{\E[Y_iY_j]}{\E[Y_i]\E[Y_j]} = 1
\]
To do this, we repeat the above argument, but for a pair of Zeno
configurations, one at $x_i$ and one at $x_j$.  Let $A$ denote the event
``$\|x_ix_j\| < 2r$ or $\{x_i,x_j\}\not\subset[r,1-r]^2$''.  Let $A^c$
denote the complement of $A$.  Conditioning on $A^c$ we obtain
\begin{align*}
\frac{\E[Y_iY_j]}{\E[Y_i]\E[Y_j]} 
& \le \frac{\E[Y_iY_j]}{1/(e3^{2k^2})} \\
& = (e3^{2k^2})^2(\Pr\{A^c\}\E[Y_iY_j|A^c] + \Pr\{A\}\E[Y_iY_j|A]) \\
& \le (e3^{2k(k-1)})^2\E[Y_iY_j|A^c]  + (4r+\pi r^2) \\
& \le (e3^{2k(k-1)})^2 
    \left(\frac{(n-2)!}{(n-2k)!}\right)
    \left(\frac{1}{n3^{2k}}\right)^{2k-2}
    \left(1-\frac{2}{n}\right)^{n-2k} + (4r+\pi r^2) \\
& \le (e3^{2k(k-1)})^2
    \left(n^{2k-2}\right)
    \left(\frac{1}{n3^{2k}}\right)^{2k-2}
    \left(1-\frac{2}{n}\right)^{n-2k} + (4r+\pi r^2) \\
& \le (e3^{2k(k-1)})^2
    \left(\frac{1}{3^{2k}}\right)^{2k-2}
    \left(1-\frac{2}{n}\right)^{n-2k} + (4r+\pi r^2) \\
& \le (e3^{2k(k-1)})^2
    \left(\frac{1}{3^{2k}}\right)^{2k-2}
    \left(1/e^2-o(1)\right) + (4r+\pi r^2) \\
& = 1-o(1) + O(1/\sqrt{n}) \rightarrow 1 \enspace ,
\end{align*}
as $n\rightarrow\infty$.
\end{proof}


\end{document}
