\documentclass[charterfonts,lotsofwhite]{patmorin}
\usepackage{graphicx}
\usepackage{amsopn}
\input{pat}

\newcommand{\defequals}{\stackrel{\mathrm{def}}{=}}
\newcommand{\boundary}{\partial}
\DeclareMathOperator{\depth}{depth}

\title{\MakeUppercase{Distribution-Sensitive Point-in-Polygon Testing}}
\author{ULB Algorithms \and John Iacono \and Pat Morin}
\date{}

\begin{document}
\maketitle

\section{Introduction}


For a set $X$, itself containing sets, we denote the union of its
elements by $\cup X=\bigcup_{x\in X} x$.

For a distribution $D$ and an event $X$, we denote by $D_{|X}$ the
distribution $D$ conditioned on $X$.  That is, the distribution where 
the probability of an even $Y$ is
$\Pr(Y|X)=\Pr(Y\cap X)/\Pr(X)$.

A \emph{classification problem} over a domain $\mathcal{D}$ is a function
$\mathcal{P}:\mathcal{D}\mapsto \{0,\ldots,k-1\}$.  The special case
in which $k=2$ is called a \emph{decision problem}.

A \emph{classification tree} is a full binary tree\footnote{A full
binary tree is a rooted ordered binary tree in which each non-leaf
node has exactly two children.} in which each internal node $v$ is
labelled with a predicate $P_v(\cdot)$ and for which each leaf $\ell$
is labelled with a value $d(\ell)\in\{0,\ldots,k-1\}$. The
\emph{search path} of an input $p$ in a comparison tree $T$ starts at
the root of $T$ and, at each internal node $v$, proceeds to the left
child of $v$ if $P_v(p)$ is false and proceeds to the right child of
$v$ otherwise.  We denote by $T(p)$ the label of the final (leaf) node
in the search path for $p$.  We say that the classification tree $T$
\emph{solves} the classification problem $\mathcal{P}$ over the domain
$\mathcal{D}$ if, for every $p\in \mathcal{D}$, $\mathcal{P}(p)=T(p)$.

When there is a probability measure $D$ over $\mathcal{D}$, we define
the \emph{expected search time} of a classification tree $T$ as the
expected length of the search path for $p$ when $p$ is drawn at random
from $\mathcal{D}$ according to the distribution $D$.  Note that, for
each leaf $\ell$ of $T$ there is a subset of $r(\ell)\subseteq
\mathcal{D}$ such that the search path for any $p\in r(\ell)$ ends at
$\ell$.  Thus, the expected search time of $T$ can be written as
\[
     \mu_D(T) = \sum_{\ell\in L(T)} \Pr(r(\ell))\times \depth(\ell)
	\enspace ,
\]
where $L(T)$ denotes the leaves of $T$ and $\depth(\ell)$ denotes the
length of the path from the root of $T$ to $\ell$.

The folloing theorem, which is a restatement of (part of) Shannon's
coding Theorem \cite{sXX}, is what all existing results use to
establish their optimality:

\begin{thm}\thmlabel{shannon}
Let $\mathcal{P}:D\mapsto \{0,\ldots,k-1\}$ be a classification
problem and let $p\in \mathcal{D}$ be selected from a distibution $D$ such
that $\Pr\{\mathcal{P}(x)= i\}=p_i$, for $0\le i< k$.  Then, any
classification tree $T$ that solves $\mathcal{P}$ has
\begin{equation}
     \mu_D(T) \ge \sum_{i=0}^{k-1} p_i\log(1/p_i) \enspace .
	\eqlabel{shannon}
\end{equation}
\end{thm}

\thmref{shannon} can provide very powerful lower bounds since it makes
no assumptions about the predicates used in the nodes of the decision
tree $T$.  However, it is useless for decision problems since the
right hand side of \eqref{shannon} is never greater than 1 in this
case.  For instance, in all existing results on distribution-sensitive
planar point location, the set $\{0,\ldots,k-1\}$ are the set of faces
in the subdivision.  However, for one of the simplest planar point
location problems, namely testing whether a point is contained in a
convex polygon, there are only two faces, so \thmref{shannon} can not
give a lower bound greater than 1 regardless of the input distribution
or the number of vertices of the polygon.  However, we know 
that testing if a point is contained in a convex $n$-gon has an
$\Omega(\log n)$ lower bound in fairly strong models of computation,
including algebraic decision trees \cite{bo}.

\section{Collections of Intervals}

Before considering point-in-polygon testing we warm up by studying a
related one-dimensional problem on a set of intervals.  Our motivation
for studying this problem is that it gives intuition into how to
characterize the complexity of point-in-polygon testing under random
distributions.

Let $I=\{[a_i,b_i]: i=1,\ldots,n\}$ be a set of $n$ disjoint closed
real intervals, with $b_i < a_{i+1}$ for all $1\le i\le n$.  We are
interested in the problem of constructing a decision tree for $I$ that
can answer the decision problem: Given $p\in\R$, is $p\in \cup I$?
The type of decision trees we consider are \emph{comparison trees}.
These are decision trees in which each internal node $v$ is labelled
with a comparison of the form $P_v(x) = [x > n(v)]$ where $n(v)$ is
some real number called the \emph{label} of $v$.\footnote{Here and
throughout, we use Iverson's notation \cite{kXX} where, for a
predicate $X$, $[X]$ evaluates to 1 if $X$ is true and 0 otherwise.}
In particular, we are interested in the case when the query point $p$
is drawn according to some probability distribution $D$.   Throughout
the remainder of this section, all references to probabilities are
implicitly with reference to $D$ and the query domain is the support
of $D$.  We call such a decision tree a membership tester for $(I,D)$.

\comment{
To study the expected search time of comparison trees for this problem
we need to define a slightly different set of intervals than $I$.  Let
$J$ be the set of intervals obtained after the following two
operations:

\begin{enumerate}

\item Remove from $I$ all intervals $\ell$ such that $\Pr(\ell) = 0$.

\item Repeat the following operation as many times as possible:  If
$\Pr([b_i,a_{i+1}])=0$ then replace the two intervals $(a_i,b_i)$ and
$(a_{i+1},b_{i+1})$ with the single interval $(a_i,b_{i+1})$
\end{enumerate}
}

\begin{lem}\lemlabel{intervals}
Let $\overline{I}$ be the set of maximal open intervals in
$\R\setminus\cup I$ and let $S=I\cup \overline{I}$.  Then,
for any decision tree that tests if a point $p$ drawn from $D$
is in $I$,
\[
    \mu_D(T) \ge H(I,D) \defequals 
	\sum_{x\in S} \Pr(x)\log(1/\Pr(x)) \enspace .
\]
\end{lem}

\begin{proof}
Consider two distinct intervals $i,j\in S$ and suppose $p\in i$ and
$q\in j$.  Note that the search paths for $p$ and $q$ must finish in
different leaves of $T$ since, otherwise, there is some point
$r\in[p,q]$ that is incorrectly classified by $T$, i.e., such
that $T(p)\neq [p\in\cup I]$.  Now, for every interval $i\in S$,
select a point $p_i\in i$ such that the length of the search path for
$p$ is minimum.   Using the standard encoding of paths in a binary
search tree, this gives a code for the intervals of $S$ where the
length of the codeword for $i$ is the depth of $p_i$ in $T$.  In this
way we obtain

\[
   \mu_D(T)
    = \sum_{\ell\in L(T)} \Pr(\ell)\depth(\ell)
   \ge \sum_{i \in S} \Pr(i)\depth(p_i)
   \ge H(I,D)
   \enspace , 
\]
where the second inequality follows from Shannon's source coding
theorem \cite{X}.
\end{proof}

Note that, to prove the above result, we required some assumptions
about the predicates used at the nodes of $T$, namely that they are
comparisons.  Without some such assumption a lower bound that holds
for every set $I$ of intervals is impossible to prove.\footnote{For
example, if the intervals are defined by $a_i=2(i-1)$ and
$b_i=2(i-1)+1$ then the predicate $P(x)=[(x\ge 0)\wedge(x\le
2n)\wedge((\floor{x}\bmod 2=0)\vee(x=\floor{x}))]$ determines if $x\in
S$ in constant time.} This is in stark contrast to the very closely
related classification problem: Given a partitioning of $R$ into a set
$S$ of $n$ intervals, construct a classification tree whose output is
$i$ if its input point is contained in the $i$th interval.  For the
latter problem, \thmref{shannon} implies that any decision tree has an
expected search time at least equal to the entropy of the distribution
of the queries over the intervals. 

Though not the focus of the current paper, it is worth noting that the
lower bound in \lemref{intervals} can be matched to within an additive
constant by storing the intervals of $I$ and $\overline{I}$ at the
leaves of a biased binary search tree \cite{kXX,mXX}.  In particular,
for every $I$ and $D$ there exists a membership tester for $(I,D)$
whose expected search time is $H(I,D)+O(1)$.

\section{Point In Convex Polygon Testing}

Let $P$ be a convex $n$-gon whose boundary is denoted by $\partial P$
and consider a probability measure $D$ over $\R^2$.  We use the
convention that $P$ contains it boundary.  In this section we are
interested preprocessing $P$ and $D$ so that we can quickly solve the
decision problem of testing whether a query point $p$, drawn according
to $D$, is contained in $P$. 

In particular, we are interested in algorithms that can be described
as \emph{linear decision trees}.  These are decision trees such that
each internal node $v$ contains a linear inequality $P_v(x,y)=[ax+by
\ge c]$.  Again, we require that, for every $p$ in the support of $D$,
the leaf reached in the search path for $p$ is labelled with a 1 if
and only if $p\in P$.  Geometrically, each internal node of $T$ is
labelled with a directed line and the decision to go to the left or
right child depends on whether p is to the left or right (or on) this
line.  

Our exposition is broken up into three sections.  We begin by
describing a data structure (in fact, a decision tree) that solves
this problem.  Next, we give an (easy) analysis of the expected search
time of this data structure.  Finally, we give a (more difficult)
proof that this expected search time is optimal.


\subsection{Triangle Trees}

We begin by describing a data structure that works by creating a
sequence of successively finer approximations $A_0,\ldots,A_k$ to
$\boundary P$.  Each approximation $A_i$ consists of two convex
polygons; an \emph{outer approximation} that contains $P$ and an
\emph{inner approximation} that is contained in $P$.

Each approximation $A_i$ is completely defined by a set $S_i$ of
$O(2^i)$ points from $\boundary P$.  The inner approximation is simply
the convex hull of $S_i$.  The outer approximation has an edge tangent
to $P$ at each of the points of $S_i$.  More precisely, for each point
$x\in S_i$ there is an edge $e$ of the outer approximation that
contains $x$.  If $x$ is in the interior of an edge of $P$ then $e$ is
contained in the same line that contains that edge. Otherwise ($x$ is
a vertex of $P$) $e$ is supported by the line containing the edge
incident to $x$ that precedes it in counterclockwise order.  We ensure
that successive approximations have a containment relationship, i.e.,
$A_i\supseteq A_{i+1}$, by choosing our boundary points so that
$S_i\subseteq S_{i+1}$.

\notice{say something about degenerate triangles.} Next we define the
sets $S_0,\ldots,S_k$ that define our approximations.  The set $S_0$
is empty, and we use the convention that the outer approximation in
this case is the entire plane and the inner approximation is the empty
set. The set $S_1$ consists of two points, $x$ and $y$ on $\boundary
P$ such that $\Pr(h_1)\le 1/2$ and $\Pr(h_2)\le 1/2$, for each of the
two open halfspaces $h_1$ and $h_2$ bounded by the line containing $x$
and $y$.

Next, we show how to obtain the set $S_i$ from the set $S_{i+1}$.  Let
$p_0,\ldots,p_{m_i-1}$ be the points in $S_i$ as they occur in
conterclockwise order around the boundary of $P$.  The approximation
$A_i$ thus consists of $m$ open triangles
$\Delta_0,\ldots,\Delta_{m-1}$ where $\Delta_j$ has one side joining
$p_j$ and $p_{j+1}$ and a third vertex $t_j$ that lies at the
intersection of two lines tangent to $P$ at $p_j$ and
$p_{j+1}$.\footnote{Throughout this discussion, subscripts are
implicitly taken modulo $k$.}  Refer to \figref{split}. For each
triangle $\Delta_j$ that has positive area we add a new point to
$S_{i+1}$  by partitioning $\Delta_j$ into two open triangles
$\Delta_{j,0}$ and $\Delta_{j,1}$ by a line $\ell$ through $t_j$ and
such that 
\[  
     \Pr(\Delta_{j,b}) \le \Pr(\Delta_{j})/2 \enspace ,
\]
for $b=0,1$.
We then select a new point to add to $S_{i+1}$ at the intersection of
$\ell$ and $\boundary P$ that occurs in $\Delta_j$.

\begin{figure}
\begin{center}
\begin{tabular}{cc}
\includegraphics{split-a} & \includegraphics{split-b}
\end{tabular}
\end{center}
\caption{Splitting the triangle $\Delta_j$ into two triangles
$\Delta_{j,0}$ and $\Delta_{j,1}$ with a line through $t_j$.}
\figlabel{split}
\end{figure}

The entire process terminates at the first value of $k$ for which all
triangles have 0 area.  The approximations $A_0,\ldots,A_k$ are stored
as a binary tree $T$ that we call a \emph{triangle tree}.  Each node
$v$ of $T$ at level $i$ in $T$ corresponds to an open triangle
$\Delta(v)$ in approximation $A_{i}$ and the two children of $v$
correspond to the two triangles into which $\Delta(v)$ is split.  The
leaves of $T$ correspond to 0 area triangles that are actually line
segments in $\boundary P$. 

To use the tree $T$ to determine if a point $p$ is contained in $P$ we
proceed top-down, starting at the root.  For a point $p$ contained in
$\Delta(v)$ one of two things can happen: (1) $p$ is contained in one
of the two open triangles $\Delta(v_r)$ or $\Delta(v_\ell)$ where
$v_r$ and $v_\ell$ are the right and left child of $v$, respectively,
in which which we recurse on the right or left child of $v$,
respectively, or (2) otherwise we can determine in constant time if
$p\in P$.

\subsection{Analysis of the Triangle Tree}

Let $T$ be the triangle tree for a convex $n$-gon $P$ and a
distribution $D$.  For each internal node $v$ of $T$, define
$\Pr(v)=\Pr(\Delta(v))-\Pr(\Delta(v_r))-\Pr(\Delta(v_\ell))$ where
$v_\ell$ and $v_r$ are the two children of $v$ in $T$.  Notice that
$\Pr(v)$ is the probability that a search terminates at node $v$.

\begin{lem}\lemlabel{upper-bound}
Using the triangle tree $T$, the expected number of linear inequalities
required to check if $p\in P$ for a point $p$ drawn from $D$ is at
most
\[
     \mu_D(T) = O(1)\times \sum_{v\in T}\Pr(v)\times\depth (v)
      \le O(1)\times \sum_{v\in T}\Pr(v)\log(1/\Pr(v))
\]
\end{lem}

\begin{proof}
The first equality is simply by the definition of expectation.  The
second inequality follows immediately from the fact that, for any node
$v\in T$ whose parent is $u$, $\Pr(v)\le \Pr(\Delta(v))\le (1/2)\Pr(\Delta(u))$.
\end{proof}


\subsection{Optimality of the Triangle Tree}

Next we will show that the performance bound given by
\lemref{upper-bound} is optimal.  That is, that there is no linear
decision tree whose expected search time (on distribution $D$) is
asymptotically better than that of the triangle tree.
The key ingredient in our argument is the following proof:

\begin{lem}
Let $V$ be a subset of the vertices of the triangle tree $T$ such that
no vertex in $V$ is the descendent of any other vertex in $V$.  Let
$R=\bigcup_{v\in V} \Delta(v)$ and let $D_{|R}$ denote the
distribution $D$ conditioned on $R$.  Then, for any linear decision
tree $T'$,
\[
    \mu_{D_{|R}}(T) 
	\ge \sum_{v\in V}\Pr(\Delta(v)|R)\log(1/\Pr(\Delta(v)|R) \enspace .
\]
\end{lem}

\begin{proof}
Easy.
\end{proof}

The remainder of our argument only involves some careful manipulations
of entropy and probabilities.  Let
\[
   H = \sum_{v\in V(T)} \Pr(v)\log (1/\Pr(v))
\] 
be the entropy of the distribution of search paths in $T$.
Partition the internal nodes of $T$ in \emph{groups} $G_1,G_2,\ldots$
where
\[
	G_i = \{v : 1/2^{i-1} > \Pr(\Delta(v)) \ge 1/2^i \} \enspace .
\]
Fix some constant $0< \epsilon< 1$ to be specified later and define a
group $G_i$ as \emph{big} if $|G_i|\ge 2^{\epsilon i}$ and
\emph{small} otherwise.

\begin{lem}
Every big group $G_i$ contains a subset $G_i'$ of size least
$|G_i'|/\epsilon i$ such that, no vertex of $G_i'$ is the
descendant of any other vertex in $G_{i}'$.
\end{lem}

\begin{proof}
Easy.
\end{proof}

Let $I$ be the set of indices of all big groups and let $\overline{I}$
be the set of indices of all small groups.

\begin{eqnarray}


\end{eqnarray*}




First we show that small groups do not contribute much to $H$ and that
most of the entropy comes from big groups:
\begin{eqnarray*}
   H & = & \sum_{j=1}^n p_j\log(1/p_j) \\
   & = & \sum_{i\in I}\sum_{j\in G_i} p_j\log (1/p_j) +
   	\sum_{i\in \overline{I}}\sum_{j\in G_i} p_j\log (1/p_j) \\
    & \le & \sum_{i\in I}\sum_{j\in G_i} p_j\log (1/p_j) +
	\sum_{i\in \overline{I}}\sum_{j\in G_i} p_j i \\
    & \le & \sum_{i\in I}\sum_{j\in G_i} p_j\log (1/p_j) +
	\sum_{i\in \overline{I}}(2^{\epsilon i}/2^i) i \\
    & \le & \sum_{i\in I}\sum_{j\in G_i} p_j\log (1/p_j) +
	\sum_{i\in \overline{I}}i/(2^{1-\epsilon})^i \\
    & \le & \sum_{i\in I}\sum_{j\in G_i} p_j\log (1/p_j) +
	\sum_{i=1}^\infty i/(2^{1-\epsilon})^i \\
    & = & \sum_{i\in I}\sum_{j\in G_i} p_j\log (1/p_j) + O(1)
\end{eqnarray*}

Next, observe that if $G_i$ is big then
\begin{equation}
    \log\left(\sum_{j\in G_i} p_j \right)  
     \ge \log\left(\sum_{j\in G_i} 1/2^{i} \right)  
      \ge \log\left(2^{\epsilon i}/2^{i} \right)  
	\ge (\epsilon -1)i 
\end{equation}





\comment{
\begin{lem}\lemlabel{lower-bound-a}
Let $C=P_1\setminus P_2$ where $P_1$ and $P_2$ are closed unbounded
convex subsets of $\R^2$ with $P_2\subseteq P_1$.  Let
$C_1,\ldots,C_k$ be the connected components of $C\setminus\partial
P$.  Then, for any linear decision tree $T$,
\[
	\mu_{D_{|C}}(T) \ge \frac{1}{2}\times H(P,D_{|C},C) 
		= -\frac{1}{2}
			\times \sum_{i=1}^k \Pr(C_i|C)\log \Pr(C_i|C)
		\enspace .
\]
\end{lem}

\begin{proof} 
We first prove a lower bound for  a slightly different type of
decision tree.  A \emph{chord} of $C$ is a line segment whose
endpoints are on the boundary of $C$.  A \emph{chord decision tree}
for $C$ is defined in exactly the same manner as a linear decision
tree except that the internal nodes of the tree are labelled with
chords of $C$.  The chord at a node $v$ partitions $C$ into two parts
and each of these corresponds to one of the children of $v$.  The
following claim relates chord decision trees to linear decision trees
and follows easily from the fact that the intersection of any line
with $C$ consists of at most 2 line segments.

\begin{clm}\clmlabel{two-approx}
Given a linear decision tree $T$ for $(P,D_{|C})$ with expected cost $k$,
there exists a chord decision tree $T_C$ for $(P,D_{|C})$ with expected cost
at most $2k$. 
\end{clm}

Let $T$ be any linear decision tree and let $T_C$ be the chord
decision tree guaranteed by \clmref{two-approx}.  As in the proof of
\lemref{intervals}, the search paths for a point $p\in C_i$ and a
point $q\in C_j$ with $i\neq j$ must finish at different leaves of the
chord decision tree (this uses the fact that the support of $D$ is a
convex set with $P$ in its interior).  Thus, we can assign a code to
$C_1,\ldots,C_k$ such that the length of the codeword for $C_i$ is the
length of the shortest path from the root of $T$ to a leaf
corresponding to some point in $C_i$.  Then we again obtain
\[
  \mu_{D_{|C}}(T_C)
	= \sum_{\ell\in L(T_C)} \Pr(\ell|C)\depth(\ell) 
	\ge -\sum_{i=1}^k \Pr(C_i|C)\log\Pr(C_i|C) \enspace .
\]
Then, from \clmref{two-approx}, $\mu_{D_{|C}}(T) \ge
\frac{1}{2}\mu_{D_{|C}}(T_C)$, completing the proof.
\end{proof}

Next we describe a data structure that uses \lemref{lower-bound-a} to
guide its construction.   The data structure works by creating a
sequence of successively finer approximations $A_0,\ldots,A_k$ to
$\boundary P$.  The initial approximation $A_0$ has, as its outer
boundary, the bounding box of $P$ and its inner boundary is the convex
hull of an uppermost, leftmost, bottommost and rightmost point at
which $P$ intersects its bounding box (see \figref{approx0}). 

\begin{figure}
\begin{center}\includegraphics{approx0}\end{center}
\caption{The approximation $A_0$ to $\boundary P$.}
\end{figure}

In general, the approximation $A_i$ is obtained by selecting a set
$S_i$ of $O(2^i)$ points from $\boundary P$.  The outer approximation
is a polygon containing $P$ whose edges are tangent to $P$ at the
points in $S_i$.  The inner approximation is the convex hull of $S_i$.
We ensure that successive approximations have a containment
relationship, i.e., $A_i\supseteq A_{i+1}$, by choosing our boundary
points so that $S_i\subseteq S_{i+1}$.

More specifically, we obtain the set $S_{i+1}$ from $S_i$ in the
following way:  Let $p_0,\ldots,p_{k-1}$ be the points in $S_i$ as
they occur in conterclockwise order around the boundary of $P$.  The
approximation $A_i$ thus consists of $k$ triangles
$\Delta_0,\ldots,\Delta_{k-1}$ where $\Delta_j$ has one side joining
$p_j$ and $p_{j+1}$ and a third vertex $t_j$ that lies at the
intersection of two lines tangent to $P$ at $p_j$ and
$p_{j+1}$.\footnote{Throughout this discussion, subscripts are
implicitly taken modulo $k$.}  For each triangle $\Delta_j$ for $0\le
j< k$ we add a new point to $S_{i+1}$  
by partitioning $\Delta_j$ into two triangles
$\Delta_{j,0}$ and $\Delta_{j,1}$ by a line $\ell$ through $t_j$ and such
that 
\[  
     \Pr(\Delta_{j,0}) = \Pr(\Delta_{j,1}) = \Pr(\Delta_{j})/2 \enspace .
\]
We then select a new point to add to $S_{i+1}$ at the intersection of
$\ell$ and $\boundary P$ (see \figref{split}).

\begin{figure}
\begin{center}
\begin{tabular}{cc}
\includegraphics{split-a} & \includegraphics{split-b}
\end{tabular}
\end{center}
\caption{Splitting the triangle $\Delta_j$ into two triangles
$\Delta_{j,0}$ and $\Delta_{j,1}$ with a line through $t_j$.}
\figlabel{split}
\end{figure}



\subsection{Working with Entropy}

Here are some useful properties of entropy. Consider a probability
distribution over $\{1,\ldots,n\}$ where $\Pr\{i\}=p_i$ and let
$H=\sum_{i=1}^n p_i\log (1/p_i)$ be the entropy of this distribution.
We partition the probabilities into $\ceil{\log n}$ \emph{groups}
$G_1,\ldots G_{\ceil{\log n}}$ where 
\[
	G_i = \{j : 1/2^{i-1} > p_j \ge 1/2^i \} \enspace 
\]
for $1\le i < \ceil{\log n}$ and $G_{i}=\{j: p_j < 1/2^i\}$ for
$i=\ceil{\log n}$.  Fix some constant $0<\epsilon < 1/2$ and define a
group $G_i$ as \emph{big} if $|G_i|\ge 2^{\epsilon i}$ and
\emph{small} otherwise.  Let $B$ contain the indices of all big groups
and let $S$ contain the indices of all small groups.

First we show that small groups do not contribute much to $H$ and that
most of the entropy comes from big groups:
\begin{eqnarray*}
   H & = & \sum_{j=1}^n p_j\log(1/p_j) \\
   & = & \sum_{i\in B}\sum_{j\in G_i} p_j\log (1/p_j) +
   	\sum_{i\in S}\sum_{j\in G_i} p_j\log (1/p_j) \\
    & \le & \sum_{i\in B}\sum_{j\in G_i} p_j\log (1/p_j) +
	\sum_{i\in S}\sum_{j\in G_i} p_j i \\
    & \le & \sum_{i\in B}\sum_{j\in G_i} p_j\log (1/p_j) +
	\sum_{i\in S}(2^{\epsilon i}/2^i) i \\
    & \le & \sum_{i\in B}\sum_{j\in G_i} p_j\log (1/p_j) +
	\sum_{i\in S}i/(2^{1-\epsilon})^i \\
    & \le & \sum_{i\in B}\sum_{j\in G_i} p_j\log (1/p_j) +
	\sum_{i=1}^\infty i/(2^{1-\epsilon})^i \\
    & = & \sum_{i\in B}\sum_{j\in G_i} p_j\log (1/p_j) + O(1)
\end{eqnarray*}

Next, observe that if $G_i$ is big then
\begin{equation}
    \log\left(\sum_{j\in G_i} p_j \right)  
     \ge \log\left(\sum_{j\in G_i} 1/2^{i} \right)  
      \ge \log\left(2^{\epsilon i}/2^{i} \right)  
	\ge (\epsilon -1)i 
\end{equation}


\begin{lem}
\end{lem}

\section{Conclusion}
}

\end{document}
