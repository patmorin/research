\documentclass[charterfonts,lotsofwhite]{patmorin}
\usepackage{graphicx}
\usepackage{amsopn}
\input{pat}

\newcommand{\defequals}{\stackrel{\mathrm{def}}{=}}
\newcommand{\boundary}{\partial}
\DeclareMathOperator{\depth}{depth}
\DeclareMathOperator{\lft}{left}
\DeclareMathOperator{\rght}{right}
\DeclareMathOperator{\prnt}{parent}


\title{\MakeUppercase{Distribution-Sensitive Point Location in Convex
Subdivisions}}
\author{ULB Algorithms \and John Iacono \and Pat Morin}
\date{}

\begin{document}
\maketitle

\section{Introduction}

The planar point location problem is one of the first problems to have
been considered by the field of computational geometry. Given a planar
straight line graph $G$, the planar point location problems asks us to
construct a data structure so that, for any query point $p$, we can
quickly determine which face of $G$ contains $p$.  The history of the
planar point location problem parallels, in many ways, the study of
binary search trees.

After a few initial attempts \cite{dl76,lp77,p81}, asymptotically
optimal (and quite different) linear-space $O(\log n)$ query time
solutions to the planar point location problem were obtained by
Kirkpatrick \cite{28}, Sarnak and Tarjan \cite{46}, Edelsbrunner
\etal\ \cite{24} in the mid 1980s.  An elegant, and optimal,
randomized solution was later given by Mulmuley \cite{34}.  Preparata
\cite{p} gives a comprehensive survey of the results of this era.

In the 1990s, several authors became interested in determining the
exact constant achievable in the query time.  Goodrich \etal\
\cite{26} gave an $O(n^2)$ size data structure that, for any query,
required the evaluation of at most $2\log n$ point-line
comparisons.\footnote{Here and throughout, logarithms are implicitly
taken modulo 2 unless otherwise specified.} Later, Adamy and Seidel
\cite{1} gave a linear space data structure that answers queries using
$\log n + 2\sqrt{\log n} + O(\log\log n)$ point-line comparisons and
showed that this result is optimal up to the third term.

Still not done with the problem, several authors considered the point
location problem under various assumptions about the query
distribution.  All these solutions compare the expected query time to
the \emph{entropy bound};  in a graph with $f$ faces, if the query point
$p$ is chosen randomly such that $p_i$ is the probability that $p$ is
contained in face $i$ of $G$, then no algorithm that makes only binary
decisions can answer queries using an expected number of decisions
that is fewer than 
\begin{equation}
    H = \sum_{i=1}^f p_i\log(1/p_i) \enspace . \eqlabel{entropy}
\end{equation}

None of the results discussed above are affected significantly by what
type of graph $G$ is.  However, when studying point location under a
distribution assumption the problem becomes more complicated and the
results become more specific.  A \emph{convex subdivision} is a plane
graph whose faces are all convex polygons, except the outer face,
which is the complement of a convex polygon.  A \emph{triangulation}
is a convex subdivision in which each face is a triangle, except the
outer face, which is the complement of a triangle.  Note that, if
every face of $G$ has a constant number of sides, then $G$ can be
augmented, by the addition of extra edges, so that it is a
triangulation without increasing \eqref{entropy} by more than a
constant.  Thus, in the following we will simply refer to results
about triangulations where it is understood that these also imply the
same result for plane graphs with faces of constant size.

Arya \etal\ \cite{XXX} showed that, for convex subdivisions, and for
distributions where the $x$ and $y$ coordinates of the query point $p$ are
independent, a data structure exists in which the expected number of
point-line comparisons is at most $4H+O(1)$ using linear space or at
most $2H+O(1)$ using quadratic space.

Iacono \cite{iXX} showed that, for triangulations, a simple variant of
Kirkpatrick's original point location structure gives a linear space,
$O(H+1)$ expected query time data structure.  Another result by Arya
\etal\ \cite{ammXX} gives a data structure for triangulations that
uses $H + O(H^{2/3}+1)$ expected number of comparisons per query and
$O(n\log n)$ space.  The space requirement of this latter algorithm
was later reduced, by the same authors, to $O(n\log^* n)$.  Finally,
the same three authors \cite{ammXX} showed that a variant of
Mulmuley's randomized algorithm gives, for triangulations, a simple
$O(H+1)$ expected query time, linear space data structure. 

The above collection of results suggest that point location, and even
distribution-sensitive point location, is a well-studied and
well-understood problem, with solutions that are optimal up to
constant factors.   However, in the above results there is a glaring
omission.  Given a convex polygon $P$, a folklore $O(\log n)$ time
algorithm exists to test if a query point $p$ is contained in $P$ and
this algorithm is optimal, in the worst case.  Testing if $p\in P$ is
a special case of point location in a convex subdivision in which the
graph has only 2 faces.  Thus, we might expect that, if $p$ is drawn
according to some distribution over $\R^2$, it may be possible to do
better in many cases. How much better?  It is certainly not possible
to achieve the entropy bound in all cases since, when $f=2$ the
entropy bound is at most 1.

In we
begin our investigation of distribution-sensitive point location 
with the fundamental problem of testing if a query point
$p$, drawn from an arbitrary distribution $D$ over $\R^2$, is
contained in a convex polygon $P$.  We describe a hierarchical
triangulation $T$ of $\R^2$ that we use to simultaneously achieve two
objectives:
\begin{enumerate}
\item $T$ is used with a query algorithm to check if a point $p$ 
	is contained in $P$, and
\item $T$ is used to give a lower bound on the expected cost of
	any linear decision tree that tests if a point $p$ selected
	according to $D$ is contained in $P$.
\end{enumerate}
The lower bound in Point~2 matches, to within a constant factor, the
expected query time of the algorithm in Point~1.  Thus, among
algorithms that can be expressed as linear decision trees, our
algorithm is optimal. Our result is the first result to give
\emph{any} lower bound on the expected complexity of \emph{any} point
location problem that exceeds the entropy bound. Proving the lower
bound is by far the hardest part of our result.  

As an easy consequence of the above result we obtain a data structure
for point location in convex subdivisions.  The expected query time of
the resulting algorithm is optimal in the linear decision tree model
of computation. Note that all known algorithms for planar point
location can be described in the linear decision tree model of
computation.  This data structure for point location in graphs whose
faces are convex polygons and where the query point is drawn according
to an arbitrary distribution is the most general result known about
planar point location and implies, to within constant factors, all the
of the results discussed above.

The remainder of this paper is organized as follows:  \Secref{prelim}
presents definitions and notations used throughout the paper.
\Secref{polygons} discusses algorithms and lower bounds for point
location in convex polygons.  \Secref{subdivisions} presents algorithms
and lower bounds for point location in convex subdivisions.  Finally,
\Secref{discussion} concludes with a discussion and points out
directions for further research.


\section{Preliminaries}
\seclabel{prelim}

In this section we give definitions, notations, and background used in
subsequent sections.

\paragraph{Triangles.}  For the purposes of this paper, a
\emph{triangle} is the common intersection of at most 3 halfplanes.
This includes triangles having 0, 1, 2, or 3, vertices.  

\paragraph{Classification Problems and Classification Trees.}

A \emph{classification problem} over a domain $\mathcal{D}$ is a
function $\mathcal{P}:\mathcal{D}\mapsto \{0,\ldots,k-1\}$.  The
special case in which $k=2$ is called a \emph{decision problem}.  A
$d$-ary \emph{classification tree} is a full $d$-ary tree\footnote{A
full $d$-ary tree is a rooted ordered tree in which each non-leaf node
has exactly $d$ children.} in which each internal node $v$ is labelled
with a function $P_v:\mathcal{P}\mapsto\{0,.\ldots,d-1\}$ and for
which each leaf $\ell$ is labelled with a value
$d(\ell)\in\{0,\ldots,k-1\}$. The \emph{search path} of an input $p$
in a classification tree $T$ starts at the root of $T$ and, at each
internal node $v$, evaluates $i=P_v(p)$ and proceeds to the $i$th
child of $v$.  We denote by $T(p)$ the label of the final (leaf) node
in the search path for $p$.  We say that the classification tree $T$
\emph{solves} the classification problem $\mathcal{P}$ over the domain
$\mathcal{D}$ if, for every $p\in \mathcal{D}$, $\mathcal{P}(p)=T(p)$.

Unless specifically mentioned otherwise, classification trees are
binary classification trees.  For a node $v$ in a (binary)
classification tree, it left child, right child, and parent,
respectively, are denoted by $\lft(v)$, $\rght(v)$ and $\prnt(v)$,
respectively.


\paragraph{Probability.}

For a distribution $D$ and an event $X$, we denote by $D_{|X}$ the
distribution $D$ conditioned on $X$.  That is, the distribution where
the probability of an event $Y$ is $\Pr(Y|X)=\Pr(Y\cap X)/\Pr(X)$.

When there is a probability measure $D$ over $\mathcal{D}$, we define
the \emph{expected search time} of a classification tree $T$ as the
expected length of the search path for $p$ when $p$ is drawn at random
from $\mathcal{D}$ according to the distribution $D$.  Note that, for
each leaf $\ell$ of $T$ there is a subset of $r(\ell)\subseteq
\mathcal{D}$ such that the search path for any $p\in r(\ell)$ ends at
$\ell$.  Thus, the expected search time of $T$ can be written as
\[
     \mu_D(T) = \sum_{\ell\in L(T)} \Pr(r(\ell))\times \depth(\ell)
	\enspace ,
\]
where $L(T)$ denotes the leaves of $T$ and $\depth(\ell)$ denotes the
length of the path from the root of $T$ to $\ell$.

The folloing theorem, which is a restatement of (part of) Shannon's
coding Theorem \cite{sXX}, is what all existing results on
distribution-sensitive planar point location use to establish their
optimality:

\begin{thm}\thmlabel{shannon}
Let $\mathcal{P}:D\mapsto \{0,\ldots,k-1\}$ be a classification
problem and let $p\in \mathcal{D}$ be selected from a distibution $D$ such
that $\Pr\{\mathcal{P}(x)= i\}=p_i$, for $0\le i< k$.  Then, any
$d$-ary classification tree $T$ that solves $\mathcal{P}$ has
\begin{equation}
     \mu_D(T) \ge \sum_{i=0}^{k-1} p_i\log_d(1/p_i) \enspace .
	\eqlabel{shannon}
\end{equation}
\end{thm}


\section{Point In Convex Polygon Testing}
\seclabel{polygons}

Let $P$ be a convex $n$-gon whose boundary is denoted by $\partial P$
and consider a probability measure $D$ over $\R^2$.  For technical
reasons, we use the convention that $P$ does not contain its boundary
so that $p\in \boundary P$ implies $p\not\in P$.  In this section we
are interested in preprocessing $P$ and $D$ so that we can quickly
solve the decision problem of testing whether a query point $p$, drawn
according to $D$, is contained in $P$. 

In particular, we are interested in algorithms that can be described
as \emph{linear decision trees}.  These are decision trees such that
each internal node $v$ contains a linear inequality $P_v(x,y)=[ax+by
\ge c]$.  We require that, for every $p\in\R^2$ the leaf reached in
the search path for $p$ is labelled with a 1 if and only if $p\in P$.
Geometrically, each internal node of $T$ is labelled with a directed
line and the decision to go to the left or right child depends on
whether p is to the left or right (or on) this line.  

Our exposition is broken up into three sections.  We begin by
describing a data structure (in fact, a decision tree) that tests if
query point $p$ is contained in a convex polygon  $P$.  Next, we give
an (easy) analysis of the expected search time of this data structure.
Finally, we give a (more difficult) proof that this expected search
time is optimal.


\subsection{Triangle Trees}

Our data structure that works by creating a sequence of successively
finer approximations $A_0,\ldots,A_k$ to $\boundary P$.  Each
approximation $A_i$ consists of two convex polygons; an \emph{outer
approximation} that contains $P$ and an \emph{inner approximation}
that is contained in $P$.

Each approximation $A_i$ is completely defined by a set $S_i$ of
$O(2^i)$ points from $\boundary P$.  The inner approximation is simply
the convex hull of $S_i$.  The outer approximation has an edge tangent
to $P$ at each of the points of $S_i$.  More precisely, for each point
$x\in S_i$ there is an edge $e$ of the outer approximation that
contains $x$.  If $x$ is in the interior of an edge of $P$ then $e$ is
contained in the same line that contains that edge. Otherwise ($x$ is
a vertex of $P$) $e$ is supported by the line containing the edge
incident to $x$ that precedes $x$ in counterclockwise order.  We ensure
that successive approximations have a containment relationship, i.e.,
$A_i\supseteq A_{i+1}$, by choosing our boundary points so that
$S_i\subseteq S_{i+1}$.

Next we define the sets $S_0,\ldots,S_k$ that define our
approximations.  The set $S_0$ is empty, and we use the convention
that the outer approximation in this case is the entire plane and the
inner approximation is the empty set. The set $S_1$ consists of any two
points, $x$ and $y$ on $\boundary P$ such that $\Pr(h_1)\le 1/2$ and
$\Pr(h_2)\le 1/2$, for each of the two open halfspaces $h_1$ and $h_2$
bounded by the line containing $x$ and $y$.

Next, we show how to obtain the set $S_i$ from the set $S_{i+1}$.  Let
$p_0,\ldots,p_{m_i-1}$ be the points in $S_i$ as they occur in
conterclockwise order around the boundary of $P$.  The approximation
$A_i$ thus consists of $m$ open triangles
$\Delta_0,\ldots,\Delta_{m_i-1}$ where $\Delta_j$ is the intersections
of the following halfspaces:
\begin{enumerate}
\item the halfspace to the right of the line through $p_j$ and $p_{j+1}$,
\item the halfspace bounded by the tangent to $P$ at $p_j$ and that
contains $p_{j+1}$, and 
\item the halfspace bounded by the tangent to $P$ at $p_{j+1}$ and
that contains $p_j$.
\end{enumerate}
Let $t_j$ be the intersection point of 
the two lines tangent to $P$ at $p_j$ and
$p_{j+1}$.\footnote{Throughout this discussion, subscripts are
implicitly taken modulo $m_i$.}  Refer to \figref{split}. For each
triangle $\Delta_j$ that is not completely contained in $P$
we add a new point to
$S_{i+1}$ as follows:  we split $\Delta_j$ into two open triangles
$\Delta_{j,0}$ and $\Delta_{j,1}$ by a line $\ell$ through $t_j$ and
such that 
\[  
     \Pr(\Delta_{j,b}) \le \Pr(\Delta_{j})/2 \enspace ,
\]
for $b=0,1$.
We then select a new point to add to $S_{i+1}$ at the intersection of
$\ell$ and $\boundary P$ that occurs in $\Delta_j$.

\begin{figure}
\begin{center}
\begin{tabular}{cc}
\includegraphics{split-a} & \includegraphics{split-b}
\end{tabular}
\end{center}
\caption{Splitting the triangle $\Delta_j$ into two triangles
$\Delta_{j,0}$ and $\Delta_{j,1}$ with a line through $t_j$.}
\figlabel{split}
\end{figure}

The entire process terminates at the first value of $k$ for which
$A_k$ is completely contained in $P$.  The approximations
$A_0,\ldots,A_k$ are stored as a binary tree $T$ that we call a
\emph{triangle tree}.  Each node $v$ of $T$ at level $i$ in $T$
corresponds to an open triangle $\Delta(v)$ in approximation $A_{i}$
and the two children of $v$ correspond to the two open triangles into
which $\Delta(v)$ is split.  

To use the tree $T$ to determine if a point $p$ is contained in $P$ we
proceed top-down, starting at the root.  For a point $p$ contained in
$\Delta(v)$ one of two things can happen: (1) $p$ is contained in one
of the two open triangles $\Delta(\lft(v))$ or $\Delta(\rght(v))$ in
which which we recurse on the right or left child of $v$,
respectively, or (2) otherwise we can determine in constant time if
$p\in P$.

\subsection{Analysis of the Triangle Tree}

Let $T$ be a triangle tree for a convex $n$-gon $P$ and a distribution
$D$.  For each internal node $v$ of $T$, we define
\[
   \Xi(v)=\Delta(v)\setminus (\Delta(\lft(v))\cup \Delta(\rght(v)))
\]
and define $\Pr(v)=\Pr(\Xi(v))$.  Notice that $\Pr(v)$ is the
probability that a search terminates at node $v$.  For a set $V$ of
nodes in $T$ we use the notation $\Pr(V)=\sum_{v\in V}\Pr(v)$ to
denote the probability that the search path ends at some node in $V$.

\begin{thm}\thmlabel{upper-bound}
Using the triangle tree $T$, the expected number of linear inequalities
required to check if $p\in P$ for a point $p$ drawn from $D$ is at
most
\[
     \mu_D(T) = O(1)\times \sum_{v\in T}\Pr(v)\times\depth (v)
      \le O(1)\times \sum_{v\in T}\Pr(v)\log(1/\Pr(v))
\]
\end{thm}

\begin{proof}
The first equality is simply by the definition of expectation.  The
second inequality follows immediately from the fact that, for any node
$v$ other than the root of $T$, $\Pr(v)\le \Pr(\Delta(v))\le
(1/2)\Pr(\Delta(\prnt(v)))$.
\end{proof}


\subsection{Optimality of the Triangle Tree}

Next we will show that the performance bound given by
\thmref{upper-bound} is optimal.  That is, that there is no linear
decision tree whose expected search time (on distribution $D$) is
asymptotically better than that of the triangle tree.
The key ingredient in our argument is the following lemma:

\begin{lem}\lemlabel{lower-bound}
Let $V$ be a subset of the vertices of the triangle tree $T$ such that
no vertex in $V$ is the descendent of any other vertex in $V$.  
Let $R=\bigcup_{v\in V} \Xi(v)$. Then, for any linear decision
tree $T'$,
\[
    \mu_{D_{|R}}(T) 
	\ge \frac{1}{6}\sum_{v\in V}\Pr(\Xi(v)\mid
R)\log(1/\Pr(\Xi(v)\mid R)) \enspace .
\]
\end{lem}

\begin{proof}
We define new model of computation, show that a lower bound in this
new model implies a lower bound in the linear decision tree model and
then prove the lower bound for the new model.  Let $S_1= \boundary P
\cup \bigcup_{v\in V} \Delta(v)$, let $S_0=S_1\cup\boundary S_1$ be
the closure of $S_0$, and let $S$ be $S\setminus\{q\}$ for some vertex
$q$ of $P$.  A \emph{chord} of $S$ is a closed line segment with both
endpoints on the boundary of $S$ and whose interior is contained in
$S$. Note that, if $c$ is a chord of $S$ then $S\setminus c$ has at
most 3 connected components $S^c_1$, $S^c_2$, and $S^c_3$.

A \emph{chord tree} is a $4$-ary decision tree that solves the problem
of testing whether a point $p\in S$ is contained in $P$.  Each
internal node $v$ of a chord tree is labelled with a chord $c(v)$ of
$S$ and the four children of $v$ correspond to the options,
$p\in S^{c(v)}_1$, $p\in S^{c(v)}_2$, $p\in S^{c(v)}_3$, and $p\in c$.
Observe that, because $S$ is bounded by two convex chains, any line
intersects $S$ in at most 2 chords.  This immediately implies the
following relationship between chord trees and linear decision trees.
\begin{clm}\clmlabel{relation}
Let $T$ be any linear decision tree that solves the problem of testing
if $p\in P$.  Then there exists a chord tree $T'$ such that 
$\mu_{D_{|R}}(T') \le 3\mu_{D_{|R}}(T)$.
\end{clm}

Now we need only prove a lower bound on $\mu_{D_{|R}}(T')$ for any
chord tree $T'$.  For two nodes $u,v\in V$, $u\neq v$, let $p$ be a
point in $\Delta(u)$ and let $q$ be a point in $\Delta(v)$.  

We claim that the search path for $p$ in $T'$ and the search path for
$q$ in $T'$ must end at different leaves of $T'$.  Assume, for the
sake of contradiction, that this is not the case and that both search
paths terminate at the leaf $\ell$. We will show that in this case
$T'$ does not solve the problem of testing if any point $p\in S$ is
contained in $P$ and thus contradicts the assumption that $T'$ is a
chord tree.  First observe that, because the search paths for $p$ and
$q$ both end at $\ell$, there exists a closed curve
$\rho$ in $S$ with one endpoint at $p$ and the other endpoint at $q$
and for which all points of $\rho$ have a search path that ends at
$\ell$.

If either $p$ or $q$ is contained in $P$ then we immediately obtain
the desired contradiction since $\rho$ must contain one of the
vertices $r$ of (e.g.) $\Delta(u)$, a point that is not in $P$ (recall
that $P$ does not contain $\boundary P$).
Otherwise, both $p$ and $q$ are not in $P$.  Then the path $\rho$
still contains one of the vertices $r$ of $\Delta(u)$.  Furthermore,
none of the vertices on the search path in $T'$ for $p$ is associated
with a chord that contains $r$ (otherwise $p$ and $q$ would not have
the same search path in $T'$).  Therefore, there is a disk $D$,
centered at $r$ and having positive radius, for which the search path
in $T'$ for any point in $D$ terminates at $\ell$.  Again, we obtain a
contradiction since the disk $D$ must contain points in the interior
of $P$.

Thus, we have established that, for any two points $p$ and $q$
contained in different triangles $\Delta(u)$ and $\Delta(v)$ the
search paths of $p$ and $q$ terminate at different leaves of $T'$.
This implies that, by labelling the leaves of $T'$ appropriately, we
obtain a $4$-ary classification tree that determines, for any
$p\in \bigcup_{v\in V} \Delta(v)$ the node $v\in V$ such
$\Delta(v)$ contains $p$.  Therefore, by \thmref{shannon},
\[
     \mu_{D_{|R}} (T') 
	\ge \sum_{v\in V}\Pr(\Xi(v)\mid R)\log_4(1/\Pr(\Xi(v)\mid R)) \enspace ,
\]
and the lemma follows from \clmref{relation}.
\end{proof}

The remainder of our argument involves constructing a lower-bound
based on \lemref{lower-bound} that matches the upper bound in
\thmref{upper-bound}.  Let 
\[
   H = \sum_{v\in V(T)} \Pr(v)\log (1/\Pr(v))
\] 
be the entropy of the distribution of search paths in $T$.  Note that
the upper bound of \thmref{upper-bound} is within a constant factor of
$H$. Thus, our goal is to show that no linear decision tree has an
expected search time in $o(H)$.

We start our analysis by partitioning the internal nodes of $T$ in
\emph{groups} $G_1,G_2,\ldots$ where
\[
	G_i = \{v\in T : 1/2^{i} \le \Pr(\Delta(v)) < 1/2^i \} \enspace .
\]
In what follows, we fix some constant $0< \alpha < 1$.  Our first
result shows that, in our lower bound, we can discard a fairly large
number of elements from each group without having much effect on the
overall entropy:

\begin{lem}\lemlabel{garbage}
For each $i$, let $G_i'$ be obtained by deleting at most $2^{\alpha
i}$ elements.  Then
\[
    \sum_{i=1}^\infty \sum_{v\in G_i'} \Pr(v)\log(1/\Pr(v)) \ge H-O(1)
	\enspace .
\]
\end{lem}

\begin{proof}
\begin{eqnarray*}
   H & = & \sum_{j=1}^n p_j\log(1/p_j) \\
   & = & \sum_{i=1}^{\infty}\sum_{v\in G_i'} \Pr(v)\log (1/\Pr(v)) +
         \sum_{i=1}^{\infty}\sum_{v\in G_i\setminus G_i'} \Pr(v)\log (1/\Pr(v)) \\
   & \le & \sum_{i=1}^{\infty}\sum_{v\in G_i'} \Pr(v)\log (1/\Pr(v)) +
         \sum_{i=1}^{\infty}\sum_{v\in G_i\setminus G_i'} (1/2^{i-1})\log (2^i) \\
   & \le & \sum_{i=1}^{\infty}\sum_{v\in G_i'} \Pr(v)\log (1/\Pr(v)) +
         \sum_{i=1}^{\infty}\sum_{v\in G_i\setminus G_i'} 2i(1/2^{i}) \\
   & \le & \sum_{i=1}^{\infty}\sum_{v\in G_i'} \Pr(v)\log (1/\Pr(v)) +
         \sum_{i=1}^{\infty} 2i(1/2^{i-\alpha}) \\
   & \le & \sum_{i=1}^{\infty}\sum_{v\in G_i'} \Pr(v)\log (1/\Pr(v)) + O(1)
\end{eqnarray*}
\end{proof}

In order to use \lemref{lower-bound} we must partition the vertices of
$T$ into sets that are compatible with the conditions of the lemma.

\begin{lem}\lemlabel{partition}
Each group $G_i$ can be partitioned into $t_i$ subgroups
$G_{i,1},\ldots,G_{i,t_i}$ such that all but one subgroup, $G_{i,t_i}$
contains at least $2^{\alpha i}/i$ elements and within any subgroup
$G_{i,j}$ with $j < t_i$ there is no pair of nodes $u,v\in G_{i,j}$
with $u$ an ancestor of $v$ in $T$.
\end{lem}

\begin{proof}
Assume that $|G_i|\ge 2^{\alpha i}$, otherwise there is nothing to
prove.  Observe that all vertices in $G_i$ appear within the first $i$
levels of $T$.  Thus, any node in $G_i$ has at most $i-1$ ancestors in
$T$.  

We can obtain the first group $G_{i,1}$ by first defining all nodes of
$G_i$ to be \emph{unmarked} and \emph{unselected}.  To obtain
$G_{i,1}$ we repeatedly \emph{select} any unselected and unmarked
node $v\in G_i$ that does not have any descendants in $G_i$ and
\emph{mark} the (at most $i-1$) ancestors of $v$ in $T$.  This
process selects at least
\[
   |G_i|/i \ge 2^{\alpha i}/i
\] 
elements to take part in $G_{i,1}$.  We can then apply this process
recursively on $G_i\setminus G_{i,1}$ to obtain the sets
$G_{i,2},\ldots,G_{i,t_i}$.
\end{proof}


\begin{thm}
Any linear decision tree $T'$ that determines whether any query point 
$p\in\R^2$ is contained in $P$ has
\[
   \mu_D(T') = \Omega(H) - O(1) \enspace .
\]
\end{thm}

\begin{proof}

Our proof is an application of the little-birdie principle.  Partition
the nodes of $T$ into the sets $G_{i,j}$ as described in
\lemref{partition}.  We work in a slightly stronger model of
computation in which we are given the tree $T$ and the partitioning of
the vertices of $T$ into the sets $G_{i,j}$ and which allows a whole
collection of decision trees $T_{i,j}$, one for each group $G_{i,j}$.
Now, when the point $p$ is selected according to $D$, a little birdie
indicates to the computer which group $G_{i,j}$ contains the vertex
$v$ of $T$ at which a search for $p$ would end.  The computer then
uses this to select the decision tree $T_{i,j}$ and uses it to
determine if $p\in P$.  The cost of this is the cost of searching in
$T_{i,j}$.  Thus, the expected cost of a search in this model of
computation is
\[
     \mu = \sum_{i=1}^\infty \sum_{j=1}^{t_i}
	\Pr\{G_{i,j}\}\mu_{D_{i,j}}(T_{i,j}) \enspace ,
\]
where $\Pr\{G_{i,j}\}$ denotes the probability of a search, in $T$,
ending at node $v\in G_{i,j}$ and $D_{i,j}$ denotes the distribution
$D$ conditioned on this event.  Clearly this model of computation is
at least as strong as the linear decision tree model since, in this
model, there is always the option of ignoring the birdie's advice by
creating a single linear decision tree $T'$ and setting $T_{i,j}=T'$
for all $i$ and $j$.

Now, applying \lemref{lower-bound} to each group $G_{i,j}$ we obtain
\begin{eqnarray*}
\mu & \ge & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i}\Pr\{G_{i,j}\}\times
	\sum_{v\in G_{i,j}}\Pr(v\mid G_{i,j})\log(1/\Pr(v\mid G_{i,j})) \\
& \ge & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i-1}\Pr\{G_{i,j}\}\times
	\sum_{v\in G_{i,j}}\Pr(v\mid G_{i,j})\log(1/\Pr(v\mid G_{i,j})) \\
& = & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i-1}
	\sum_{v\in G_{i,j}}\Pr(v)\log(\Pr\{G_{i,j}\}/\Pr(v)) \\
& \ge & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i-1}
	\sum_{v\in G_{i,j}}\Pr(v)(\log(1/\Pr\{v\})+ \log(\Pr\{G_{i,j}\}) \\
& \ge & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i-1}
	\sum_{v\in G_{i,j}}\Pr(v)(\log(2^{i-1}) + \log(2^{\alpha i}/i2^{i}) \\
& \ge & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i-1}
	\sum_{v\in G_{i,j}}\Pr(v)(i-1 + \alpha i -i -2\log i) \\
& \ge & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i-1}
	\sum_{v\in G_{i,j}}\Pr(v)\times \Omega(i) \\
& \ge & \sum_{i=1}^{\infty}\sum_{j=1}^{t_i-1}
	\sum_{v\in G_{i,j}}\Omega(\Pr(v)\log (1/\Pr(v)) \\
& \ge & \Omega(H) -O(1)
\end{eqnarray*}
\end{proof}

\section{Convex Subdivisions}
\seclabel{subdivisions}

In this section we consider the problem of point location in convex
subdivisions. Our data structure is simple.  For each face $F_i$ of
the convex subdivision we triangulate $F_i$ using the triangles of a
triangle tree $T_i$ for the polygon $F_i$ and the distribution
$D_{|F_i}$.  Next, we preprocess the resulting triangulation using any
of the distribution-sensitive point location data structures for
triangulations referenced in \secref{intro}.

The resulting data structure has a query time of 
\begin{eqnarray*}
time & = & \sum_{i=1}^f 
	\sum_{v\in T_i} \Pr(v)\log(1/\Pr(v)) \\
\end{eqnarray*}


\section{Discussion}
\seclabel{discussion}




\end{document}
