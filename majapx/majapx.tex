\documentclass{patmorin}
\usepackage{amsthm,amsmath}
\usepackage{pat}


\newcommand{\eps}{\varepsilon}

\title{\MakeUppercase{Approximating Majority Depth}}
\author{Dan Chen and Pat Morin}

\begin{document}
\maketitle

\begin{abstract}
We consider algorithms for approximating the majority depth (Liu and
Singh, 1993) of a point with respect to an $n$-point set, $S$.  At the
heart of our this approximation is a data structure that can preprocess
$n$ lines in $O(n\log n)$ time and can test, in $O(n^{1/6})$ expected
time, whether a randomly selected vertex in the arrangement of these
lines is above or below the median level.
\end{abstract}

\section{Introduction}

Let $S$ be a set of $n$ points in $\R^2$.  For a pair $x,y\in S$,
the \emph{major side} of $x,y$ is the union of the (at most 2) closed
halfplanes with $x$ and $y$ on their boundary that contain at least
$n/2$ points of $S$.  The \emph{majority depth} \cite{ls93,s91} of a
point $q\in\R^2$ with respect to $S$ is defined as the number of pairs
$x,y\in S$ that have $q$ in their major side.

Under the usual projective duality \cite{e97}, the set $S$ becomes a
set, $S^*$, of lines; pairs of points in $S$ becomes vertices in the
arrangement, $A(S^*)$, of $S^*$; $q$ becomes a line, $q^*$; and the
majority depth is the number of vertices $z$ in $A(S^*)$ such that
\begin{enumerate}
\item $z$ is above $q^*$ and $z$ is above the median level; or
\item $z$ is below $q^*$ and $z$ is below the median level.
\end{enumerate}

Chen and Morin \cite{cm11} present an algorithm for computing majority
depth that works in the dual.  Their algorithm works by computing the
\emph{median level} of $S^*$; that is, the set of points on lines in
$S$ that have exactly $\lfloor n/2\rfloor$ lines of $S$ above them.
Their algorithm computes the intersections of $q^*$ with the median
level and uses fast inversion counting to determine the number, $r$,
of vertices of the arrangement sandwiched between $q^*$ and the median
level.  The majority depth of $q$ is then equal to $\binom{n}{2}-r$.
The running time of this algorithm is within a logarithmic factor of $m$,
the complexity of the median level.

The worst-case complexity the median level of $n$ lines has been the
subject of intense study since it was first posed.  The current best upper
bound is $O(n^{4/3})$, due to Dey \cite{d98} and the current best lower
bound is $2^{\Omega(\sqrt{\log n})}$, due to T\'oth \cite{t00}.  Thus, the
worst-case running time of our majority depth algorithm is $\Omega(n\log^c
n)$ for any constant $c$, but no worse than $O(n^{4/3}\log n)$.
The median level can be computed in time $O(\min\{m\log n,n^{4/3}\})$
by an algorithm whose decisions can be modelled as an algebraic decision
tree \cite{c99,bj02} and even in $O(m(\log n)/(\log\log n))$ time in the
word-RAM model \cite{dp07}.

It seems difficult for any algorithm that computes the exact majority
depth of a point to avoid (at least implicitly) computing the median
level of $A(S^*)$.  Therefore, in this paper, we turn to approximation by
random sampling.  In particular, we use the simple technique of sampling
vertices of the arrangement and checking whether
\begin{enumerate}
  \item each sample lies above or below $q^*$; and
  \item each sample lies above or below the median level of $S^*$.
\end{enumerate}

The former test takes constant time but the latter test leads to a
data structuring problem:  Preprocess the set $S^*$ so that one can
quickly test, for any query point, $z$, whether $z$ is above or below
the median level of $q^*$.   We know of two immediate solutions to this
problem.  The first solution is to compute the median level explicitly,
in $O(\min\{m\log n,n^{4/3}\})$ time, after which any query can be
answered in $O(\log n)$ time by binary search on the x-coordinate of $z$.
The second solution is to construct a half-space range counting structure
(a partition tree) in $O(n\log n)$ time that can answer these queries
in $O(n^{1/2})$ time \cite{c12}.

The first solution is not terribly good, since Chen and Morin's algorithm
shows that computing the \emph{exact} majority depth of $q$ can be done
in time that is within a logarithmic factor of $m$, the complexity of
the median level.  (Though if the goal is to preprocess in order to
approximate the majority depth for many different points, then this
method may be the right choice.)

In this paper, we show that the second solution can be improved
considerably, at least for our application.  In particular, we show that
when the query point $z$ is a randomly chosen vertex of the arrangement
$A(S^*)$, partition trees can be used to answer queries in $O(n^{1/6})$
expected time.  This faster query time means that we can use more random
samples which leads to a more accurate approximation.

The remainder of this paper is organized as follows.  In
\secref{range-counting} we review Chan's optimal partition trees
and show how they can be used for approximate range counting.
In \secref{fast-testing} we show how these approximate range
counting results can be used to quickly answer queries about whether
a random vertex of $S^*$ is above or below the median level of $S^*$.
In \secref{majority-depth} we briefly mention how all of this applies
to the problem of approximating majority depth.

\section{Approximate Range Counting}
\seclabel{range-counting}

In this section, we show that Chan's optimal partition trees \cite{c12},
which are designed for exact range counting, are also good for approximate
range counting.\footnote{We note that by no means are Chan's partition
trees the only range counting structure we could use.  They do, however,
offer the current base construction-time/query-time combination.}
A partition tree, $T=T(S)$, is a rooted $b$-ary tree that can be
constructed in $O(n\log n)$ time and that has the following properties
\cite[Theorems~3.2 and 4.2]{c12}:

\begin{enumerate}
  \item each node, $v$, of the tree is associated with a triangle $\Delta(v)$;
  \item for each node, $v$ and each descendant, $w$, of $v$,
    $\Delta(w)\subseteq\Delta(v)$;
  \item for any two nodes $v$ and $w$ at the same level, $\Delta(v)$
    and $\Delta(w)$ are disjoint; and
  \item For any line $\ell$, and any integer
    $k\in\{1,\ldots,n/\log^{\omega(1)} n\}$, the number of nodes, $v$,
    such that $\Delta(v)$ intersects $\ell$ and $|\Delta(v)\cap S|\ge k$
    is $O((n/k)^{1/2})$.

    Furthermore, this set of nodes can be found in $O((n/k)^{1/2})$ time.
\end{enumerate}

Applying Property~4 with $k=1$ is what allows for range counting in
$O(n^{1/2})$ time.  The following lemma shows that, by choosing different
values of $k$ we can peform approximate range counting more quickly.

\begin{lem}\lemlabel{approx-count}
  Given any closed halfplane $h$, and any integer $i$, $1\le i\le
  n/\log^{\omega(1)} n$, a partition tree $T$ can be used to determine,
  in $O(\min\{n^{1/2},n/i\})$ time, a value $n_h$ such that 
  \[ |h\cap S| \le n_h \le |h\cap S|+i \enspace .\]
\end{lem}

\begin{proof}
  Consider performing a standard range-counting  search in $T$, starting
  at the root and recursing on any node, $v$, such that (1)~the
  interior of $\Delta(v)$ intersects the bounding line of $h$ and
  (2)~$|\Delta(v)\cap S| > k$, for a value $k$ to be determined shortly.

  By the fourth property of partition trees, this search takes
  $O((n/k)^{1/2})$ time and the leaves of this recursion are a set, $V$,
  of $c(n/k)^{1/2}$ nodes.  For each $v\in V$, $\Delta(v)$ contains at
  most $k$ points of $S$, for a total of at most $c(nk)^{1/2}$ points.
  These are the only points for which the search has not determined if
  they are contained in $V$ or not.  Therefore, stopping at this point,
  the algorithm can return a value that estimates $|h\cap S|$ to within
  an additive error of $c(nk)^{1/2}$.  Choosing $k$ to be the maximum
  integer such that $i \le c(nk)^{1/2}$ completes the proof.
\end{proof}

We remark that this lemma compares favourably with the use of (the most
basic) $\eps$-approximations.  Another way to do approximate range
counting with an error of $i$ is to compute an $(i/n)$-approximation
of the points, which has size $O((n/i)^2)$ \cite{lls01,t94}.
By then building a partition tree of size $O((n/i)^2)$ on this
$(i/n)$-approximation, one obtains a data structure that does approximate
range-counting with an error of $i$, has size $O((n/i)^2)$, and can
answer queries in $O(n/i)$ time.

A more sophisticated form of $\eps$-approximation, whose size is only
$O((n/i)^{4/3}\log n)$ leads to faster query algorithms.  This is
discussed in \secref{faster}.


The following result shows that \lemref{approx-count} gives a method of
testing if a point $x$ is above or below the median level; the running
time is sensitive to the distance (measured in levels) between $x$
and the median level.

\begin{lem}\lemlabel{side-test}
  Testing if a point, $x$, which is on the $n/2-i$ level or the $n/2+i$
  level, is above or below the median level of $S$ can be done in time
  \[
       Q(i) = \begin{cases}
          O(n^{1/2}) & \text{if $|n/2-i| \le n^{1/2}$} \\
          O(n/i)      & \text{if $n^{1/2} \le |n/2-i| \le n^{1-\eps}$} \\
          O(n^{\eps}) & \text{otherwise.}
       \end{cases}
  \]
\end{lem}

\begin{proof}
  Assume, without loss of generality that $x$ is on the $n/2-i$
  level.  We begin by setting $i'= n^{1-\eps}$ and performing
  an approximate range counting query with error $i'=n^{1-\eps}$
  using \lemref{approx-count}.  If $i \ge n^{1-\eps}$, then this
  determines that the level of $x$ is no more than $n/2$ and we are done.
  This first query takes $O(n^{\eps})$ time, so this satifies the
  third case in the running time requirement.

  If $i < i'$, then we halve the value of $i'$ and try again, performing
  an approximate range counting query with error $i$.  This continues
  until either,
  \begin{enumerate}
    \item We reach a value $i' < i$, and determine that $x$ is above the
      median level; or
    \item The value of $i'$ falls below $n^{1/2}$, in which case we perform
      an exact range counting query.
  \end{enumerate}
  In the first case, the final query takes $O(n/i)$ time. In the second
  case, the final query takes $O(n^{1/2})$ time.  In both cases, the cost
  of all queries leading up the final query form an exponential increasing
  sequence, so the total cost of all queries is $O(\max\{n/i,n^{1/2}\})$,
  as required.
\end{proof}

We remark that the proof of the \lemref{side-test} involves performing
up to $\Theta(\log n)$ queries in a partition tree.  In practice, these
queries are related and one can start the query for the next value $i'$
by exploring the nodes of the tree where the recursion bottomed out
(because these nodes contain fewer than $2i'$ points).  In this way,
the entire query can be implemented using a list in combination with
depth-first-search on subtrees to implement a form of approximate breadth
first search.

\section{Side of Median Level Testing}
\seclabel{fast-testing}

We are now ready to tackle the main problem that comes up in trying to
estimate majority depth by random sampling:  Given a random vertex, $x$,
in the arrangement of $S^*$, determine whether $x$ is above or below
the median level of $S^*$.  Before proving our main theorem, we recall
a result of Dey \cite[Theorem~4.2]{d98} about the maximum complexity of
a sequence of levels.

\begin{lem}\lemlabel{dey}
 Let $L$ be any set of $n$ lines and let $r$ be the number of vertices
 in the arrangement of $L$ that are on levels $k,k+1,\ldots,k+j$.  Then,
 $r \in O(k^{4/3}j^{2/3})$.
\end{lem}

We are interested in the special case of \lemref{dey} where $k=n/2-i$
and $j=2i$:

\begin{cor}\corlabel{dey}
  Let $L$ be any set of $n$ lines.  Then, for any $i\in\{1,\ldots,n/2\}$
  the maximum total number of vertices of $A(L)$ whose level is in
  $\{n/2-i,\ldots,n/2+i\}$ is $O(n^{4/3}i^{2/3})$.
\end{cor}

\begin{thm}\thmlabel{exp-side-test}
  Given any set, $L$, of $n$ lines, there exists an $O(n)$ space data
  structure, that can be constructed in $O(n\log n)$ time that can test
  if a point $x$ is above or below the median level of $A(L)$.  When given
  a random vertex of $A(L)$ as a query, the expected query time of this
  structure is $O(n^{1/6})$.
\end{thm}

\begin{proof}
  The data structure is, of course, a partition tree and the query
  algorithm is the algorithm given by \lemref{side-test}.  Let $n_i$
  be the number of vertices of $A(L)$ on levels $n/2-i$ and $n/2+i$.  Then
  the expected query time of this data structure is at most
  \begin{equation}
    F(n_0,\ldots,n_{n/2}) 
      = \binom{n}{2}^{-1}\sum_{i=0}^{n/2} n_i Q(i) \enspace ,
     \eqlabel{expected}
  \end{equation}
  where, for sufficiently large $n$, $Q(i)$ is upper-bounded by
  \[
        Q(i) \le \begin{cases}
          cn^{1/2} & \text{if $|n/2-i| \le n^{1/2}$} \\
          cn/i      & \text{if $n^{1/2} < |n/2-i| \le n^{1-\eps}$} \\
          cn^{\eps} & \text{otherwise,}
        \end{cases}
  \]
  for some constant $c>0$.   Our goal, therefore, is to
  upper-bound $F(n_0,\ldots,n_{n/2})$ subject to Dey's constraints:
  \[
     \sum_{i=0}^{j}n_i \le dn^{4/3}j^{2/3} 
  \]
  for some constant $d>0$ and all $i\in\{0,\ldots,n/2\}$.

  Working in our favour is that $Q(i) \ge Q(i')$ for all $i \le i'$.
  This implies that, to obtain an upperbound on $F(n_0,\ldots,n_{n/2})$,
  we can set
  \begin{equation}
      \sum_{i=0}^{j}n_i = dn^{4/3}j^{2/3}   \eqlabel{deys-equality}
  \end{equation}
  for all $i\in\{0,\ldots,n/2\}$.  To see why this is so, suppose we
  have a sequence $S=n_0,\ldots,n_{n/2}$ that satisfies Dey's constraints
  but for which 
  $\sum_{i=0}^{j}n_i < dn^{4/3}j^{2/3}$ for some value $j$. Then the sequence
  \[
     S'=n_0,\ldots,n_{j}+\delta,n_{j+1}-\delta,\ldots,n/2
  \]
  where $\delta=dn^{4/3}j^{2/3}-\sum_{i=0}^{j}n_i$, also satisifies
  Dey's constraints.  Furthermore, 
  \[  F(S')-F(S) = \delta Q(j) - \delta Q(j+1) \ge 0 \enspace , \]
  so $F(S')\ge F(S)$.  Repeatedly applying this type of modification (or
  using induction) shows that the sequence $S=n_0,\ldots,n_{n/2}$ that
  satisifies \eqref{deys-equality} is a sequence that maximizes $F(S)$.

  Finally, we can solve for the sequence that satisfies
  \eqref{deys-equality} to obtain $n_i \le \alpha n^{4/3}/i^{1/3}$, for some
  constant $\alpha>0$.  Plugging this back into \eqref{expected} yields
  \begin{align}
     F(n_0,\ldots,n_{n/2}) 
      & \le \binom{n}{2}^{-1}\sum_{i=0}^{n/2} \alpha (n^{4/3}/i^{1/3})Q(i) \notag \\
      & = \binom{n}{2}^{-1}\sum_{i=0}^{n^{1/2}} O(n^{4/3}/i^{1/3})cn^{1/2}  \eqlabel{header}\\
      & \qquad {} + \binom{n}{2}^{-1}\sum_{i=n^{1/2}+1}^{n^{1-\eps}} (\alpha n^{4/3}/i^{1/3})(cn/i)  \eqlabel{middle} \\
      & \qquad {} + \binom{n}{2}^{-1}\sum_{i=n^{1-\eps}+1}^{n/2} O(n^{4/3}/i^{1/3})cn^{\eps} \eqlabel{trailer} \enspace .
  \end{align}
  Recall that $\int_1^n x^{-1/3}\,\mathrm{d}x = \frac{3}{2}(n^{2/3}-1)$.
  Using this integral allows us to bound the sums in \eqref{header}
  and \eqref{trailer} as follows:
  \begin{align*}
    \eqref{header} & = \binom{n}{2}^{-1}\sum_{i=0}^{n^{1/2}} O(n^{4/3}/i^{1/3})cn^{1/2} \\
        & = c\binom{n}{2}^{-1}O(n^{4/3}(n^{1/2})^{2/3})n^{1/2}
           & \text{(bounding by integral)}  \\
        & = O\left(\frac{n^{1/2}}{n^{1/3}}\right) \\
        & = O(n^{1/6})
  \end{align*}
  and 
  \begin{align*}
    \eqref{trailer} 
      & \le \binom{n}{2}^{-1}\sum_{i=0}^{n/2} O(n^{4/3}/i^{1/3})n^{\eps} \\
      & \le \binom{n}{2}^{-1}\sum_{i=0}^{n/2} O(n^{4/3}n^{2/3})n^{\eps} 
                & \text{(bounding by integral)} \\
        & = O(n^{\eps})
  \end{align*}
  \Eqref{middle} is a little more delicate.  To bound it, we
  use $\int_1^n x^{-4/3}\,\mathrm{d}x = 3-3n^{-1/3}$, as follows:
  \begin{align*}
    \eqref{middle} 
      & = \binom{n}{2}^{-1}
       \sum_{i=n^{1/2}+1}^{n/2} (\alpha n^{4/3}/i^{1/3})(cn/i) \\
      & = \binom{n}{2}^{-1}
       c\alpha n^{7/3}\left(\sum_{i=n^{1/2}+1}^{n/2} i^{-4/3} \right) \\
      & \le \binom{n}{2}^{-1}
       c\alpha n^{7/3}\int_{n^{1/2}}^{n} i^{-4/3}\,\mathrm{d}i \\
      & = \binom{n}{2}^{-1} 
       c\alpha n^{7/3}\left(\int_{1}^{n} i^{-4/3}\,\mathrm{d}i
         - \int_{1}^{n^{1/2}} i^{-4/3}\,\mathrm{d}i  \right) \\
      & = \binom{n}{2}^{-1} 
       3c\alpha n^{7/3}\left(n^{-1/6} - n^{-1/3}\right) \\
      & \le \binom{n}{2}^{-1} 
       3c\alpha n^{7/3}n^{-1/6} \\
      & = \binom{n}{2}^{-1} O(n^{13/6}) \\
      & = O(n^{1/6}) \enspace .
  \end{align*}
  To summarize, the expected running time of the query algorithm is at most
  \[ 
     F(n_1,\ldots,n_{n/2}) \le \eqref{header} + \eqref{middle} + \eqref{trailer}
     = O(n^{1/6}) \enspace . \qedhere
  \]
\end{proof}


\section{Faster Queries}
\seclabel{faster}

In this section, we show that the expected query time of $O(n^{1/6})$
can be reduced further, to $\log^{O(1)} n$, even with a linear-space
data structure.  The cost of this, however, is preprocessing time;
the preprocessing algorithm has a running time that is only polynomial
in $n$.  The data structure is based on the use of the following theorem
\cite{c04,mww93}:

\begin{thm}[Matou\v{s}ek, Welzl, and Wernisch 1993]\thmlabel{approx}
  For any set $S$ of $n$ points in the plane, and any $i\in\{1,\ldots,n\}$,
  there exists a subset $S'\subseteq S$ so that, for any halfplane $h$,
  \[  \left|\frac{|h\cap S|}{|S|} - \frac{|h\cap S'|}{|S'|}\right| \le \frac{i}{n} \] 
  and $|S'| = O((n/i)^{4/3}(\log (n/i))^{2/3})$.  Furthermore, the set $S'$
  can be computed in time polynomial in $n$.
\end{thm}

\newcommand{\Oh}{\tilde{O}}

To avoid the typical log pile, we will use a the notation $\Oh(f(n))$
to mean the set $O(f(n)(\log n)^c)$ for some constant $c \ge 0$.
\thmref{approx} allows us to estimate $|h\cap S|$ to within an error of
$i$ by computing $|h\cap S'|$.  If we build a partition tree on $S'$,
computing $|h\cap S'|$ can be done in time
\[
 \Oh(((n/i)^{4/3})^{1/2}) 
    = \Oh((n/i)^{2/3})
\]
Observe that we can construct a sequence $S_0,\ldots,S_r$ of these
$(i/n)$-approximations, where $S_j$ is constructed with the
value $i=2^jn^{1/4}/\log n$.  The total size of these sets is
$O(|S_0|)=O(n)$.  Using these approximations in exactly the same way
as \lemref{approx-count} is used in the proof of \lemref{side-test},
we obtain the following version of \lemref{side-test}:

\begin{lem}\lemlabel{side-test2}
  Testing if a point, $x$, which is on the $n/2-i$ level or the $n/2+i$
  level, is above or below the median level of $S$ can be done in time
  $Q(i) = O(\min\{n^{1/2},(n/i)^{2/3}\})$.
\end{lem}

Plugging this data structure into the same machinery used to prove
\thmref{exp-side-test} we obtain the following result:

\begin{thm}
  Given any set, $L$, of $n$ lines, there exists an $O(n)$ space data
  structure, that can be constructed in time polynomial in $n$ that can test
  if a point $x$ is above or below the median level of $A(L)$.  When given
  a random vertex of $A(L)$ as a query, the expected query time of this
  structure is $\log^{O(1)} n$.
\end{thm}

\begin{proof}[Proof Sketch]
  The proof follows along the same lines as the proof of
  \thmref{exp-side-test}, except that the sum that bounds the expected
  running time becomes:
  \begin{align*}
     F(n_0,\ldots,n_{n/2})
       & \le \binom{n}{2}^{-1}\left(\sum_{i=0}^{n^{1/4}}\Oh(n^{4/3}n^{1/2}/i^{1/3})
      + \sum_{i=n^{1/4}+1}^{n/2}\Oh(n^{4/3}(n/i)^{2/3}/i^{1/3}) \right) \\
     & \le \binom{n}{2}^{-1}\left(\Oh(n^{4/3}n^{1/2}(n^{1/4})^{2/3})
      + \sum_{i=n^{1/4}+1}^{n/2}\Oh(n^{2}/i) \right) \\
     & \le \binom{n}{2}^{-1}\left(\Oh(n^2) + \Oh(n^{2}) \right) 
       \qquad \text{(since the second sum is harmonic)} \\
     & \le \Oh(1) \enspace ,
  \end{align*}
  as required.
\end{proof}


\section{Estimating Majority Depth}
\seclabel{majority-depth}

Finally, we return to our application, namely estimating majority depth. 

\begin{thm}
  Given a set $S$ of $n$ points in $\R^2$, there exists a data structure
  that can preprocess $S$ in $O(n\log n)$ time such that, for any point
  $q$, the data structure can compute, in $O(rn^{1/6})$ time, a value
  $d'(q,S)$ such that $\E[d'(q,S)]=d(q,S)$ and
  \[
     \Pr\left\{|d'(q,S)-d(q,S)| \ge \eps d(q,S)\right\} 
        \le \exp\left(-\Omega\left(\eps^2rp\right)\right) \enspace ,
  \]
  where $p=d(q,S)/\binom{n}{2}$ is the normalized majority depth of $q$.
\end{thm}

\begin{proof}
  Let $p=d(q,S)/\binom{n}{2}$.
  Select $r$ random vertices of $A(S^*)$ (by taking random pairs of lines
  in $S^*$) and, for each sample, test if it contributes to $d(q,S)$.  This
  yields a count $r' \le r$ where
  \[ 
     \E[r'] = rp \enspace .
  \]
  We then return the value $d'(q,S)=(r'/r)\binom{n}{2}$, so that
  $\E[d'(q,S)]=d(q,S)$, as required.

  To prove the error bound, we use the fact that $r'$ is a binomial$(p,r)$ random variable.  Applying Chernoff's Bounds on $r'$ yields:
  \[
     \Pr\{|r' - rp| \ge \eps rp\} \le e^{-\Omega(\eps^2rp)} \enspace ,
  \]
  as required.
\end{proof}

\section{Conclusions}

Although the estimation of majority depth is the original motivation for
studying this problem, the underlying question of the tradeoffs involved
in preprocessing for testing whether a point is above or below the median
level seems a fundamental question that is still far from answered.  In particular, what is the answer to the following question:

\begin{op}
What is the fastest linear-space data structure for testing if an
arbitrary query point query point $q$ is above or below the median level
of a set of $n$ lines?
\end{op}

To the best of our knowledge, the current state of the art is partition
trees, which can only answer these queries in $O(n^{1/2})$ time.

\bibliographystyle{plain}
\bibliography{majapx}

\end{document}


