\documentclass[lotsofwhite,charterfonts]{patmorin}
\usepackage{amsopn,graphicx}
\input{pat}

\DeclareMathOperator{\lft}{left}
\DeclareMathOperator{\rght}{right}
\DeclareMathOperator{\prnt}{parent}
\DeclareMathOperator{\tp}{up}
\DeclareMathOperator{\bttm}{down}
\newcommand{\depth}{d}


\title{\MakeUppercase{Biased Range Trees}}
\author{Vida Dujmovi\'c
	\and John Howat
	\and Pat Morin}

\begin{document}
\maketitle
\begin{abstract}
A data structure, called a \emph{biased range tree}, is presented that
preprocesses a set $S$ of $n$ points in $\R^2$ and a query
distribution $D$ for 2-sided orthogonal range counting queries.  The
expected query time for this data structure, when queries are drawn
according to $D$, is proportional to that of the optimal decision tree
for $S$ and $D$.   The memory and preprocessing requirements of the
data structure are $O(n\log n)$.
\end{abstract}

\section{Introduction}

Let $S$ be a set of $n$ points in $\R^2$ and let $D$ be a probability
measure over $\R^2$.  A \emph{2-sided orthogonal range counting query}
over $S$ asks, for a query point $q=(q_1,q_2)$, to report the number
of points $(p_1,p_2)\in S$ such that $p_1 \ge q_1$ and $p_2 \ge q_2$.
A 2-sided range counting query \emph{has distribution $D$} if the
query point $q$ is chosen from the probability measure $D$.  If $T$ is
a data structure for answering 2-sided range counting queries over $S$
then we denote by $\mu_D(T)$ the expected time, using $T$, to
answering a range query with distribution $D$.  The current paper is
concerned with preprocessing the pair $(S,D)$ to build a data
structure $T$ that minimizes $\mu_D(T)$.

\subsection{Previous Work}

The general topic of geometric range queries is a field that has seen
an enormous amount of activity in the last century.  Results in this
field depend heavily on the types of objects the data structure stores
and on the shape of the query ranges.  In this section we only mention
a few data structures for orthogonal range counting and semigroup
queries in 2 dimensions.  The interested reader is directed to the
excellent, and easily accessible, survey by Agarwal and Erickson
\cite{ea99}.

Orthogonal range counting is a classic problem in computational
geometry.  The 2- (and 3- and 4-) sided range counting problem can be
solved by Bentley's \emph{range trees} \cite{ae42}.  Range trees use
$O(n\log n)$ space and can be constructed in $O(n\log n)$ time.
Originally, range trees answered queries in $O(\log^2 n)$ time.
However, with the application of fractional cascading \cite{ae76,ae196} the
query time can be reduced to $O(\log n)$.  Range trees can also answer
more general \emph{semigroup queries} in which each point of $S$ is
assigned a weight from a commutative semigroup and the goal is to
report the weight of all points in the query range \cite{ae133,ae292}.

For range counting queries, Chazelle \cite{ae55,ae58} proposes a data
structure of size $O(n)$, that can be constructed in $O(n\log n)$
time, and that can answer range couting queries in $O(\log n)$ time.
Unfortunately, this data structure is not capable of answering
semigroup queries in the same time bound.  For semigroup queries in
the RAM model, Chazelle provides data structures with the following
requirements: (1)~$O(n)$ space and $O(\log^{2+\epsilon} n)$ query
time, (2)~$O(n\log\log n)$ space and $O(\log^{2}n\log\log n)$ query
time, and (3)~$O(n\log^\epsilon n)$ space and $O(\log^2 n)$ query
time.

Practical linear space data structures for range counting include
$k$-d trees \cite{ae41}, quad-trees \cite{ae251}, and their
variants.  These structures are practical in the sense that they are
easy to implement and use only $O(n)$ space.  Unfortunately, neither
of these structures has a worst-case query time of $O(\log^{O(1)} n)$.
Thus, in terms of query time, $k$-d trees and quad-trees are nowhere
near competitive with range trees.

Chazelle \cite{ae61} has proven lower bounds on the time-space
tradeoffs associated with orthogonal range searching in the semigroup
model.  For orthogonal range search in $d$ dimensions, he has shown
that any data structure that uses $m$ units of memory has a worst-case
query time of at least $\Omega((\log n/(\log 2m/n))^{d-1})$.

\subsection{New Results}

In the current paper we present a data structure, the \emph{biased
range tree}, of size $O(n\log n)$, that can be constructed in $O(n\log
n)$ time, and that answers range counting (or semigroup) queries in
$O(\mu_D(T^*))$ expected time, where $T^*$ is any comparison tree that
answers range counting queries over $S$.  In particular, $T^*$ could
be the comparison tree that minimizes $\mu_D(T^*)$ implying that the
expected query time of our data structure is as fast as the fastest
comparison-based data structure for answering range counting queries
over $S$.  Moreover, the worst-case search time of biased range trees
is $O(\log n)$, matching the worst-case performance of range trees.

Note that we do not place any restrictions on the comparison tree $T$.
Biased range trees, while requiring only $O(n\log n)$ space, are
competitive with any comparison-based data structure, without any
limits on the space required by that data structure.  Thus, the memory
requirement of biased range trees is the same as that of range trees
but their query time can never be any worse.
 
The remainder of the paper is organized as follows. In
\secref{preliminaries} we present background material that is used in
subsequent sections.  In \secref{data-structure} we define the biased
range trees. In \secref{lower-bound} we prove that biased range trees
are optimal.  In \secref{summary} we recap, summarize, and describe
directions for future work.

\section{Preliminaries}
\seclabel{preliminaries}


In this section we give definitions, notations, and background
that are prerequisites for subsequent sections.

\paragraph{Rectangles.}

For the purposes of the current paper, a \emph{rectangle}
$R(a,b,c,d)$ is defined as
\[
    R(a,b,c,d) = \{ (x,y) : \mbox{$a\le  x \le b$ and $c \le y \le d$}\}
	\enspace .
\]
We also allow unbounded rectangles by setting $a,c=-\infty$ and/or
$b,d=\infty$.  Therefore, under this definition, rectangles can have
0, 1, 2, 3, or 4 sides.  For a query point $q=(q_1,q_2)$ we denote the
by $R(q)$ the query range $R(q_1,\infty,q_2,\infty)$.  A
\emph{horizontal strip} is rectangle of the form
$R(-\infty,\infty,a,b)$ and a \emph{vertical strip} is a rectangle of
the form $R(a,b,-\infty,\infty)$.

\paragraph{Classification Problems and Classification Trees.}

A \emph{classification problem} over a domain $\mathcal{D}$ is a
function $\mathcal{P}:\mathcal{D}\mapsto \{0,\ldots,k-1\}$.  The
special case in which $k=2$ is called a \emph{decision problem}.  A
$d$-ary \emph{classification tree} is a full $d$-ary tree\footnote{A
full $d$-ary tree is a rooted ordered tree in which each non-leaf node
has exactly $d$ children.} in which each internal node $v$ is labelled
with a function $P_v:\mathcal{D}\mapsto\{0,.\ldots,d-1\}$ and for
which each leaf $\ell$ is labelled with a value
in $\{0,\ldots,k-1\}$. The \emph{search path} of an input $q$
in a classification tree $T$ starts at the root of $T$ and, at each
internal node $v$, evaluates $i=P_v(q)$ and proceeds to the $i$th
child of $v$.  We denote by $T(q)$ the label of the final (leaf) node
in the search path for $q$.  We say that the classification tree $T$
\emph{solves} the classification problem $\mathcal{P}$ over the domain
$\mathcal{D}$ if, for every $q\in \mathcal{D}$, $\mathcal{P}(p)=T(p)$.

The particular type of classification trees we are concerned with are
\emph{comparison trees}.  These are binary classification trees in
which the function $P_v$ at each node $v$ compares either the first or
second coordinate of $q$ to a fixed value (that may depend on the
point set $S$ and the distribution $D$).  For the problem of 2-sided
range counting over $S$, the leaves of $T$ are labelled with values in
$\{0,\ldots,n\}$ and $T(q)$ is the number of points in $S$ in
the query range $R(q)$.


\paragraph{Probability.}

For a probability measure $D$ and an event $X$, we denote by $D_{|X}$ the
distribution $D$ conditioned on $X$.  That is, the distribution where
the probability of an event $Y$ is $\Pr(Y\mid X)=\Pr(Y\cap X)/\Pr(X)$.
The probability measures used in this paper are usually defined over
$\R^2$.  We make no assumptions about how these measures are
represented, but we assume that an algorithm can, in constant time,
given a rectangle $r$, determine $\Pr(r)$.

For a classification tree $T$ that solves a problem
$P:\mathcal{D}\mapsto\{0,\ldots,k-1\}$ and a probability measure $D$
over $\mathcal{D}$, the \emph{expected search time} of $T$ is the
expected length of the search path for $q$ when $q$ is drawn at random
from $\mathcal{D}$ according to $D$.  Note that, for each leaf $\ell$
of $T$ there is a maximal subset $r(\ell)\subseteq \mathcal{D}$ such
that the search path for any $q\in r(\ell)$ ends at $\ell$.  Thus, the
expected search time of $T$ (under distribution $D$) can be written as
\[
     \mu_D(T) = \sum_{\ell\in L(T)} \Pr(r(\ell))\times \depth(\ell)
	\enspace ,
\]
where $L(T)$ denotes the leaves of $T$ and $\depth(\ell)$ denotes the
length of the path from the root of $T$ to $\ell$.  Note that, for comparison
trees, the closure of $r(\ell)$ is always a rectangle.

The following theorem is a restatement of (half of) Shannon's
Fundamental Theorem for a Noiseless Channel \cite[Theorem 9]{s48}.

\begin{thm}\thmlabel{shannon}
Let $\mathcal{P}:\mathcal{D}\mapsto \{0,\ldots,k-1\}$ be a classification
problem and let $p\in \mathcal{D}$ be selected from a distibution $D$ such
that $\Pr\{\mathcal{P}(p)= i\}=p_i$, for $0\le i< k$.  Then, any
$d$-ary classification tree $T$ that solves $\mathcal{P}$ has
\begin{equation}
     \mu_D(T) \ge \sum_{i=0}^{k-1} p_i\log_d(1/p_i) \enspace .
	\eqlabel{shannon}
\end{equation}
\end{thm}

In terms of range counting, \thmref{shannon} immediately implies that,
if $p_i$ is the probability that the query range contains $i$ points
of $S$, then any binary decision tree $T$ that does range counting has
$\mu_D(T) \ge \sum_{i=0}^{n} p_i\log(1/p_i)$.  Unfortunately for us,
this lower bound is too weak and, in general, there is no decision
tree whose performance matches the entropy lower bound.
Instead, we will prove that any comparison tree for range counting
can be used to solve a more difficult classification problem.

\paragraph{Biased Search Trees.}

\emph{Biased search trees} are a classic data structure for solving
the following 1-dimensional problem:  Given an increasing sequence of
real numbers $X=\langle x_0=-\infty,x_1,x_2,\ldots ,
x_n,x_{n+1}=\infty\rangle$ and a probability distribution $D$ over
$\R$, construct a binary search tree  $T=T(X,D)$ so that, for any
query value $q$ drawn from $D$, one can quickly find the unique
interval $[x_i,x_{i+1})$ containing $q$.  If $p_i$ is the probability
that $q\in[x_1,x_{i+1})$ then the expected number of comparisons
performed while searching for $q$ is given
by
\[
   \mu_D(T) \le \sum_{i=1}^{n} p_i\log(1/p_i) + 1 
\]
and the tree $T$ can be constructed in $O(n)$ time \cite{m75}.
Clearly, by \thmref{shannon}, the query time of this binary search
tree is optimal up to an additive constant term.  Note that, by having
each node of $T$ store the size of its subtree, a biased search tree
can count the number of elements of $X$ in the interval
$I(q)=[q,\infty)$ without increasing the search time by more than a
constant factor.  Thus, biased search trees are an optimal data
structure for 1-dimensional range counting.

\section{Biased Range Trees}
\seclabel{data-structure}

In this section we describe the biased range tree data structure,
which has three main parts: the backup tree, the primary tree, and a
set of catalogues that adorn the nodes of the primary tree.

\subsection{The Backup Tree}

In trying to achieve optimal query time, biased range trees will try
to quickly answer queries that are, in some sense, easy.  In some
cases, a query is difficult and our data structure can not answer it
in $o(\log n)$ time.  For these queries, our data structure keeps a
\emph{backup} range tree that stores the points of $S$ and can answer
any 2-sided range query in $O(\log n)$ worst-case time.  The
preprocessing time and space requirements of this backup tree are
$O(n\log n)$.

\subsection{The Primary Tree}

Like a range tree, a biased range tree is a two level structure
consisting of a primary tree whose nodes store secondary structures.
However, in a range tree the primary tree is a binary search tree that
discriminates based only on the first coordinate of the query point
$q$.  In order to achieve optimal expected query time, this turns out
to be insufficient, so instead biased range trees use a $k$-d tree
as the primary tree.

Each node $v$ of $T$ is associated with a rectangle $r(v)$.  The
rectangle associated with the root of $T$ is all of $\R^2$. The two
children of a node $v$ are associated with the two rectangles obtained
by removing an open horizontal strip or vertical strip from
$r(v)$ depending on whether the distance from $v$ to the root of $T$
is even or odd, respectively.  We call a node $v$ at even distance
from the root a \emph{vertical node}, otherwise we call $v$ a
\emph{horizontal node}.  We denote by $s(v)$ the strip that is removed
from $r(v)$ to make its two children.  For a vertical (respectively,
horizontal) node $v$, we use the convention that
$s(v)$ includes its left (respectively, bottom) side but not its
right (respectively, top) side.

Refer to \figref{left-right}.
For a vertical node $v$, we denote its children by $\lft(v)$ and
$\rght(v)$ and call them the \emph{left child} and \emph{right child}
of $v$, depending on which side of the vertical strip (left or right)
they are.  For uniformity, we will also call the children of a node
$v$ that is split with a horizontal strip $\lft(v)$ and $\rght(v)$
depending on whether the child is below or above the strip,
respectively.  Similarly, the left and right boundaries of a strip
$s(v)$ at a horizontal node $v$ refer to the bottom and top sides of
$s(v)$.  Note that, with these conventions, if the query point $q$ is
in $r(\lft(v))$ then $R(q)$ intersects $r(\rght(v))$.  However, if
$q\in r(\rght(v))$ then $R(q)$ does not intersect $r(\lft(v))$.
Similarly, for a query $q$ in the interior of $s(v)$, the query range
$R(q)$ intersects the right boundary of $s(v)$ but not the left
boundary.

\begin{figure}
  \begin{center}
    \begin{tabular}{cc}
      \includegraphics{left-right-a} & \includegraphics{left-right-b} \\
      (a) & (b)
    \end{tabular}
  \end{center}
  \caption{The splitting of (a)~a vertical node $v$ and (b)~a horizontal
  node $v$.}
  \figlabel{left-right}
\end{figure}

The strip $s(v)$ is selected as the unique maximal strip containing no
point of $r(v)\cap S$ in its interior and such that each of the two
components of $r(v)\setminus s(v)$ has probability at most
$\Pr(r(v))/2$.  In the degenerate case when $s(v)$ is a line, the two
rectangles $r(\lft(v))$ and $r(\rght(v))$ overlap.  In this case, we
use the convention that $\rght(v)$ ``owns'' the overlap so that
$r(\lft(v))$ is open on the side that it shares with $\rght(v)$.  This
splitting process is applied recursively to any node $v$ as long as
$r(v)$ contains more than one point of $S$ and $\depth(v)\le \log n$.
These rules guarantee that a node $v$ at level $i$ in $T$ has
$\Pr(r(v))\le 1/2^i$.

The leaves of $T$ are partitioned into two classes; the \emph{bad}
leaves are those leaves $v$ such that $|r(v)\cap S| > 1$ and the
remaining leaves are the \emph{good} leaves.  We will use the
convention that, at a leaf, $s(v)=r(v)$ so that, for any point
$q\in\R^2$ there is exactly one node $v\in T$ such that $q\in s(v)$.
Finally, we use the convention that, if $v$ has no children then
$\lft(v)=\rght(v)= v$.

\subsection{The Catalogues}

The nodes of the tree $T$ are augmented with additional data
structures called \emph{catalogues} that hold subsets of $S$.  Each
node $v$ has two catalogues, $C_x(v)$ and $C_y(v)$ that store subsets
of $S$ sorted by their first, respectively, second, coordinate. 
If $v$ is a horizontal node, then $C_x(\lft(v))=
r(\rght(v))\cap S$ and $C_y(v)=\emptyset$.  If $v$ is a
vertical node, then $C_y(\lft(v)) = r(\rght(v))\cap S$ and
$C_y(\rght(v))=\emptyset$.  This defines the catalogues of all nodes
except the root of $T$ which has $C_x(v)=C_y(v)=\emptyset$. See
\figref{catalogues}.

\begin{figure}
  \begin{center}
    \begin{tabular}{c|c}
      \includegraphics{catalogue-horizontal} &
        \includegraphics{catalogue-vertical} \\
       (a) & (b) 
    \end{tabular}
  \end{center}
  \caption{The catalogues of (a)~a horizontal node $v$ and (b)~a
  vertical node $v$.}
  \figlabel{catalogues}
\end{figure}

Consider any node $v$ that is not a bad leaf and any point $q\in
r(v)$.  Let $v_1,\ldots,v_k$ denote the path from $\lft(v)$ to the
root of $T$.  Then the catalogues of $v_1,\ldots,v_k$ have the
following properties (see \figref{3d-view}):
\begin{figure}
  \begin{center}
    \includegraphics{3d-view}
  \end{center}
  \caption{The area covered by catalogues on the path $v$ to the root
of $T$. The $\times$ symbol shows the location of the query point $q$.}
  \figlabel{3d-view}
\end{figure}

\begin{enumerate}
\item The points in the catalogues of $v_1,\ldots,v_k$ are above
or to the right of $q$.  For each $1\le i \le k$, all points in $C_y(v_i)$, respectively,
$C_x(v_i)$ have their first, respectively, second, coordinate greater
than the first, respectively, second, coordinate of $q$.

\item All catalogues at nodes in $v_1,\ldots,v_k$ are disjoint.  That,
is, for each $1\le i\le j \le k$,
$C_x(v_i)\cap C_x(v_j) = \emptyset$,
$C_y(v_i)\cap C_y(v_j) = \emptyset$,
$C_x(v_i)\cap C_y(v_j) = \emptyset$, and
$C_x(v_j)\cap C_y(v_i) = \emptyset$.

\item The catalogues at nodes $v_1,\ldots,v_k$ contain all points in
the query range $R(q)$.  That is,
\[
     R(q)\cap S \subseteq \bigcup_{i=1}^k \left(C_x(v_i)\cap C_y(v_i)\right)
        \enspace . 
\]
\end{enumerate}

%\begin{figure}
%
%\caption{The catalogues of $T$ at (a)~a horizontal node and (b)~a
%vertical node.}
%\figlabel{catalogues}
%\end{figure}

To speed up the process of navigation in $T$, fractional cascading
\cite{ae76} is applied to the catalogues of $T$.  If $v$ is a
horizontal node, then (a fraction of) all the data in $C_x(v)$ is
cascaded into both $C_x(\lft(v))$ and $C_x(\rght(v))$.  However,
$C_y(v)$ is partitioned into two sets.  If $h(v)$ denotes the smallest
horizontal slab that that contains $r(v)$, then $C_y(v)$ is
partitioned into the two sets $h(\rght(v))\cap C_y(v)$ and
$h(\lft(v))\cap C_y(v)$ and (a fraction of) these is cascaded into
$\lft(v)$, respectively, $\rght(v)$.  Similarly, if $v$ is a vertical
node, then (a fraction of) all the data in $C_y(v)$ is cascaded into
both $C_y(\lft(v))$ and $C_y(\rght(v))$ but the data in $C_x(v)$ is
partitioned before being cascaded into $C_x(\lft(v))$ and
$C_x(\rght(v))$.

Finally, each catalogue $C_x(v)$ and $C_y(v)$ is indexed by a biased
binary search tree $T_x(v)$, respectively, $T_y(v)$.  The weight of an
interval $[x_1,x_2)$ in $T_x(v)$, respectively, $T_y(v)$ is given by
the probability that the first, respectively, second, coordinate of
$q$ is in the interval $[x_1,x_2)$, when $q$ is drawn according to the
distribution $D_{\mid s(\prnt(v))}$.


\subsection{The Query Algorithm}

The algorithm to answer a 2-sided range query $q=(q_1,q_2)$ proceeds
in three steps:

\begin{enumerate}

\item The algorithm navigates the tree $T$ from top to bottom to
locate the unique node $v$ such that $q\in s(v)$. This step takes
$O(d_T(q))$ time, where $d_T(q)$ is the depth of the node $v$.  If $v$
is a bad leaf (so $d_T(q)\ge \log n$) then the algorithm performs a
range query in $O(\log n)$ time using the backup range tree and the
query algorithm does not execute the next two steps.

\item  Otherwise, the algorithm uses $T_x(\lft(v))$ and
$T_y(\lft(v))$ to locate $q_1$ and $q_2$, respectively, in the catalogues
$C_x(\lft(v))$ and $C_y(\lft(v))$, respectively.

\item The algorithm walks back from $v$ to the root of $T$, locating
$q$ in the catalogues of all nodes on this path and computing the
results of the range counting query as it goes.  Thanks to fractional
cascading, each step of this walk can be done in constant time, so the
overall time for this step is also $O(d_T(q))$.
\end{enumerate}

Observe that Steps~1 and 3 of the query algorithm each take
$O(d_T(q))$ time.  The time needed to accomplish Step~2 of the
algorithm depends on exactly what is in the catalogues $C_x(v)$ and
$C_y(v)$, and will be the first quantity we study in the next section.

\section{Optimality of Biased Range Trees}
\seclabel{lower-bound}

In this section we show that the expected query time of biased range
trees is as good as the expected query time of any comparison tree.
The expected query time has two components.  The first component is
the expected depth, $d_T(q)$,  of the node $v$ such that $s(v)$
contains $q$.  The second component is the expected cost of locating
$q$ in the catalogues of $\lft(v)$.  We will show that each of these two
components is a lower bound on the expected cost of any decision tree
for two-sided range searching on $S$ where queries come from
distribution $D$.  In order to save on notation in this section we
will use the convention $\Pr(v)=\Pr(s(v))$ is the probability that a
search terminates at node $v$ of $T$.

\subsection{The Catalogue Location Step}

First we show that the expected cost of locating $q$ in the two
catalogues, $C_x(\lft(v))$ and $C_y(\lft(v))$, of the leaf $v$
with $q\in r(v)$ is
a lower bound on the expected cost of any decision tree for answering
2-sided range queries in $S$.  The intuition behind this proof is
that, in order to correctly answer range counting queries, any decision tree
for range counting must locate the $x$-coordinate of $q$
with respect to the $x$-coordinates of all points above $q$.  
Similarly, it must locate the $y$-coordinate of $q$ with respect to
the $y$-coordinates of all points to the right of $q$.  The structure
of the catalogues ensures that biased range trees do this in the most
efficient manner possible.
 

\begin{lem}\lemlabel{cataloguer}
Let $T^*$ be any decision tree for 2-sided range counting in $S$ and let
$C_2(S,D)$ denote the expected cost of locating $q$ in Step 2 of the
biased range tree query algorithm on a biased range tree $T=T(S,D)$. 
Then
\[
  \mu_D(T^*) = \Omega(C_2(S,D)) \enspace .
\] 
\end{lem}

\begin{proof}
We first observe that, by definition,
\[
  C_2(S,D) =  \sum_{v\in T} 
              \Pr(v)\left( \mu_{D_{\mid s(v)}}(T_x(\lft(v)))
                               +  \mu_{D_{\mid s(v)}}(T_y(\lft(v))) \right)
           \enspace .
\]
Consider some node $v$ of $T$.  For a point $q\in s(v)$, all of the
points in $T_x(v)$ are points that may or may not be in the query
range $R(q)$ depending on where exactly $q$ is located within $s(v)$.
This implies that, if $T^*$ correctly answers range queries for every
point $q\in s(v)$ then it must determine the location of the
$x$-coordinate of $q$ with respect to all points in $T_x(v)$.  More
precisely, the leaves of $T^*$ could be relabelled to obtain a
comparison tree that determines, for any $q\in s(v)$, which interval
of $T_x(v)$ contains $q_1$.  Since $T_x(\lft(v))$ is a biased search tree,
for the probability measure $D_{\mid s(v)}$, this implies that
\[
  \mu_{D_{\mid s(v)}}(T^*) \ge \mu_{D_{\mid s(v)}}(T_x(\lft(v))) - 1\enspace .
\]
Similarly, the same argument applied to $T_y(v)$ yields 
\[
  \mu_{D_{\mid s(v)}}(T^*) \ge \mu_{D_{\mid s(v)}}(T_y(\lft(v))) - 1\enspace .
\]
We can now complete the proof with
\begin{eqnarray*}
\mu_D(T^*) 
 & = & \sum_{v\in T} \Pr(v)\cdot\mu_{D_{\mid s(v)}}(T^*) \\
 & \ge & \sum_{v\in T}
	\Pr(v) \cdot\max\left\{\mu_{D_{\mid s(v)}}(T_x(\lft(v))), 
		       \mu_{D_{\mid s(v)}}(T_y(\lft(v)))\right\} - 1 \\
 & \ge & \sum_{v\in T}
	\frac{1}{2}\Pr(v)\cdot\left( \mu_{D_{\mid s(v)}}(T_x(\lft(v)))
                             +  \mu_{D_{\mid s(v)}}(T_y(\lft(v))) \right) - 1 \\
 & = & \frac{1}{2}\cdot C_2(S,D) - 1 = \Omega(C_2(S,D)) \enspace .
\end{eqnarray*}
\end{proof}

\subsection{The Tree Searching Step}

Next we bound the expected depth of the node $v$ of $T$ such that
$q\in s(v)$.  We do this by showing that any decision tree $T^*$ for
range counting in $S$ must solve a set of point location problems and
that the expected depth of $v$ is a lower bound on the complexity of
solving these problems.

We say that a set of rectangles is \emph{HV-independent} if no
horizontal or vertical line intersects more than one rectangle in the
set.  We say that a set $\{v_1,\ldots,v_k\}$ of nodes in $T$ is
\emph{HV-independent} if the set $\{r(v_1),\ldots,r(v_k)\}$ is
HV-independent.

\begin{lem}\lemlabel{independent}
Let $T=T(S,D)$ be a biased range tree and label each node of $T$ white
or black.  There exists constants $\alpha,\gamma\in(1,2)$ and $x>1$ such that,
if $T$ contains more than $\gamma^i$ white nodes at level at most $i$
then there is an HV-independent set of white nodes at level at most $i$
of size at least $\alpha^{i-x}$.
\end{lem}

\begin{proof}
\end{proof}

We can now provide the second piece of the lower bound.

\begin{lem}\lemlabel{depth}
Let $T^*$ be any comparison tree that does range counting over $S$. Let
$C_1(S,D)$ denote the expected depth of the node $v$ of the biased
range tree $T=T(S,D)$ such that $q\in s(v)$.  Then
\[
    \mu_D(T^*) = \Omega(C_1(S,D))
\]
\end{lem}


\begin{proof}

Partition the nodes of $G$ into groups $G_1,G_2,\ldots$ where $G_i$
contains all nodes $v$ such that $1/2^{i} \le \Pr(v) \le 1/2^{i-1}$.
Observe that the nodes in group $G_i$ occur in the first $i$ levels of
$T$.  Select a constant $\beta$ with $\gamma < \beta < 2$.  By
repeatedly applying \lemref{independent}, each group $G_i$ can be
partitioned into groups $G_{i,1},\ldots,G_{i,t_i}$ where, for each $1
\le j < t_i$, $G_{i,j}$ is an HV-independent set with $|G_{i,j}|
\ge \alpha^i$.  Furthermore, $|G_{i,t_i}| \le \gamma^i$. (Note that
$G_{i,t_i}$ is not necessarily HV-independent.)

Consider some group $G_{i,j}$ for $1\le j < t_i$.  Let $x$ be a leaf
of $T^*$ and observe that, because the nodes in $G_{i,j}$ are
independent and each one contains at least one point of $S$, there are
at most 4 nodes $v$ in $G_{i,j}$ such that $r(x)$ intersects $r(v)$.
(Otherwise $r(x)$ contains a point of $S$ in its interior and
therefore does not solve the range counting problem for $S$.)
By performing two additional comparisons, $T^*$ can be used to
determine which node of $v\in G_{i,j}$ (if any) contains the query point
$q$ in $s(v)$.  
However, $G_{i,j}$ contains $\alpha^i$ nodes and the search path
for $q$ terminates at each of these with probability between $1/2^i$
and $1/2^{i-1}$.
Therefore, if we denote by $D_{i,j}$ the distribution $D$ conditioned
on the search path for $q$ terminating in one of the nodes in
$G_{i,j}$ then we have, by applying \thmref{shannon},
\begin{eqnarray*}
   \mu_{D_{i,j}}(T^*) + 2 
    & \ge & \sum_{v\in G_{i,j}}\Pr(v\mid G_{i,j})\log(1/\Pr(v\mid G_{i,j}) \\
    & \ge & \sum_{v\in G_{i,j}}\Pr(v\mid G_{i,j})\log(\alpha^i/2) \\
    & \ge & \log(\alpha^i/2) \\
    & = & i\log\alpha - 1 \enspace .
\end{eqnarray*}

Putting this all together, we obtain
\begin{eqnarray*}
\mu_D(T^*) 
  & = & \sum_{i=1}^{\ceil{\log n}}\sum_{j=1}^{t_i}\Pr(G_{i,j})\mu_{D_{i,j}}(T^*) \\
  & \ge & \sum_{i=1}^{\ceil{\log n}}
    \sum_{j=1}^{t_i-1}\Pr(G_{i,j})\mu_{D_{i,j}}(T^*) \\
  & \ge & \sum_{i=1}^{\ceil{\log n}}\sum_{j=1}^{t_i-1}\Pr(G_{i,j})( i\log\alpha -3) \\
  & \ge & (\log\alpha)\cdot
         \sum_{i=1}^{\ceil{\log n}}\sum_{j=1}^{t_i-1}
		\sum_{v\in G_{i,j}}\Pr(v)\cdot\depth(v) - 3\\
  & = & (\log\alpha)\cdot\sum_{v\in T}\Pr(v)\cdot \depth(v)
          -    \sum_{i=1}^{\ceil{\log n}}
		\sum_{v\in G_{i,t_i}}\Pr(v)\cdot\depth(v) - 3\\
  & \ge & (\log\alpha)\cdot\sum_{v\in T}\Pr(v)\cdot \depth(v)
          -    \sum_{i=1}^{\ceil{\log n}}i\cdot\Pr(G_{i,t_i}) - 3 \\
  & \ge & (\log\alpha)\cdot\sum_{v\in T}\Pr(v)\cdot \depth(v)
          -    \sum_{i=1}^{\ceil{\log n}}i\gamma^i/2^{i-1} - 3 \\
  & \ge &  (\log\alpha)\cdot\sum_{v\in T}\Pr(v)\cdot \depth(v) - O(1) \\
  & = & \Omega(C_1(S,D)) \enspace ,
\end{eqnarray*}
where the last inequality follows from the fact that $\gamma/2 < 1$. 
\end{proof}

And now the main event:

\begin{thm}
Let $T=T(S,D)$ be a biased range tree and let $T^*$ be any decision
tree that answers range counting queries for $S$.  Then
\[
  \mu_D(T^*) = \Omega(\mu_D(T)) \enspace .
\]
\end{thm}

\begin{proof}
By the definition of $C_1$ and $C_2$, the expected cost of searching in
$T$ is $\mu_D(T)=O(C_1(S,D)+C_2(S,D))$.  On the other hand, by
\lemref{depth} and \lemref{cataloguer} $\mu_D(T^*) =
\Omega(\max\{C_1(S,D),C_2(S,D)\}) =
\Omega(C_1(S,D)+C_2(S,D))=\Omega(\mu_D(T))$.  This completes the proof.
\end{proof}

\section{Summary, Discussion, and Conclusions}
\seclabel{summary}

We have presented biased range trees, an optimal data structure for
2-sided orthogonal range counting queries when the point set $S$ and
query distribution $D$ is known in advance. The expected time required
to answer queries with this data structure is within a constant factor
of the best decision tree.  Like standard range trees, biased range
trees use $O(n\log n)$ space and can also answer semigroup queries
\cite{ae133,ae292}.

As a small optimization, biased range trees could eliminate the need
for a separate backup range tree. Instead, once the probability of a
node $v$ drops below $1/n$ the node can be split by ignoring the
distribution $D$ and simply splitting the points of $r(v)\cap S$ into
two sets of roughly equal size.  This results in a tree of depth at
most $2(\log n+1)$.

This work is just one of many possible results on
distribution-sensitive range searching.  Several open problems
immediately arise from this work.

\begin{op}
Are there efficient distribution-sensitive data structures for 3-sided
and 4-sided range queries?
\end{op}

\begin{op}
A point $q$ is \emph{maximal} if $R(q)\cap S=\emptyset$.  Is there a
distribution sensitive structure for testing if $q$ is maximal?  For
point sets in 2-dimensions, a variant of the point-location techniques
of Collette \etal\ \cite{cdilm08} seem to apply.  What about in dimensions
$d\ge 3$?
\end{op}

\begin{op}
Biased range trees require that $S$ and $D$ be known in advance.  Is
there a self-adapting version of biased range trees that, without
knowing $D$ in advance, can answer $m$ queries, each drawn
independently from $D$ in $O(n\log n+ m\mu_D(T^*))$ expected time?
\end{op}

\begin{op}
Determine the worst-case or the average case constants associated with
2-d orthogonal range searching for comparison-based data structures.
The results of Adamy and Seidel \cite{as98} on point location  imply an
$O(n^2)$ space data structure that answers queries using at most
$2\log n + O(\log\log n)$ comparisons.  Is there an $O(n\log n)$ space
structure with the same performance?
\end{op}

\bibliographystyle{plain}
\bibliography{rangesearch}


\end{document}
