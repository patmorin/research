\documentclass{DIKU-article}
%\usepackage{charter}
\usepackage[dvips]{graphicx}

%\renewcommand{\baselinestretch}{1.3}
%\input{pat.txt}
\newcommand{\thmref}[1]{Theorem~\ref{theorem:#1}}
\newcommand{\comment}[1]{}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Search}{\mbox{$\mathit{search}$}}
\newcommand{\sequence}[1]{\left\langle#1\right\rangle}

\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\Erase}{\mbox{$\mathit{erase}$}}
\newcommand{\Findmin}{\mbox{$\mathit{find}$\textnormal{-}}\allowbreak{}\mbox{$\mathit{minimum}$}}
\newcommand{\Findmax}{\mbox{$\mathit{find}$\textnormal{-}}\allowbreak{}\mbox{$\mathit{maximum}$}}
\newcommand{\Member}{\mbox{$\mathit{find}$\textnormal{-}}\allowbreak{}\mbox{$\mathit{member}$}}
\newcommand{\Predecessor}{\mbox{$\mathit{find}$\textnormal{-}}\allowbreak{}\mbox{$\mathit{predecessor}$}}
\newcommand{\Successor}{\mbox{$\mathit{find}$\textnormal{-}}\allowbreak{}\mbox{$\mathit{successor}$}}
\newcommand{\Insert}{\mbox{$\mathit{insert}$}}
\newcommand{\Extract}{\mbox{$\mathit{extract}$}}
\newcommand{\Extractmin}{\mbox{$\mathit{extract}$\textnormal{-}$\mathit{minimum}$}}
\newcommand{\Extractmax}{\mbox{$\mathit{extract}$\textnormal{-}$\mathit{maximum}$}}
\newcommand{\Meld}{\mbox{$\mathit{meld}$}}
\newcommand{\Merge}{\mbox{$\mathit{merge}$}}
\newcommand{\Split}{\mbox{$\mathit{split}$}}
\newcommand{\Decrease}{\mbox{$\mathit{decrease}$}}
\newcommand{\Begin}{\mbox{$\mathit{begin}$}}
\newcommand{\End}{\mbox{$\mathit{end}$}}
\newcommand{\Borrow}{\mbox{$\mathit{borrow}$}}

\newcommand{\seclabel}[1]{\label{sec:#1}}
\newcommand{\secref}[1]{\mbox{Section~\ref{sec:#1}}}

\newcommand{\Null}{\mbox{$\mathit{null}$}}
\newcommand{\Operator}{\mbox{$\mathit{operator}$}}
\newcommand{\Pred}{\mathrm{pred}}
\newcommand{\Succ}{\mathrm{succ}} 
\newcommand{\Elem}{\mathrm{elem}} 
\newcommand{\Size}{\mathrm{size}}

\newcommand{\ceils}[1]{\lceil #1 \rceil}

\titlehead{Putting your data structure on a diet}
\authorhead{Herv\'e Br\"onnimann, Jyrki Katajainen, and Pat Morin}

\title{Putting your data structure on a diet}

\author{Herv\'e Br\"onnimann\inst{1}
\and
Jyrki Katajainen\inst{2}\fnmsep\thanks{%
Partially supported by the Danish Natural Science
Research Council under contract 272-05-0272
(project ``Generic programming---algorithms and tools'').%
}
\and
Pat Morin\inst{3}%
}

\institute{Department of Computer and Information Science, Polytechnic University\\
Six Metrotech, Brooklyn NY 11201, USA; \email{hbr@poly.edu}
\and
Department of Computing, University of Copenhagen\\
Universitetsparken 1, 2100 Copenhagen East, Denmark; \email{jyrki@diku.dk}
\and
School of Computer Science,
Carleton University\\
1125 Colonel By Drive,
Ottawa, Ontario, Canada K1S 5B6; 
\email{morin@cs.carleton.ca}%
}

\dates{CPH STL Report 2006-X, October 2006. Version: \today }

\begin{document}
\maketitle
\begin{abstract}
Consider a data structure $\mathcal{D}$ that stores a dynamic set or
multiset of elements. Assume that $\mathcal{D}$ uses a linear number
of words in addition to the elements stored.  In this paper several
data-structural transformations are described which can be used to
transform $\mathcal{D}$ into another data structure $\mathcal{D}'$
which has the same functionality as $\mathcal{D}$, has considerably
smaller memory overhead than $\mathcal{D}$, and performs the supported
operations by a small constant factor or a small additive term slower
than $\mathcal{D}$, depending on the data structure and operation in
question. The compaction technique has been successfully applied for
linked lists, ordered dictionaries, unordered dictionaries,
single-ended priority queues, and double-ended priority queues.

%Consider a data structure D that stores a dynamic set or multiset of
%elements. Assume that D uses a linear number of words in addition to
%the elements stored.  In this paper several data-structural
%transformations are described which can be used to transform D into
%another data structure D' which has the same functionality as D, has
%considerably smaller memory overhead than D, and performs the
%supported operations by a small constant factor or a small additive
%term slower than D, depending on the data structure and operation in
%question. The compaction technique has been successfully applied for
%linked lists, ordered dictionaries, unordered dictionaries,
%single-ended priority queues, and double-ended priority queues.
%
\end{abstract}

\begin{keywords}
Data structures, lists, dictionaries, priority queues, space efficiency
\end{keywords}

\section{Introduction}
\seclabel{intro}

In this paper we consider the space efficiency of data structures that
can be used for maintaining a dynamic set or multiset of
elements. Earlier research in this area has concentrated on the space
efficiency of some specific data structures or on the development of
implicit data structures. Our focus is on data-structural
transformations that can be used to transform a data structure
$\mathcal{D}$ into another data structure $\mathcal{D}'$ that has the
same---or augmented---functionality as $\mathcal{D}$, has about the same efficiency as
$\mathcal{D}$, but uses significantly less space than $\mathcal{D}$.
In particular, we consider a compaction technique that can be
applied with minor variations to several different data structures.

One particular aspect of concrete implementations of element containers
that has received much attention is the issue of \emph{memory
overhead}, which is the amount of storage used by a data
structure beyond what is actually required to store the elements manipulated.
The starting point for our research was the known implicit data
structures where the goal is to reduce memory overhead to $O(1)$ words
or $O(\lg n)$ bits, where $n$ denotes the number of elements stored.  A
classical example is the binary heap of Williams \cite{Wil64} which is a
priority queue supporting the methods \Findmin{}, \Insert{}, and
\Extractmin{}. Another example is the searchable heap of Franceschini
and Grossi \cite{FG03} which is an ordered dictionary supporting
methods \Member{}, \Predecessor{}, \Successor{}, \Insert{}, and
\Erase{}. A searchable heap is a complicated data structure and
because of the bit encoding techniques used it can only store a set,
not a multiset.

One should observe that implicit data structures are
designed on the assumption that there is an infinite array available
to be used for storing the elements. In other words, it is assumed
that the whole memory is used for the data structure alone and no
other processes are run simultaneously. To relax this assumption,
a resizable array could be used instead.  It is known that any
realization of a resizable array requires at least $\Omega(\sqrt{n})$
space for pointers and/or elements \cite{BCDMS99}, and realizations
exist that only require $O(\sqrt{n})$ extra space \cite{BCDMS99,KM01}.
In \cite{BCDMS99}, it was shown that a space-efficient resizable array
can perform well under a realistic model of dynamic memory
allocation. In particular, the waste of memory due to internal
fragmentation is $O(\sqrt{n})$ even though external
fragmentation can be large \cite{LL85}.

We call a data structure \emph{elementary} if it only allows
key-based access to elements. In particular, all
implicit data structures are elementary.  An important requirement often
imposed by modern libraries (e.g.~C\texttt{++} standard library
\cite{ISO}) is to provide locator-based access to elements, as well as
to provide iterators to iterate over a subset of elements. In
general, for efficiency reasons it might be advantageous to allow
application programs to maintain references, pointers, locators, or iterators to
elements stored inside a data structure. For the sake of correctness,
it is important to keep these external references valid at all times. We call
this issue \emph{referential integrity}. For non-elementary priority queues,
locator-based access is essential since without it methods \Erase{}
or \Decrease{} cannot be provided. For ordered
dictionaries, a finger can be seen as a special locator, so to
support finger search locator-based access must be possible.  In the
widely-used textbook
\cite{CLRS01}, \emph{handles} are proposed as a general solution to
achieve referential integrity but this technique 
alone increases the memory overhead of the data structure in
question by $2n$ pointers.

For fundamental data structures, like ordered dictionaries and
priority queues studied in this paper, many implementations
are available, each having own advantages and disadvantages.  In
earlier research, the issues considered include the repertoire of
methods supported, complexity of implementation, constants involved
in running times, worst-case versus average-case versus amortized
performance guarantees, memory overhead, and so on.  Many
implementations are also special because they provide certain
\emph{properties}, not provided by implicit data structures.  Examples
of such properties include the working-set property \cite{i01,st85b},
the queueish property \cite{il02}, the unified property
\cite{i01,st85b}, the ability to \Insert{} and \Erase{} in constant
time \cite{Fle96,hm82,LO88}, and the (static or dynamic) finger
property \cite{as96,bt80,c95,gmpr77,hm82}.  These special properties
are a large part of the reason that so many implementations of
fundamental data structures exist.  Many algorithmic applications require
data structures that have one or more of these special properties to ensure the
bounds on their running times or storage requirements.  Thus, it is
not always possible to substitute one implementation for another.

Several ordered-dictionary implementations, including variants of
height-balanced search trees \cite{Bro79}, self-adjusting search trees
\cite{And99,gr93,st85b}, and randomized search trees \cite{as96,p90},
have been proposed that reduce the extra storage to two pointers per
element. However, when an ordered dictionary has to support
locator-based access to elements, additional pointers (like parent
pointers) are often needed. By using the child-sibling representation
of binary trees (see, e.g.~\cite[Section 4.1]{Tar83}) and by
compacting the bits describing the type and colour of nodes in pointers (as proposed, for example, in
\cite{BK06}), red-black trees supporting locator-based access are
obtained that only use $2n + O(1)$ words of memory in addition to the
$n$ elements stored.

In contrast with fully implicit binary heaps, 
many priority-queue implementations rely on
pointers. Actually, pointer-based implementations are often necessary
to provide referential integrity, and to support \Erase{} and
constant-time \Decrease{}. Of the known implementations, all need at
least $2n$ extra words \cite{DW93,MP05}, $3n$ extra words
\cite{EJK05}, or more \cite{DGST88,KST02}. In particular, observe that
in \cite{MP05} the data structure was claimed to be implicit, but it
actually uses $2n + O(1)$ words of extra storage. It was also claimed
that the space efficiency cannot be improved if \Decrease{} is to be
supported in constant time. This claim is in a direct contradiction to
the results presented in the present paper (cf.~Corollary
\ref{corollary:queue}).

\noindent\textbf{The tone of the above paragraph is a bit strong for
my taste.  I'd like to read \cite{MP05} before I pass judgement
though.}

In this paper we elaborate upon a technique that can be used to make a
pointer-based data structure that uses $cn$ words of extra storage,
for a positive constant $c$, into an equivalent data structure that
uses either $\epsilon n$ or $(1 + \varepsilon) n$ words of extra
storage, for any $\varepsilon>0$ and sufficiently large $n >
n(\varepsilon)$.  By applying this technique, any application program
can employ the data structure (and its properties) without worrying
about the memory overhead. In some special cases, we can even make the
amount of extra space needed as small as $O(n/\lg n)$.

The compaction technique used by us is simple, and similar to the idea
used in degree-balanced search trees \cite{hm82} where each node
contains at least $a$ elements and at most $b$ elements for
appropriately chosen $a$ and $b$. That is, instead of operating on
elements themselves, we operate on groups of elements which we call
\emph{chunks}. \comment{Each chunk has a \emph{representative} and the
underlying data structure operates on these representatives. The
elements in a chunk determine an \emph{interval} of values.} When the
compaction technique is applied to different data structures, there
are minor variations in the scheme depending on how chunks are
implemented (as a singly-linked list or an array), how elements are
ordered within chunks (in sorted or in unsorted or in some other
order), and how the chunks are related to each other.

The technique itself is not new; it has been explicitly used, for
example, in the context of AVL-trees \cite{Mun86} and in the context
of run-relaxed heaps \cite{DGST88}. Unfortunately, in the latter paper
the technique is applied incorrectly because the issue of referential
integrity is ignored.  Our purpose in this paper is to advocate the
wide applicability and generality of the technique.  Also, the
technique seems to be of practical value so our plan is to verify this
experimentally.

\noindent\textbf{Again, the tone is a bit strong here.  Did the
authors care about referential integrity?}

The remainder of the paper is organized as follows.  In
\secref{sec:def}, the data structures studied are defined to fix the
notation used throughout the paper.  In
\secref{elementary-dictionaries}, we show how the compaction technique
can be used for reducing the memory overhead of any elementary ordered
dictionary. More specifically, we extend an old result of Munro
\cite{Mun86} by proving that any elementary dictionary can be
converted into an equivalent data structure that requires at most
$O(n/\lg n)$ extra space without affecting the asymptotic running
times of algorithms for searching, inserting, and erasing.  In
\secref{iterator-dictionaries}, we describe two more applications of
the technique that are able to transform any linear-size ordered
dictionary providing iterator support into a more space-efficient data
structure. In \secref{iterator-queues} we prove similar results for
priority queues.  Finally, in \secref{discussion} we mention some
other applications of the compaction technique and conclude with a few
final remarks.

\section{Data structures studied}%
\seclabel{sec:def}

When defining the data structures considered, we have used the
interfaces specified in the C\texttt{++} standard \cite{ISO} as a
model, but we use our own conventions for the naming of the
methods. The main reason for using a generic framework for method
naming is brevity and our aim at using a unified notation independent
of whether a container stores a set or a multiset.

\subsection*{Elementary ordered dictionary}

An \emph{ordered dictionary} is a container that stores a set or
multiset of elements drawn from an ordered universe and supports
insertion and erasure of elements and search of elements with respect
to a given element.  We define the methods in a generic form, in which
they can take parameters, $\propto$ and $\mathit{r}$, specified at
compile time and parameters, $\mathcal{S}$ and $x$, specified at run
time. Parameter $\propto \in \set{\diamond,<,>,=}$ is a binary
predicate used for specifying an interval of interest in
$\mathcal{S}$. Binary predicate $\diamond$ is special since $x
\diamond y$ is defined to be true for any two elements $x$ and $y$. We
only allow $r$ to take two values $1$ and $-1$, meaning the first and
the last element on an interval, respectively.

In its elementary form, a realization of an ordered dictionary should
provide the following methods:
\begin{description}

\item[$\Search\sequence{\propto,\mathit{r}}(\mathcal{S}, x).$] Return
a reference to the $r$th element among all elements $v$ in container
$\mathcal{S}$ for which predicate $v \propto x$ is true.  If no such
element exists, return a reference to a special \Null{} element.
%The
%function used in element comparisons is to be specified at the
%construction time of $\mathcal{S}$.

\item[$\Insert{}(\mathcal{S}, x).$] Insert a copy of element $x$ into container $\mathcal{S}$.

\item[$\Erase\sequence{\propto,\mathit{r}}(\mathcal{S}, x).$] Remove
the $r$th element among all elements $v$ in container $\mathcal{S}$
for which predicate $v \propto x$ is true.  If no such element exists,
do nothing.
\end{description}
Argument $x$ for these methods can be
partially constructed; i.e.~if an element is a pair of a key and some
satellite data, only the key needs to be specified before a search.

The generic form of \Search{} is an archetype for 8 different methods
depending on how the compile-time parameters are set. If $\mathcal{S}$
stores a set, generic parameter $r$ would be superfluous.
Colloquially, invocations of various forms of \Search{} are called
\emph{searches}, and invocations of \Insert{} and
\Erase{} \emph{updates}. 
Conventional
names for methods (for example, used in \cite{CLRS01})  
$\Search\sequence{<,-1}$,
$\Search\sequence{>,1}$,
$\Search\sequence{\diamond,1}$, 
$\Search\sequence{\diamond,-1}$,
$\Erase\sequence{\diamond,1}$, and
$\Erase\sequence{\diamond,-1}$
would be 
\Predecessor{}, 
\Successor{}, 
\Findmin{}, 
\Findmax{},
\Extractmin{}, and
\Extractmax{},
respectively.
Sometimes $\Search\sequence{=,1}$ is called a \emph{member search}.
In addition to the above-mentioned methods, there should be methods
for constructing and destroying elements and sets, and methods for
examining the cardinality of sets, but these are algorithmically less
interesting and therefore omitted in our discussion.

\subsection*{Semi-implicit ordered dictionaries}

*** To be written ***

Just define formally what Munro's model of computation is. We need an
infinite array for elements.

\subsection*{Ordered dictionaries with iterator support}

A \emph{locator} is a mechanism for maintaining the association
between an element and its location in a data structure \cite[Section
6.4]{GT98}. A locator follows its element even if the element changes
its location inside the data structure. In terms of the C\texttt{++}
programming language \cite{ISO}, a locator is an object that provides
default constructor, copy constructor, unary operator * (prefix), and
binary operators \texttt{==} and \texttt{!=}. That is, for
locator $p$, *$p$ denotes the element stored at the specified
location.  An \emph{iterator} is a generalization of a locator which
captures the concepts \textit{location} and \textit{iteration} in a
container of elements. In addition to the operations supported for a
locator, a \emph{bidirectional iterator} provides
unary operators \texttt{++} and \texttt{-}\texttt{-} (both prefix and
postfix). That is,
for iterator $p$, \texttt{++}$p$ returns an iterator pointing to the
element following the current element according to some
specific order. An error occurs if $p$ points to the past-the-end
element of the corresponding container.

For an ordered dictionary providing locator-based access to elements,
the methods \Search{}, \Insert{}, and
\Erase{} are overloaded as follows:
\begin{description}
\item[$\Search\sequence{\propto, r}(\mathcal{S}, p, x).$] 
Start the search from the element pointed to by iterator $p$ and return
an iterator to the $r$th element among all elements $v$ in container
$\mathcal{S}$ for which predicate $v \propto x$ is true. If no such
element exists,  return an iterator to the past-the-end element.

\item[\Insert{}$(\mathcal{S}, p, x).$] Insert a copy of element $x$
\emph{immediately before} the element pointed to by iterator $p$ in
container $\mathcal{S}$, and return an iterator pointing to the newly inserted element.

\item[\Erase{}$(\mathcal{S}, p).$] Remove the element pointed to by iterator $p$
from container $\mathcal{S}$.
\end{description}

To iterate over a collection of elements, it is also possible to
access some special locations:
\begin{description}
\item[\Begin{}$(\mathcal{S}).$] Return an iterator pointing to the first element
in container $\mathcal{S}$.

\item[\End{}$(\mathcal{S}).$] Return an iterator pointing to the past-the-end
element for container $\mathcal{S}$.
\end{description}
Since \Begin$(\mathcal{S})$ points to the minimum element of $\mathcal{S}$ and
\texttt{-}\texttt{-}\End$(\mathcal{S})$ to the maximum element (if any),
methods \Findmin{} and \Findmax{} will be superfluous.

\subsection*{Ordered dictionaries with random-access iterators}

*** To be written ***

Now I also know how to obtain these without any lost in the space
efficiency. I will write a paragraph or two about these in the section
about ``compaction of ordered dictionaries with iterator support''.

\subsection*{Elementary priority queues}

In its elementary form, a \emph{priority queue} stores a multiset of
elements and supports operations \Findmin{}, \Insert{}, and
\Extractmin{}.  We want to point out that we do not consider
elementary priority queues since a fully implicit priority queue
exists that can support \Findmin{} and \Insert{} in $O(1)$ worst-case
time, and \Extractmin{} in $O(\lg n)$ worst-case time \cite{CMP88},
$n$ being the number of elements stored. As mentioned in
\secref{intro}, in a realistic model of computation, the amount of
extra space used by such a data structure is only $O(\sqrt{n})$.  In
\cite{CMP88}, it was shown that there even exists a fully implicit
double-ended priority queue, i.e.~a priority queue that supports
\Findmax{} and \Extractmax{} as well.

\subsection*{Priority queues with locator support}

In a general form, a \emph{priority queue} is an element container
that provides methods \Findmin{} and locator-based \Erase{} as well as (a
subset of) the following methods:
\begin{description}
\item[\Insert{}$(\mathcal{Q}, x).$] Insert a copy of element $x$ into
container $\mathcal{Q}$ and return a locator pointing to the newly inserted
element.

\item[\Borrow{}$(\mathcal{Q}).$] Remove an \emph{arbitrary} element from
container $\mathcal{Q}$ and return a locator pointing to that element. If $\mathcal{Q}$
is empty, return a locator pointing to the past-the-end element.

\item[\Decrease{}$(\mathcal{S}, p, x).$] Replace the element pointed to by
locator $p$ with element $x$, which is assumed to be no greater than
the original element pointed to by $p$.
\end{description}
Of these methods, \Borrow{} is non-standard, but according to our
earlier research \cite{EJK04,EJK07} this is a valuable tool, for
instance, in data-structural transformations. Our point to include it
in the repertoire of priority-queue operations is to show that our
compaction scheme can handle it quite easily. If the original priority
queue provides it, the compact version will do the same at about equal
efficiency.

As ordered dictionaries, priority queues could also provide iterator
support. However, in a priority queue there is no natural order of
storing the elements. In a basic form, an iteration starting from
\Begin{} and ending at \End{} is just guaranteed to visit all
elements. Iterator operations could be realized in the same way as for
ordered dictionaries, so in this paper we concentrate on priority
queues that support the above-mentioned priority-queue
operations.

\section{Elementary ordered dictionaries}%
\seclabel{elementary-dictionaries}


In this section we describe transformations that can be applied to
elementary ordered dictionaries to make them more space-efficient.
Our approach is to partition the data into chunks and store these
chunks in the ordered dictionary under transformation (skiplist,
2-3 tree, red-black tree, etc).  To avoid confusion, we will refer the
data structure we are describing as ``the dictionary'' and the data
structure we are using as ``the data structure.''  Similarly, we will
use the terms chunks and elements to refer to groups of elements and
individual elements, respectively.  

We study two different schemes based on this idea.  Both schemes have
the following in common:  Each chunk has a header containing a
constant amount of data, the elements of each chunk are stored in an
array, the elements stored within each chunk are kept in sorted order,
and the chunks themselves are stored in a doubly-linked list (the
pointers for which are stored in the header).  At all times we
maintain the \emph{global invariant} that every element in chunk $A$
is less than or equal to every element in chunk $B$ if $A$ appears
before $B$ in this list.  Thus, a traversal of this \emph{chunk list}
is sufficient to output all elements in sorted order.

In addition to being linked into a list, the chunks themselves are
stored in the data structure.  Notice that, because of the above
invariant, any two chunks can be compared in $O(1)$ time.  To test,
for example, if chunk $A$ is less than chunk $B$ we simply check if
the last element of $A$ is less than the first element of $B$.  It is
this comparison function that we use to store the chunks in the
dictionary.  Similarly, we can compare a chunk to an item $x$.  To
test if $x$ is less than $A$ we test if $x$ is less than the first
element of $A$.  To test if $x$ is greater than $A$ we test if $x$ is
greater than the last element of $A$.  If $x$ is neither greater than
nor less than $A$ then $x$ is equal to $A$.  

\noindent\textbf{Say something about multisets here.}

With this representation, it is easy to see how \Search\ can be
implemented by performing at most one search in the dictionary
followed by one (linear or binary) search in a chunk.  The special
searches  \Findmin{} and \Findmax{} are even easier;  we simply return
the first element of the first chunk or the last element of the last
chunk, respectively.  Where our two implementations differ is in how
they implement the \Insert{} and \Erase{} operations.

\subsection{Fixed-Sized Chunks}

The simplest realization of the above ideas is to store the chunks as
fixed-size arrays, each of size $b+1$.  With the exception of at most
one chunk, each chunk will store either $b-1$, $b$, or $b+1$ data
elements.

To insert $x$ into the data structure we first search for $x$ in the
data structure.  The result of this is some chunk $A$ that is either
the first chunk, the last chunk, or a chunk in which $x$ is greater
than the first element and less than the last element.  Next, by
traversing the chunk list we examine up to $b$ consecutive chunks
including $A$.

If any of these $b$ consecutive chunks contains fewer than $b+1$
elements then we can reassign the elements to chunks so as to place
$x$ into some chunk and restore our global invariant.  Otherwise, each
of the $b$ chunks contains $b+1$ elements.  In this case we create a
new chunk and reassign these $b(b+1)$ elements across the $b+1$ chunks
so that each chunk contains exactly $b$ elements.  Finally, we place
$x$ in the appropriate chunk and insert the newly created chunk into
the data structure.

Erasing an element is similar to insertion. To erase $x$, we first
search for the chunk $A$ containing $x$. Then, in the $b$ consecutive
chunks including $A$ either: (1)~there is some chunk containing $b$ or
$b+1$ elements, in which case we remove $x$ and redistribute elements
so as to maintain the global invariant, or (2)~all $b$ chunks contain
exactly $b-1$ elements in which case we delete one chunk from the
data structure and redistribute the $b(b-1)-1$ elements among the $b-1$
remaining chunks so that each chunk stores $b$ elements, except one,
which contains $b-1$ elements.

It is clear that both the insertion and deletion procedures require
one search in the data structure, $O(b^2)$ work, and at most one
insertion or deletion, respectively, in the data structure.  Indeed a
slightly more careful amortized analysis shows that a sequence of $n$
\Insert/\Erase\ operations on the dictionary requires only $O(n/b)$
\Insert/\Erase\ operations on the data structure.  

\begin{theorem}
\label{theorem:elementary-a}
Given an ordered dictionary that requires $S(n)$, $I(n)$, and $D(n)$
time to search, insert, and delete (respectively) an element from a
set of $n$ elements and that has a memory overhead of $cn$ for a
positive constant $c$, we can construct an equivalent data structure
that requires
\begin{enumerate}
\item $S'(n)=O(S(n/b)+\lg b)$, $I'(n)=O(S(n/b)+I(n/b)+b^2)$,
and $D'(n)=O(S(n/b)+D(n/b)+b^2)$ worst-case time to search, insert,
and erase (respectively), 
\item $S''(n)=O(S(n/b)+\lg b)$,
$I''(n)=O(S(n/b)+\frac{1}{b}I(n/b)+b^2)$, and
$D'(n)=O(S(n/b)+\frac{1}{b}D(n/b)+b^2)$ amortized time to search,
insert, and erase (respectively), and 
\item a memory overhead of $O(n/b)$.
\end{enumerate}
\end{theorem}

One issue that several researchers have been concerned with is that of
\emph{fragmentation}, i.e., how many distinct memory locations are
required for a data structure.  Using the above scheme, in combination
with simple techniques for maintaining a pool of buffers for the
chunks, the entire dictionary described above can be made to live in
one resizeable array that contains $n$ data elements and $O(n/b)$
additional information.   In this case, the fragmentation is no worse
than the fragmentation required in an implementation of a resizeable
array.

\subsection{Variable-Sized Chunks}

One drawback of the scheme described in the previous subsection is the
$O(b^2)$ term in the running time for the \Insert\ and \Extract\
operations.  In this subsection we describe an alternative that avoids
this problem but requires more extensive use of memory management and,
consequently, for which it is more difficult to avoid fragmentation.
In this scheme, all but one chunk contains somewhere between $b/2$ and
$2b$ elements, inclusive.\footnote{Throughout this section we assume
$b$ is even so that $b/2$ is an integer.}  However, unlike the
previous scheme, each chunk is allocated to the precise size required
to store the number of elements it currently contains.

Since chunks will be reallocated frequently and are addressed by the
nodes of the data structure, each chunk also has an associated
\emph{handle}.  This is a memory location that stores a pointer to the
chunk and the header of each chunk stores a pointer to this handle.
The data structure, rather than maintain direct pointers to the
chunk, maintains pointers to the handle.  In this way, when a chunk
is reallocated it only need update its handle.\footnote{This extra
level of indirection adds some overhead to the running time.  It is
unnecessary for dictionary implementations that store, or refer to,
each data element only once, as is the case with most dictionary
implementations.} From this point on, updating of handles is implicit
when we say that a chunk is resized.

An insertion of the element $x$ in the data structure proceeds as
follows: The chunk $A$ that should contain $x$ is located.  If $A$
contains fewer than $2b$ elements then $A$ is resized to make room for
$x$ and $x$ is added to $A$.  Otherwise, if $A$ already contains $2b$
elements then $A$ is split into two chunks, each containing $b$
elements, one of the new chunks is inserted into the dictionary, and
$x$ is added to the appropriate chunk.

The \Erase\ procedure is similar to the \Insert\ procedure.  If the
chunk $A$ containing $x$ contains more than $b/2$ elements then $A$ is
resized and $x$ is removed.  Otherwise, we check one of the
neighbouring chunks, $B$, of $A$. If $B$ contains more than $b/2$
elements then an element is removed from $B$ and used to replace $x$
in $A$ and then $B$ is resized. Otherwise, each of $A$ and $B$
contains exactly $b/2$ elements, in which case we merge them into a
single chunk containing $b$ elements and delete one of the chunks from
the data structure.

Again, it is clear that both \Insert\ and \Erase\ can be done using at
most one search, $O(b)$ time to operate on the at most 2 chunks
involved, and at most one insertion and extraction.  Again, a simple
amortization argument shows that a sequence of $n$ \Insert\ and
\Erase\ operations on the dictionary results in only $O(n/b)$ \Insert\
and \Erase\ operations in the data structure.

\begin{theorem}
\label{theorem:elementary-b}
Given an ordered dictionary that requires $S(n)$, $I(n)$, and $D(n)$
time to search, insert, and erase (respectively) an element from a
set of $n$ elements and that has a memory overhead of $cn$ for a
positive constant $c$, we can construct an equivalent data structure
that requires 
\begin{enumerate}
\item $S'(n)=O(S(n/b)+\lg b)$, $I'(n)=O(S(n/b)+I(n/b)+b)$,
and $D'(n)=O(S(n/b)+D(n/b)+b)$ worst-case time to search, insert,
and erase (respectively), 
\item $S''(n)=O(S(n/b)+\lg b)$,
$I''(n)=O(S(n/b)+\frac{1}{b}I(n/b)+b)$, and
$D'(n)=O(S(n/b)+\frac{1}{b}D(n/b)+b)$ amortized time to search,
insert, and erase (respectively), and 
\item a memory overhead of
$O(n/b)$.
\end{enumerate}
\end{theorem}

\comment{
\comment{The components used by this compaction scheme are
illustrated in Figure~\ref{fig:array}.

\begin{figure}
\begin{center}
\input{array.pstex_t}
\end{center}
\caption{A chunk storing seven integers in an array.\label{fig:array}}
\end{figure}
}


We call the objects containing a pointer to the beginning of a chunk,
the \emph{head} of that chunk.  A head $f$ contains two pointers
$\Pred(f)$ and $\Succ(f)$ that point to other heads, a pointer to an
array $\Elem(f)$ that contains anywhere between $b$ and $2b$ elements,
and an integer $\Size(f)$ that tells the number of elements in
$\Elem(f)$.  The $\Pred$ and $\Succ$ fields are used to link all heads
together into a doubly-linked list.  The elements of $\Elem(f)$ are
always sorted in increasing order, and the elements of $\Elem(f)$ are
all less than the elements of $\Elem(g)$ if $f$ appears before $g$ in
the doubly-linked list.  In this way, the list of heads gives the
elements of the dictionary in sorted order.  Additionally, two special
pointers are maintained that point to the first and the last head,
respectively.  The data structure, denoted $\mathcal{S}$, stores
\emph{handles} to the heads.  Each head contains an iterator pointing
back to the corresponding node inside the data structure.  If the
iterators of the data structure were normal pointers, the extra
indirection could be avoided and the data structure could store the
heads directly.

Note that the comparison of two heads is a constant time operation,
since to test if $f<g$ we compare the last element of $\Elem(f)$ to
the first element of $\Elem(g)$.  Similarly, given an element $x$ we
can test if $x<f$, respectively $f<x$, by comparing $x$ with the
first, respectively last, element in $\Elem(f)$.  Therefore, to reduce
the storage requirements of a dictionary, we make a data structure on
a set of heads whose $\Elem$ arrays contain all the elements in our
dictionary.

\paragraph{Searching.}
If we search for some element $x$ in the data structure, we find the
head $f$ such that $\Elem(f)$ contains the smallest key greater
than or equal to $x$.  An additional search in $\Elem(f)$ finds the
actual element we are looking for at a cost of $O(b)$, or $O(\lg b)$
if we use binary search.  If the data structure takes $S(n)$ time to
perform a search on a set of $n$ elements, then the dictionary takes
$S'(n)=O(S(n/b)+\lg b)$ time to perform the search.

\Insert{}$(\mathcal{S}, x)$.
Assume we are given a pointer to the dictionary node containing the
head $f$ we would find if we searched for $x$.  To insert $x$, we
proceed as follows.  If $\Elem(f)$ contains fewer than $2b$ elements
then we simply reallocate $\Elem(f)$ to increase its size by 1 and add
$x$ to $\Elem(f)$.  Otherwise ($\Elem(f)$ contains $2b$ elements), we split
$\Elem(f)$ into two heads, one that contains $b$ elements and one
that contains $b+1$ elements and insert the newly created head
into our data structure.  Therefore, if the data structure takes
$I(n)$ time to perform an insertion on a set of $n$ elements then
insertion in the dictionary takes $I'(n)=O(I(n/b)+b)$ time.

\Extract{}$(\mathcal{S}, x)$.
To remove element $x$ from the dictionary, we assume we are given a
pointer to the dictionary node containing the head $f$ such that
$\Elem(f)$ contains $x$.  If $\Elem(f)$ has size greater than $b$, then
we simply reallocate $\Elem(f)$ to decrease its size by one and exclude
$x$.  Otherwise ($\Elem(f)$ has size $b$), we examine a head $g$
that is a neighbour of $f$ in the linked list.  If $\Elem(g)$ contains
more than $b$ elements then we take the first or last element in
$\Elem(g)$ (depending on whether $g=\Succ(f)$ or $g=\Pred(f)$) and use it
to replace $x$ in $g$.  Otherwise ($\Elem(f)$ and $\Elem(g)$ both have
size $b$), we merge $f$ and $g$ into a single chunk and remove one
of them from the data structure.  If $D(n)$ is the time it takes to
remove element $x$ from a data structure of size $n$, then deletion
in the modified data structure takes $D'(n)=O(D(n/b)+b)$ time.

A data structure containing $m$ heads contains at least $n=bm$
elements.  Each head has a constant amount of overhead, and the data
structure has an overhead of $cm$ for some constant $c$, so the
overhead is $O(n/b)$. The above discussion can be summarized as
follows.

\begin{theorem}
\label{theorem:elementary}
Given an ordered dictionary that requires
$S(n)$, $I(n)$, and $D(n)$ time to search, insert, and delete
(respectively) an element from a set of $n$ elements and that has a memory
overhead of $cn$ for a positive constant $c$, we can construct an equivalent
data structure that requires $S'(n)=O(S(n/b)+\lg b)$,
$I'(n)=O(I(n/b)+b)$, and $D'(n)=O(D(n/b)+b)$ time to
search, insert, and delete (respectively) and that has a memory
overhead of $O(n/b)$.
\end{theorem}
}

Knowing that element-based operations on an ordered dictionary take at
least logarithmic time in the worst case, it is tempting to choose $b
= \lg n$.  The problem with this choice is that $n$, the number of
elements stored, is varying whereas $b$ must be kept fixed. To avoid
this problem, we use Frederickson's partitioning scheme \cite{Fre83}
and partition the data structure into $O(\lg\lg n)$ separate
\emph{portions} of exponentially increasing sizes $2^{2^{1}}$,
$2^{2^{2}}$, $2^{2^{3}}$, and so on, except that the last portion can
be smaller.  Inside each portion we can use a fixed chunk size
$b_i=2^i$ in the $i$th portion. Insertions are done in the last
portion, searches visit all structures but the overall cost is
dominated by the cost incurred by the last portion [all $O(\lg\lg n)$
searches still take $O(\lg n)$ time if the data structure supports
searches in $O(\lg n)$ time], and deletions are performed by borrowing
an element from the last portion. 

\begin{theorem}
\label{theorem:elementary-c}
Given an ordered dictionary that requires $S(n)$, $I(n)$, and $D(n)$
time to search, insert, and erase (respectively) an element from a
set of $n$ elements and that has a memory overhead of $cn$ for a
positive constant $c$, we can construct an equivalent data structure
that requires 
\begin{enumerate}
\item $S'(n)=O(S(n/\lg n)+\lg\lg n)$, $I'(n)=O(S(n/\lg n)+I(n/\lg n)+\lg n)$,
and $D'(n)=O(S(n/\lg n)+D(n/\lg n)+\lg n)$ worst-case time to search, insert,
and erase (respectively), 
\item requires $S''(n)=O(S(n/\lg n)+\lg\lg n)$,
$I''(n)=O(S(n/\lg n)+\frac{1}{\lg n}I(n/\lg n)+\lg n)$, and
$D'(n)=O(S(n/\lg n)+\frac{1}{\lg n}D(n/\lg n)+\lg n)$ amortized time to search,
insert, and erase (respectively), and 
\item that has a memory overhead of
$O(n/\lg n)$.
\end{enumerate}
\end{theorem}

As an example, we can apply Theorem \ref{theorem:elementary-c} to any
worst-case efficient balanced search tree such as a red-black tree to
obtain the following result:

\begin{corollary}
\label{corollary:red-black}
There exists an ordered dictionary that supports searches and updates
in $O(\lg n)$ worst-case time, \Findmin{} and \Findmax{} in $O(1)$
worst-case time, and has a memory overhead of $O(n/\lg n)$.
\end{corollary}

\noindent\textbf{Discuss issues with fragmentation}

\section{Ordered dictionaries with iterator support}
\seclabel{iterator-dictionaries}

The data structural transformations described in the previous sections
can be used to reduce the memory overhead of dictionary data
structures.  However, they break down when the interface to those data
structures allows access to elements using iterators or even locators.
The reason for this is that, as updates are performed, individual data
elements migrate between chunks.  Thus, without some special mechanism
the iterators for these elements will become invalid.

In this section we show two ways in which these data structures can be
extended to allow iterator access.  The first method is preferable
when the total number of iterators to elements in the data structure
is small and when unused iterators are cleaned up. The second method
is preferable when there are many iterators in use simultaneously. 

\subsection{Element handles lists}

In this section we present a method of maintaining iterators where the
memory overhead at any given time is proportional to the total number
of iterators in use at that time.  This method augments either of the
two schemes described in the previous section in the following way:
The header of each chunk is augmented so that it maintains a pointer
to an ordered dictionary with iterator support that allows constant
time insertion and deletion.\footnote{In practice, this is probably
overkill and a sorted doubly-linked list would suffice, but this will
increase the search time in \thmref{iterator-a} to $O(S(n/b)+d)$.}
This dictionary, which we call the \emph{element handle list} for the
chunk stores element handles. Each \emph{element handle} maintains a
reference count, a pointer to the chunk that contains the element
referenced, and the index within the chunk of the element referenced.
The iterators themselves are pointers to element handles.

When a new iterator for an element $x$ is created, the element handle
list for the chunk containing $x$ is searched.  If there is already an
element handle for $x$ then its reference count is incremented and a
pointer to this handle is used as the iterator.  Otherwise, a new
element handle for $x$ is created, added to the list, and initialized
with a reference count of 1.  Since an element handle list stores at
most $b$ element handles, this process takes $O(\log b)$ time.

When an iterator for an element $x$ is destroyed, the reference count
of the element handle is decremented. If the reference count drops to
zero then the element handle is deleted from the list and the memory
associated with it is freed.  This process takes $O(1)$ time.

When the algorithms of the previous section operate on chunks by
either growing them, splitting them, merging or moving elements
between them, the element handles and element handle lists associated
with them must be updated appropriately to reflect the changes.
However, these changes only require changing the contents and/or
pointers of the element handles and therefore, once they are updated,
the iterators remain valid.  This updating of element handles is
easily done without increasing the ($O(b)$) running time of each of
the above operations.

It is clear that the iterators defined this way offer constant time
access to the element which they locate.  They can be created in
$O(\log b)$ time and destroyed in $O(1)$ time.  Furthermore, because
the element handle lists support iteration and constant time insertion
and deletion, iterators can be incremented and decremented in $O(1)$
time.  If $k$ is the total number of elements in the dictionary
referenced by iterators at any point in time then the overhead of the
dictionary, at that point in time, is $O(n/b+k)$.  Note that the
$O(k)$ term is in some sense necessary since, if a program using the
dictionary maintains $k$ active iterators, then these must be stored
somewhere within the program and thus there is already $\Omega(k)$
space used within the program to keep track of these iterators.
Applying this augmentation to the data structure of
\thmref{elementary-b} we obtain the following result:

\begin{theorem}\label{theorem:iterator-a}
Given an ordered dictionary with iterator support that requires 
$S(n)$, $I(n)$, and $D(n)$
time to search, insert, and erase (respectively) an element from a
set of $n$ elements and that has a memory overhead of $cn$ for a
positive constant $c$, we can construct an equivalent data structure
that requires 
\begin{enumerate}
\item $S'(n)=O(S(n/b)+\lg b)$, $I'(n)=O(I(n/b)+b)$,
and $D'(n)=O(D(n/b)+b)$ worst-case time to search, insert,
and erase (respectively), 
\item $S''(n)=O(S(n/b)+\lg b)$,
$I''(n)=O(\frac{1}{b}I(n/b)+b)$, and
$D'(n)=O(\frac{1}{b}D(n/b)+b)$ amortized time to search,
insert, and erase (respectively), and 
\item a memory overhead of
$O(k+n/b)$ where $k$ is the number of elements of the $S$ currently 
referenced by iterators.
\end{enumerate}
\end{theorem}

\subsection{Storing chunks as linked lists}

An alternative method of implementing ordered dictionaries with
iterator-based access is to apply any of the schemes used in
\secref{elementary-dictionaries} but, instead of using a sorted array
for each chunk, to use a sorted singly-linked list.  In this case, an
iterator for item $x$ is implemented simply as a pointer to the list
node containing $x$ and the iterator operations are trivial to
implement.  All the other operations remain the same except that a
search within a chunk must now be a linear search as opposed to a
binary search.  Using the data structure of \thmref{elementary-b} we
obtain the following result:

\begin{theorem}
\label{theorem:iterator-b}
Given an ordered dictionary with iterator support that requires 
$S(n)$, $I(n)$, and $D(n)$
time to search, insert, and erase (respectively) an element from a
set of $n$ elements and that has a memory overhead of $cn$ for a
positive constant $c$, we can construct an equivalent data structure
that requires 
\begin{enumerate}
\item $S'(n)=O(S(n/b)+b)$, $I'(n)=O(I(n/b)+b)$,
and $D'(n)=O(D(n/b)+b)$ worst-case time to search, insert,
and erase (respectively), 
\item $S''(n)=O(S(n/b)+b)$,
$I''(n)=O(\frac{1}{b}I(n/b)+b)$, and
$D'(n)=O(\frac{1}{b}D(n/b)+b)$ amortized time to search,
insert, and erase (respectively), and 
\item a memory overhead of
$n$ pointers plus $O(n/b)$ additional storage.
\end{enumerate}
\end{theorem}

As a concrete example of the usage of Theorem
\ref{theorem:iterator-b}, let us apply it to the ordered dictionary
presented in \cite{BLMTT03}. This gives us the following corollary.

\begin{corollary}
\label{corollary:dictionary}
There exists an ordered dictionary that supports iterator-based
updates in $O(1/\varepsilon)$ worst-case time, finger searches in
$O(\lg d + 1/\varepsilon)$ worst-case time, $d$ being the distance to
the target, and requires at most $(1 + \varepsilon)n$ words of
extra storage, for any $\varepsilon > 0$ and sufficiently large
$n > n(\varepsilon)$.
\end{corollary}

\section{Priority queues with locator support}\seclabel{iterator-queues}

Any ordered dictionary can be used as a priority queue as well.
However, two methods, \Insert{} and \Decrease{}, have turned out to be
special since there exist priority queues that support all of them in
constant time.  When our previous transformations are applied to such
a priority queue, the resulting data structure is not necessarily
optimal with respect to these operations. Therefore, in this section
we describe a transformation that is suitable for priority queues.

We again use the idea of partitioning the elements into chunks.
However, this time the chunks are not sorted but the minimum element
within each chunk is stored as the first element of the chunk.  The
minimum (first) element within each chunk is referred to as the
\emph{representative} of the chunk.  Furthermore, the chunks are not
linked into a list, and there is no special relationship between the
elements of one chunk and the elements of another chunk. One special
chunk, denoted by $B$ and called the \emph{buffer}, is treated
differently.  All chunks except $B$ are stored in a priority queue
where the key of a chunk is its representative.  As before, we refer
to the data structure we are describing as ``the priority queue'' and
the data structure we are transforming as the ``the data structure.''

Either of the two methods described in \secref{iterator-dictionaries}
for adding locator support can be used.  That is, we can either store
each chunk as an array and create, for each chunk, a locator handle
list or we can store each chunk as a singly-linked list. In either
case, locators for an element can be created in constant time if the
element is the first or last element of its chunk. 

Next we consider how the various priority-queue operations can be
realized.

\Findmin{}$(\mathcal{Q})$. The minimum is stored either in buffer $B$
or the minimum representative in the data structure. A locator for
the minimum of the two elements can be returned. Since both the
relevant elements are the first elements in their respective chunks,
this operation can be accomplished in $O(1)$ worst-case time.

\Insert{}$(\mathcal{Q}, x)$. Insert $x$ into buffer $B$. If $x$ is
smaller than the current minimum of $B$, store it as the first
element; otherwise, store it somewhere convenient. If the size of $B$
is equal to $2b$, insert $B$ into the data structure and create a new
empty buffer.  In addition to a single invocation of \Insert\ for the
data structure, only a constant amount of work is done (under the
assumption that chunks know their size).

\Borrow{}$(\mathcal{Q})$. If $B$ is non-empty, extract a convenient
element and return (an locator pointing to) it.  Otherwise, $B$ is
empty, extract one chunk from the data structure and make that a new
buffer $B$. If, after this, $B$ is still empty, return an locator to
the past-the-end element and stop.  Otherwise, take a convenient
element from $B$ and return (an locator pointing to) that element.
Again one invocation of \Borrow{} for the data structure may be
necessary, but otherwise only a constant amount of work is done.

\Decrease{}$(\mathcal{Q}, p, x)$. Access the head of the chunk to
which $p$ points. Thereafter, carry out the element replacement and
make the replaced element the representative of the chunk if it is
smaller than the previous representative. If the chunk is not buffer
$B$ and if the representative of the chunk changes, invoke \Decrease{}
on the data structure. The work done in addition to one invocation of
\Decrease{} depends on the method being used.  If arrays are being
used to store chunks then the element handle contains a pointer to the
head of the chunk so finding the head takes $O(1)$ time.  If the
chunks are stored as lists, it may be necessary to traverse the entire
list to find the head of the chunk and adjust the appropriate
pointers, so the time is $O(b)$.

\Extract{}$(\mathcal{Q}, p)$. Access the head of the chunk to which
$p$ points. If the size of the chunk becomes smaller than $b$ or if
the representative of the chunk is removed, remove the chunk from the
data structure. Then merge the removed chunk with buffer $B$ to form a
new buffer. If the size of $B$ is larger than $2b$, cut off a piece of
size $2b$ from $B$, make the rest a buffer and insert the chunk cut
off into the data structure. To sum up, one invocation of \Extract{}
and \Insert{} for $Q$ may be necessary, but the other work done is
proportional to $b$.

The above discussion is summarized in the following two theorems, that
come from applying each of the two methods in
\secref{iterator-dictionaries}, respectively.  Let
$\mathcal{D}$ be a data structure and let $n$ denote the number of
elements stored in $\mathcal{D}$ prior to each operation.  We use the
notation that $\mathcal{D}$ executes \Findmin{}, \Insert{},
\Borrow{}, \Decrease{}, and \Extract{} in 
$F^{\mathcal{D}}(n)$,
$I^{\mathcal{D}}(n)$, 
$B^{\mathcal{D}}(n)$, 
$D^{\mathcal{D}}(n)$, and
$E^{\mathcal{D}}(n)$ time, respectively. To get the claimed result, we
should just make $b$ large enough to get the amount of extra storage
used down from $O(n)$ to $\varepsilon n$.

\begin{theorem}
\label{theorem:queue-a}
A priority queue $\mathcal{D}$ that uses $O(n)$ words of extra storage
can be transformed into an
equivalent data structure that, when referenced by at most $k$
iterators,
uses
$O(k)+\varepsilon n$ words of extra storage, for an arbitrary
$\varepsilon > 0$ and sufficiently large $n > n(\varepsilon)$, and performs 
\Findmin{}, \Insert{},
\Borrow{}, \Decrease{}, and \Extract{} in
$F^{\mathcal{D}}(\varepsilon n) + O(1)$,
$I^{\mathcal{D}}(\varepsilon n) + O(1)$,
$B^{\mathcal{D}}(\varepsilon n) + O(1)$,
$D^{\mathcal{D}}(\varepsilon n) + O(1/\varepsilon)$,
$E^{\mathcal{D}}(\varepsilon n) + I^{\mathcal{D}}(\varepsilon n) + O(1/\varepsilon)$ time, respectively.
\end{theorem}



\begin{theorem}
\label{theorem:queue-b}
A priority queue $\mathcal{D}$ that uses $O(n)$ words of extra storage
can be transformed into a equivalent data structure that
uses
$(1+\varepsilon)n$ words of extra storage, for an arbitrary
$\varepsilon > 0$ and sufficiently large $n > n(\varepsilon)$, and performs 
\Findmin{}, \Insert{},
\Borrow{}, \Decrease{}, and \Extract{} in
$F^{\mathcal{D}}(\varepsilon n) + O(1)$,
$I^{\mathcal{D}}(\varepsilon n) + O(1)$,
$B^{\mathcal{D}}(\varepsilon n) + O(1)$,
$D^{\mathcal{D}}(\varepsilon n) + O(1/\varepsilon)$,
$E^{\mathcal{D}}(\varepsilon n) + I^{\mathcal{D}}(\varepsilon n) + O(1/\varepsilon)$ time, respectively.
\end{theorem}

\noindent\textbf{Is it possible to get amortized bounds like those we
get for dictionaries, where only one out of every $b$ operations
results in an operation on the underlying priority queue?}

To illustrate the power of Theorem \ref{theorem:queue-b}, let us apply
it to the priority queue described in \cite{EJK06}. As an outcome we
get the following result:

\begin{corollary}
\label{corollary:queue}
There exists a priority queue that supports \Findmin{} and \Insert{}
in $O(1)$ worst-case time, \Decrease{} in $O(1/\varepsilon)$
worst-case time, \Extract{} in $O(\lg n + 1/\varepsilon)$ worst-case
time including at most $\lg n + 3\lg\lg n + O(1/\varepsilon)$ element
comparisons, and requires at most $(1 + \varepsilon)n$ words of extra
storage, for any $\varepsilon > 0$ and sufficiently large $n > n(\varepsilon)$.
\end{corollary}

\section{Discussion}\seclabel{discussion}

We showed that many data structures of size $O(n)$ can be put on a
diet, so that the memory overhead is reduced to $O(n/\lg n)$,
$\epsilon n$, or $(1 + \varepsilon)n$ words for any $\varepsilon >0$
and sufficiently large $n > n(\varepsilon)$. This diet does not change
the running times of the operations on the data structures except by a
small constant factor independent of $\varepsilon$ and/or an additive
term of $O(1/\varepsilon)$.

We considered ordered dictionaries and single-ended priority queues,
but the compaction technique can be used for slimming down other data
structures as well. Other applications, where the technique is known
to work, include positional sequences (doubly-linked lists), unordered
dictionaries (e.g.~hash tables relying on linear hashing
\cite{Lit80}), and double-ended priority queues.  The details follow
closely the guidelines given in this paper, so we leave them
to the interested reader.

\textbf{I agree with the referee that we shouldn't put theorems in the
conclusion unless we are willing to supply details.}

\comment{
\begin{theorem}
A positional sequence, which provides locator-based updates and
bidirectional iterators, can be transformed into an equivalent data
structure that uses $(1 + \varepsilon)n$ words of memory, for any
$\varepsilon > 0$ and sufficiently large $n > n(\varepsilon)$,
excluding the space used by the $n$ elements stored. This
transformation only slows down the running time of the supported
operations by an additive term of $O(1/\varepsilon)$.
\end{theorem}

\begin{theorem}
An unordered dictionary, which relies on chaining and provides member
searches and updates, can be transformed into an equivalent data
structure that uses $(1 + \varepsilon)n$ words of extra storage, for
any $\varepsilon > 0$ and sufficiently large $n > n(\varepsilon)$.
This transformation slows down the running time of the supported
operations by a \emph{multiplicative factor} of $O(1/\varepsilon)$,
except that the slowdown is an additive term of $O(1/\varepsilon)$ for
\Insert{} when a multiset is stored and for locator-based \Erase{}.
\end{theorem}

\begin{theorem}
A doubly-ended priority queue, which uses $O(n)$ words of extra
storage and provides methods \Findmin{}, \Findmax{}, \Insert{},
\Borrow{}, and \Erase{}, can be transformed into an equivalent data
structure that uses $(1 + \varepsilon)n$ words of extra storage, for
any $\varepsilon > 0$ and sufficiently large $n > n(\varepsilon)$.
This transformation only slows down the running time of \Erase{} by an
additive term of $O(1/\varepsilon)$ and that of the other operations
by an additive term of $O(1)$.
\end{theorem}
}

The practical value of the compaction technique is that users of data
structures need not worry about the memory overhead associated with
the data structure they choose.  If it becomes a problem, they can
simply apply the technique and the problem is solved.  The theoretical
value of the compaction technique is that designers of data structures
need no longer make unnecessarily complicated algorithms and
representations for the sake of reducing the memory overhead.  They
can design data structures that have
fast running times and useful properties, and put less emphasis on the
memory overhead of the data structures.

%\section*{Acknowledgement}
%We thank Nicolai Esbensen for pointing out that the compaction technique
%can also be used for slimming down a doubly-linked list.

\bibliography{diet}
\bibliographystyle{DIKU}


\end{document}
