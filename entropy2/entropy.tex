\documentclass[charterfonts,lotsofwhite]{patmorin}
\usepackage{graphicx}
\usepackage{amsopn}
\input{pat}

\newcommand{\defequals}{\stackrel{\mathrm{def}}{=}}
\newcommand{\boundary}{\partial}
\DeclareMathOperator{\depth}{depth}
\DeclareMathOperator{\lft}{left}
\DeclareMathOperator{\rght}{right}
\DeclareMathOperator{\prnt}{parent}
\newcommand{\blah}{O(1/(1-\alpha)^2)}
\newcommand{\junk}[1]{O\left(H(#1)^{2/3}+1\right)}
\newcommand{\note}[1]{$^\star$\marginpar{\small $^*$#1}}

\newcommand{\kangle}{kangle}
\newcommand{\kangles}{kangles}
\newcommand{\kangulation}{kangulation}
\newcommand{\kangulate}{kangulate}
\newcommand{\kangulated}{kangulated}


\title{\MakeUppercase{Distribution-Sensitive Point 
	Location in Simple Subdivisions}%
	\thanks{The research presented in this article took place
while the fourth author was a visiting researcher at the Universit\'e Libre de
Bruxelles, supported by a grant from FNRS and when the fourth author
was a visiting researcher at National ICT Australia/University of
Sydney supported by grants from National ICT Australia and the
University of Sydney.  The researchers are
partially supported by NSERC, FNRS, and the Ontario Ministry of Research and
Innovation.}}

\author{S\'ebastien Collette \\ \textit{Universit\'e Libre de Bruxelles}
  \and Vida Dujmovi\'c \\ \textit{McGill University}
  \and John Iacono \\ \textit{Polytechnic University}
  \and Stefan Langerman \\ \textit{Universit\'e Libre de Bruxelles}
  \and Pat Morin \\ \textit{Carleton University}}
\date{}

\begin{document}
\maketitle

\begin{abstract}
A data structure is presented for point location in connected planar
subdivisions when the distribution of queries is known in advance.
The data structure has an expected query time that is within a
constant factor of optimal.  More specifically, the algorithm
preprocesses a planar subdivision $G$ and a query distribution $D$ to
produce a point location data structure for $G$. The expected number
of point-line comparisons performed by this data structure, when the
queries are distributed according to $D$, is $H +
O(H^{2/3}+1)$ where $H=H(G,D)$ is a lower bound on the expected number of
point-line comparisons performed by any linear decision tree for point
location in $G$ under the query distribution $D$.
\end{abstract}

\keywords{Planar point location, Entropy}

\section{Introduction}
\seclabel{intro}

The planar point location problem is one of the classic problems in
computational geometry. Given a planar subdivision $G$,\footnote{A
\emph{planar subdivision} is a partitioning of the plane into points
(called \emph{vertices}), open line segments (call \emph{edges}), and
open polygons (called \emph{faces}).} the planar point location
problem asks us to construct a data structure so that, for any query
point $p$, we can quickly determine which face of $G$ contains
$p$.\footnote{In the degenerate case where $p$ is a vertex or contained
in an edge of $G$ any face incident on that vertex/edge may be
returned as an answer.}

The history of the planar point location problem parallels, in many
ways, the study of binary search trees.  After a few initial attempts
\cite{dl76,lp77,p81}, asymptotically optimal (and quite different)
linear-space $O(\log n)$ query time solutions to the planar point
location problem were obtained by Kirkpatrick \cite{k83}, Sarnak and
Tarjan \cite{st86}, and Edelsbrunner \etal\ \cite{egs86} in the mid
1980s.  These results were based on hierarchical simplification, data
structural persistence, and fractional cascading, respectively.  All
three of these techniques have subsequently found many other
applications.  An elegant randomized solution, combining aspects of
all three previous solutions, was later given by Mulmuley \cite{m90},
and uses randomized incremental construction, a technique that has
since become pervasive in computational geometry
\cite[Section~9.5]{bcko08}.  Preparata \cite{p90} gives a
comprehensive survey of the results of this era.

In the 1990s, several authors became interested in determining the
exact constants achievable in the query time.  Goodrich \etal\
\cite{gor97} gave a linear-size data structure that, for any query,
requires at most $2\log n + o(\log n)$ point-line comparisons and
conjectured that this query time was optimal for linear-space data
structures. Here and throughout, logarithms are implicitly base 2
unless otherwise specified. The following year, Adamy and Seidel
\cite{as98} gave a linear-space data structure that answers queries
using $\log n + 2\sqrt{\log n} + O(\log\log n)$ point-line comparisons
and showed that this result is optimal up to the third term.

Still not done with the problem, several authors considered the point
location problem under various assumptions about the query
distribution.  All these solutions compare the expected query time to
the \emph{entropy bound};  in a planar subdivision with $f$ faces, if the query
point $p$ is chosen from a probability measure over $\R^2$ such that
$p_i$ is the probability that $p$ is contained in face $i$ of $G$,
then no algorithm that makes only binary decisions can answer queries
using an expected number of decisions that is fewer than 
\begin{equation}
    H(p_1,\ldots,p_f) = \sum_{i=1}^f p_i\log(1/p_i) \enspace . 
	\eqlabel{entropy}
	\eqlabel{entropy-face}
\end{equation}

In the previous results on planar point location, none of the query
times are affected significantly by the structure of $G$;  they hold
for arbitrary planar subdivisions.  However, when studying point
location under a distribution assumption the problem becomes more
complicated and the results become more specific.  A \emph{convex
subdivision} is a planar subdivision whose faces are all convex
polygons, except the outer face, which is the complement of a convex
polygon.  A \emph{triangulation} is a convex subdivision in which each
face has at most 3 edges on its boundary.

Note that, if every face of $G$ has a constant number of sides, then
$G$ can be augmented, by the addition of extra edges, so that it is a
triangulation without increasing \eqref{entropy} by more than a
constant.  Similarly, in a convex subdivision $G$ where the query
distribution $D$ is uniform within each face of $G$, the faces of the
subdivision can be triangulated without increasing the entropy by more
than a constant \cite{amm00}. Thus, in the following we will simply
refer to results about triangulations where it is understood that
these also imply the same result for planar subdivisions with faces of
constant size or convex subdivisions when the query distribution is
uniform within each face.

Arya \etal\ \cite{acmr00} gave two results for the case where the
query point $p$ is chosen from a known distribution where the $x$ and
$y$ coordinates of $p$ are chosen independently and $G$ is a convex
subdivision.  They give a linear space data structure for which the
expected number of point-line comparisons is at most $4H+O(1)$ and a
quadratic space data structure for which the expected number of
point-line comparisons is at most $2H+O(1)$.  The assumption about the
independence of the $x$ and $y$ coordinates of $p$ is crucial to the
these results.

For arbitrary distributions that are known in advance, several results
exist.  Iacono \cite{i01,i04} showed that, for triangulations, a
simple variant of Kirkpatrick's original point location structure
gives a linear space, $O(H+1)$ expected query time data structure.  A
result by Arya \etal\ \cite{amm00} gives a data structure for
triangulations that uses $H + O(H^{2/3}+1)$ expected number of
comparisons per query and $O(n\log n)$ space.  The space requirement
of this latter data structure was later reduced, by the same authors,
to $O(n\log^* n)$ \cite{amm01a}.  The same three authors
\cite{amm01b} also showed that a variant of Mulmuley's randomized algorithm
gives, for triangulations, a simple $O(H+1)$ expected query time,
linear space data structure.  Most recently, Arya \etal\
\cite{ammw07}, have given an $O(n)$ space structure for point-location
in triangulations with query time $H+O(H^{1/2}+1)$. 

\begin{thm}[Arya \etal\ 2007]\thmlabel{ammw07}
Let $G$, $D$, $n$, and $H$ be as defined above.  Then there exists a
data structure of size $O(n)$ that can be constructed in $O(n\log n)$
time, and for which the expected number of point/line comparisons
required to locate the face of $G$ containing a query point $p$, drawn
according to $D$, is $H + O(\sqrt{H}+1)$.
\end{thm}
 
The above collection of results suggest that point location, and even
distribution-sensitive point location, is a well-studied and
well-understood problem, with solutions that are optimal up to
lower order terms.  However, in the above results there is a glaring
omission.  Given a convex polygon $P$, a folklore $O(\log n)$ time
algorithm exists to test if a query point $p$ is contained in $P$ and
this algorithm is optimal, in the worst case \cite{ps85}.  Testing if
$p\in P$ is a special case of point location in a convex subdivision
in which the subdivision has only 2 faces.  Thus, we might expect that, if
$p$ is drawn according to some distribution over $\R^2$, it may be
possible to do better in many cases. How much better?  It is certainly
not possible to achieve the entropy bound in all cases since, when
$f=2$ the entropy bound is at most 1.

We begin our investigation of distribution-sensitive point location
with the fundamental problem of testing if a query point $p$, drawn
from an arbitrary distribution $D$ over $\R^2$, is contained in a
convex polygon $P$.  We describe a hierarchical triangulation
$T=T(P,D)$ of $\R^2$ that we use to simultaneously achieve two
objectives:
\begin{enumerate}
\item $T$ is used with a query algorithm to check if a point $p$ 
	is contained in $P$, and
\item $T$ is used to give a lower bound on the expected cost of
	any linear decision tree that tests if a point $p$ selected
	according to $D$ is contained in $P$.
\end{enumerate}
The lower bound in Point~2 matches, to within a constant factor, the
expected query time of the algorithm in Point~1.  Thus, among
algorithms that can be expressed as linear decision trees, our
algorithm is optimal. This result is the first to give
\emph{any} lower bound on the expected complexity of \emph{any} point
location problem that exceeds the entropy bound. Proving the lower
bound is by far the hardest part of our results.  

In proving the lower bound, we show that finding (near) optimal point
location structures is essentially equivalent to finding (near)
minimum-entropy conforming triangulations, and we develop tools for
proving upper and lower bounds on the entropy of the minimum-entropy
conforming triangulation of a geometric graph.  By applying these
tools and combining them with the recent results of Arya \etal\
\cite{ammw07}, we obtain linear-space data structures for point
location in convex subdivisions and, more generally, in connected
planar subdivisions.  The expected query time of these data structures
is optimal, up to a lower order term, in the linear decision tree
model of computation.  More precisely, the data structures answer a query
in expected time $O(\tilde H)$, where $\tilde H=\tilde H(G,D)$ is a
lower bound on the expected cost of any linear decision tree for point
location in the subdivision $G$ with query distribution $D$.  In terms
of comparisons, the expected number of point/line comparisons
performed by our data structure is $\tilde H + O(\tilde H^{2/3}+1)$.

Note that all known algorithms for planar point location that do not
place special restrictions on the input subdivision can be described
in the linear decision tree model of computation.\footnote{Although
significant breakthroughs have recently been made in this area
\cite{c06,p06}, we deliberately do not survey algorithms that require
the vertices of the subdivision to be on integer coordinates.}  The
data structures presented in the current paper are the most general
results known about planar point location and imply, to within a
lower order term, all of the results discussed in the introduction.

The remainder of this paper is organized as follows:  \Secref{prelim}
presents definitions and notations used throughout the paper.
\Secref{polygons} discusses algorithms and lower bounds for point
location in convex polygons.  \Secref{subdivisions} presents
algorithms and lower bounds for point location in convex subdivisions.
\Secref{simple} extends these results to simple (non-convex) planar
subdivisions.


\section{Preliminaries}
\seclabel{prelim}

In this section we give definitions, notations, and background
required in subsequent sections.

\paragraph{Triangles and Convex Polygons.}  For the purposes of this
paper, a \emph{triangle} is the common intersection of at most 3
closed halfplanes.  This includes triangles with infinite area and
triangles having 0, 1, 2, or 3, vertices. Similarly, a \emph{\kangle}
is the common intersection of at most $k$ closed halfplanes.  For a
closed region $X\subseteq \R^2$, a \emph{\kangulation} of $X$ is a set
of \kangles\ whose interiors are pairwise disjoint and whose union is
$X$.  We use the convention that, unless $X$ is explicity mentioned,
the \kangulation\ in question is a \kangulation\ of $\R^2$.  A
\kangle\ conforms to a geometric graph $G$ if the interior of the
\kangle\ does not intersect any edge or vertex of $G$.  A
\kangulation\ $\Delta$ of $X$ \emph{conforms} to a geometric graph $G$
if each \kangle\ of $\Delta$ conforms to $G$.

\paragraph{Classification Problems and Classification Trees.}

A \emph{classification problem} over a domain $\mathcal{D}$ is a
function $\mathcal{P}:\mathcal{D}\mapsto \{0,\ldots,k-1\}$.  The
special case in which $k=2$ is called a \emph{decision problem}.  A
$d$-ary \emph{classification tree} is a full $d$-ary tree\footnote{A
full $d$-ary tree is a rooted ordered tree in which each non-leaf node
has exactly $d$ children.} in which each internal node $v$ is labelled
with a function $P_v:\mathcal{D}\mapsto\{0,.\ldots,d-1\}$ and for
which each leaf $\ell$ is labelled with a value
$d(\ell)\in\{0,\ldots,k-1\}$. The \emph{search path} of an input $p$
in a classification tree $T$ starts at the root of $T$ and, at each
internal node $v$, evaluates $i=P_v(p)$ and proceeds to the $i$th
child of $v$.  We denote by $T(p)$ the label of the final (leaf) node
in the search path for $p$.  We say that the classification tree $T$
\emph{solves} the classification problem $\mathcal{P}$ over the domain
$\mathcal{D}$ if, for every $p\in \mathcal{D}$, $\mathcal{P}(p)=T(p)$.

Unless specifically mentioned otherwise, classification trees are
binary classification trees.  For a node $v$ in a (binary)
classification tree, its left child, right child, and parent are
denoted by $\lft(v)$, $\rght(v)$ and $\prnt(v)$, respectively.


\paragraph{Probability.}

For a distribution $D$ and an event $X$, we denote by $D_{|X}$ the
distribution $D$ conditioned on $X$.  That is, the distribution where
the probability of an event $Y$ is $\Pr(Y|X)=\Pr(Y\cap X)/\Pr(X)$.
For any set $X$, we use the shorthand $\cup X$ to denote
$\bigcup_{x\in X} x$.  For a set $S$ of subsets of $\R^2$, we define
the \emph{induced entropy of $S$}, denoted by $H(S)$ as
$H(S)=\sum_{s\in S}\Pr(s|{\cup S})\log(1/\Pr(s|{\cup S}))$, where the
probability measure used is the query distribution $D$.

The probability measures used in this paper are usually defined over
$\R^2$.  We make no assumptions about how these measures are
represented, but we assume that an algorithm can, in constant time,
determine the $\Pr(t)$ for any triangle $t$.

For a classification tree $T$ that solves a problem
$P:\mathcal{D}\mapsto\{0,\ldots,k-1\}$ and a probability measure $D$
over $\mathcal{D}$, the \emph{expected search time} of $T$ is the
expected length of the search path for $p$ when $p$ is drawn at random
from $\mathcal{D}$ according to $D$.  Note that, for each leaf $\ell$
of $T$ there is a maximal subset $r(\ell)\subseteq \mathcal{D}$ such
that the search path for any $p\in r(\ell)$ ends at $\ell$.  Thus, the
expected search time of $T$ (under distribution $D$) can be written as
\[
     \mu_D(T) = \sum_{\ell\in L(T)} \Pr(r(\ell))\times \depth(\ell)
	\enspace ,
\]
where $L(T)$ denotes the leaves of $T$ and $\depth(\ell)$ denotes the
length of the path from the root of $T$ to $\ell$.

The following theorem, which is a restatement of (half of) Shannon's
Fundamental Theorem for a Noiseless Channel \cite[Theorem 9]{s48}, is
what all existing results on distribution-sensitive planar point
location use to establish their optimality:

\begin{thm}[Fundamental Theorem for a Noiseless Channel]\thmlabel{shannon}
Let $\mathcal{P}:\mathcal{D}\mapsto \{0,\ldots,k-1\}$ be a classification
problem and let $p\in \mathcal{D}$ be selected from a distibution $D$ such
that $\Pr\{\mathcal{P}(p)= i\}=p_i$, for $0\le i< k$.  Then, any
$d$-ary classification tree $T$ that solves $\mathcal{P}$ has
\begin{equation}
     \mu_D(T) \ge \sum_{i=0}^{k-1} p_i\log_d(1/p_i) \enspace .
	\eqlabel{shannon}
\end{equation}
\end{thm}
\thmref{shannon} is typically applied to the point location problem by
treating point location as the problem of classifying the query point
$p$ based on which face of $G$ contains it.  In this way, we obtain
the lower bound in \eqref{entropy-face}.


\section{Point In Convex Polygon Testing}
\seclabel{polygons}
\seclabel{convex}

Let $P$ be a convex $n$-gon whose boundary is denoted by $\partial P$
and consider a probability measure $D$ over $\R^2$.  For technical
reasons, we use the convention that $P$ does not contain its boundary
so that $p\in \boundary P$ implies $p\not\in P$.  In this section we
are interested in preprocessing $P$ and $D$ so that we can quickly
solve the decision problem of testing whether a query point $p$, drawn
according to $D$, is contained in $P$. 

In particular, we are interested in algorithms that can be described
as \emph{linear decision trees}.  These are decision trees such that
each internal node $v$ contains a linear inequality $P_v(x,y)=ax+by
\ge c$, and the node evaluates to 1 or 0 depending on whether the
query point $(x,y)$ satisifies the inequality or not, respectively.
We require that, for every $p\in\R^2$ the leaf reached in the search
path for $p$ is labelled with a 1 if and only if $p\in P$.
Geometrically, each internal node of $T$ is labelled with a directed
line and the decision to go to the left or right child depends on
whether $p$ is to the left or right (or on) this line.  

Our exposition is broken up into three sections.  We begin by
describing a data structure (in fact, a decision tree) that tests if
query point $p$ is contained in a convex polygon  $P$.  Next, we give
an (easy) analysis of the expected search time of this data structure.
Finally, we give a (more difficult) proof that this expected search
time is optimal.


\subsection{Triangle Trees}

At a high level, our data structure works by creating a sequence of
successively finer approximations $A_0,\ldots,A_k$ to $\boundary P$.
Each approximation $A_i$ consists of two convex polygons; an
\emph{outer approximation} that contains $P$ and an \emph{inner
approximation} that is contained in $P$.  For an edge $e$ of $P$, let
$e_0$, respectively, $e_1$, denote the first, respectively, endpoint
of $e$ that we encounter when traversing the boundary of $P$ in
counterclockwise order, starting from a point $p\not\in e$.

Each approximation $A_i$ is completely defined by a set $S_i$ of edges
of $P$.  The inner approximation $I_i$ is simply the convex hull of $S_i$.
The outer approximation $O_i$ is the common intersection of all closed
halfplanes whose bounding lines contains the edges in $S_i$ and whose
interiors contain $P$.  We ensure that successive approximations have
a containment relationship, i.e., $A_i\supseteq A_{i+1}$, by choosing
our edges so that $S_i\subseteq S_{i+1}$.

Next we define the sets $S_0,\ldots,S_k$ that define our
approximations.  The set $S_0$ is empty, and we use the convention
that the outer approximation in this case is the entire plane and the
inner approximation is the empty set. 

The set $S_1$ consists of any two edges, $x$ and $y$ so that each of
the two connected components of $O_1\setminus I_1$ has probability at
most $1/2$.  The existence of $x$ and $y$ is guaranteed, for example,
by the planar Ham Sandwich Theorem \cite{bz04}, and such a pair of
edges is easily found in $O(n)$ time.

We now show how, for $i\ge 1$, to obtain the set $S_{i+1}$ from the
set $S_{i}$.  Let $e_0,\ldots,e_{m_i-1}$ be the edges in $S_i$ as they
occur in conterclockwise order around the boundary of $P$.  The
approximation $A_i$ thus consists of $m_i$ edges of $P$ joining a
sequence of triangles triangles $\Delta_0,\ldots,\Delta_{m_i-1}$ where
$\Delta_j$ is the common intersection of the following halfspaces:

\begin{enumerate}
\item the halfspace to the right of the directed line through 
	the $e_{j,1}$ and $e_{j+1,0}$,
\item the halfspace that contains $P$ and whose bounding line contains
$e_j$, and
\item the halfspace that contains $P$ and whose bounding line contains
$e_{j+1}$.
\end{enumerate}

Refer to \figref{split}. For each triangle $\Delta_j$ that is not
completely contained in $P$ we find an edge $e_{j.5}$ of $P$ that
occurs between $e_j$ and $e_{j+1}$ and such that adding $e_{j.5}$ to
$S_{i+1}$ results in two triangles
$\Delta_{j,0}$ and $\Delta_{j,1}$ of $A_{i+1}$ contained in $\Delta_i$
and that each have the property that
\[  
     \Pr(\Delta_{j,b}) \le \Pr(\Delta_{j})/2 \enspace ,
\]
for $b\in\{0,1\}$.
We call the two triangles $\Delta_{j,0}$ and $\Delta_{j,1}$ the
\emph{children} of $\Delta_j$ and we say that this operation
\emph{splits} $\Delta_j$ into these two triangles.

\begin{figure}
\begin{center}
\begin{tabular}{cc}
\includegraphics{crack-1} & \includegraphics{crack-2} \\
(a) & (b)
\end{tabular}
\end{center}
\caption{Splitting $\Delta_j$ into its two children $\Delta_{j,0}$ and
$\Delta_{j,1}$.}
\figlabel{split}
\end{figure}

The entire process terminates at the first value of $k$ for which
$A_k$ is completely contained in $P$.  The approximations
$A_0,\ldots,A_k$ are stored as a binary tree $T=T(P,D)$ that we call a
\emph{triangle tree} for $P$ and $D$.  Each node $v$ of $T$ at level
$i$ in $T$ corresponds to an open triangle $\Delta(v)$ in
approximation $A_{i}$ and the two children of $v$ correspond to the
two open triangles into which $\Delta(v)$ is split. See \figref{tree}.
A crucial property of this construction guaranteed by the splitting
process is that, for any node $v$ at distance $i$ from the root
of $T$, $\Pr(\Delta(v))\le 1/2^i$.

\begin{figure}
  \begin{center}
    \begin{tabular}{cccc}
      \includegraphics{t0} & 
      \includegraphics{t1} & 
      \includegraphics{t2} & 
      \includegraphics{t3} \\
      $A_0$ & $A_1$ & $A_2$ & $A_3$ 
    \end{tabular}
  \end{center}
  \caption{The sequence $A_0,\ldots,A_3$ that approximate $\boundary P$.}
  \figlabel{tree}
\end{figure}

To use the tree $T$ to determine if a point $p$ is contained in $P$ we
proceed top-down, starting at the root.  For a point $p$ contained in
$\Delta(v)$ one of two things can happen: (1) $p$ is contained in one
of the two open triangles $\Delta(\lft(v))$ or $\Delta(\rght(v))$ in
which case we recurse on the right or left child of $v$, respectively,
or (2) we can determine in constant time if $p\in P$.

Before we proceed, we make note of the following simple lemma that
will be crucial to our arguments in subsequent sections.

\begin{lem}\lemlabel{gen-indep}
Let $P$ be a convex polygon, let $D$ be a probability measure over
$\R^2$, and let $T=T(P,D)$ be a triangle tree for $P$ and $D$.
Let $v_1,\ldots,v_m$ be a set of nodes in $T$ such $v_i$ is not an
ancestor of $v_j$ for any $1\le i,j\le m$.  Then, for any \kangle\
$t$ that conforms to $P$, there exists at most $k$ indices $i$ such that
$t$ intersects $\Delta(v_i)$.
\end{lem}

\begin{proof}
To be added later.
\end{proof}

\subsection{Analysis of the Triangle Tree}

Let $T=T(P,D)$ be a triangle tree for a convex $n$-gon $P$ and a
probability measure 
$D$ over $\R^2$. Define, for each node $v$ of $T$,
\[
   \Xi(v)=\Delta(v)\setminus (\Delta(\lft(v))\cup \Delta(\rght(v)))
\]
and define $\Pr(v)=\Pr(\Xi(v))$.  
In many cases, $\Xi(v)$ has two connected components, one outside of
$P$ and one inside of $P$, which we denote
by $\Xi_0(v)$ and $\Xi_1(v)$, respectively.
Notice that the search for a point
$p$ terminates at $v$ precisely when $p\in\Xi(v)$.  Thus, $\Pr(v)$ is
the probability that a search terminates at node $v$.  For a set $V$
of nodes in $T$ we use the notation $\Pr(V)=\sum_{v\in V}\Pr(v)$ to
denote the probability that the search path ends at some node in $V$.

\begin{lem}\lemlabel{upper-bound}
A triangle tree $T$ contains $O(n)$ nodes and can be constructed in
$O(n)$ time.
Using the triangle tree, the expected number of linear inequalities
required to check if $p\in P$ for a point $p$ drawn from $D$ is at
most
\[
 \mu_D(T) 
   \le O(1)+O(1)\times \sum_{v\in T}\Pr(v)\log(1/\Pr(v)) \enspace .
\]
\end{lem}

\begin{proof}

Observe that each internal node of $T$ is split by choosing some edge
$e$ of $P$, and each edge is used to split at most one node of $T$.
This implies that $T$ has $O(n)$ nodes.
To obtain an $O(n)$ time algorithm to construct $T$ we apply a trick
used by Mehlhorn \cite{m75} in the construction of biased binary
search trees.  Finding an edge $e$ that splits a node $v$ can be done
in $O(\log (\min\{m-k,k\}))$ time, where $m$ is the number of edges of $P$ that
contained in $\Delta(v)$ and $k$ is the rank of $e$ in this set of
edges.  In this way, the overall running time of the
construction algorithm is given by the recurrence
\[
    T(n) \le T(n-k) + T(k) + O(\log(\min\{n-k,k\}))
\]
which resolves to $O(n)$.

The expected running time of the query algorithm on $T$ is follows
immediately from the fact that, for any node $v$ at a distance of $i$
from the root of $T$, $\Pr(v) \le \Pr(\Delta(v)) \le 1/2^i$.
\end{proof}


\subsection{Optimality of the Triangle Tree}

Next we will show that the performance bound given by
\lemref{upper-bound} is optimal.  More precisely, we show that there
is no linear decision tree whose expected search time (on distribution
$D$) is asymptotically better than that of the triangle tree $T$.  The
argument proceeds as follows:  First we show that the expected search
time of any decision tree is (almost) lower bounded by the cost of the
minimum-entropy conforming triangulation of $P$
(\lemref{triangulate}).  Next, we develop a technique for lower
bounding the entropy of one triangulation in terms of another
(\lemref{pieces}).  Finally, we use this technique to show how the
entropy of the triangulation defined by the triangle tree $T$ can be
used to lower bound the entropy of the minimum-entropy triangulation
(\lemref{partition} and \lemref{big-bugger}). The loose ends are then
wrapped up to produce the proof of \lemref{lower-bound}.

\begin{lem}\lemlabel{triangulate}
Let $T^*$ be any linear decision tree for a classification problem
$\mathcal{P}$ over $\R^2$.  Then there exists a linear decision tree
$T'$ for $\mathcal{P}$, such that, for each leaf $\ell$ of $T'$,
$r(\ell)$ is a triangle and $T'$ satisifies
\[
    \mu_D(T') \le \mu_D(T^*) + O(\log\mu_D(T^*))
\]
for any probability measure $D$ over $\R^2$.
\end{lem}

\begin{proof}
Each leaf $\ell$ of $T^*$ has a region $r(\ell)$ that is a convex
polygon.  If $r(\ell)$ has $k$ sides then the depth of $\ell$ in $T$
is at least $k$.  To obtain the tree $T'$ replace each such leaf
$\ell$ of $T^*$ by a balanced binary tree of depth $O(\log k)$ by
repeatedly splitting the leaf into two children $\ell_1$ and $\ell_2$
whose regions have $\ceil{(k+2)/2}$ and $\floor{(k+2)/2}$ vertices.
For a leaf $\ell\in L(T)$, let $s(\ell)$ denote the set of leaves in $T'$
in the subtree of $\ell$.   Then
\begin{eqnarray*}
   \mu_D(T^*) 
     &  =  & \sum_{\ell\in L(T^*)} \Pr(r(\ell))\cdot \depth(\ell) \\
     &  =  & \sum_{\ell\in L(T^*)}\sum_{\ell'\in s(\ell)} 
              \Pr(r(\ell'))\cdot \depth(\ell) \\
     & \ge & \sum_{\ell\in L(T^*)} 
             \sum_{\ell'\in s(\ell)}\Pr(r(\ell'))\cdot (\depth(\ell')
                   - O(\log (\depth(\ell)))) \\
     &  =  & \mu_D(T') - \sum_{\ell\in L(T^*)} 
             \sum_{\ell'\in s(\ell)}\Pr(r(\ell'))\cdot O(\log (\depth(\ell))) \\
     &  =  & \mu_D(T') - \sum_{\ell\in L(T^*)} 
             \Pr(r(\ell))\cdot O(\log (\depth(\ell))) \\
     & \ge & \mu_D(T') - O(\log(\mu_D(T^*)) \enspace , 
\end{eqnarray*}
where the last inequality is an application of Jensen's Inequality.
\end{proof}

\lemref{triangulate} says that for any linear decision tree for point
location, there is an underlying triangulation.  The entropy of this
triangulation gives a lower bound on the cost of the decision tree.
Thus, one way to give lower bounds for linear decision trees is to
give lower bounds on the entropy of a minimum-entropy triangulation.

To obtain lower bounds on the entropy of a minimum-entropy
triangulation $\Delta^*$, consider the following easily proven
observation: If each triangle in $\Delta^*$ intersects at most $c$
triangles of some \kangulation\ $\Delta$ then $H(\Delta^*) \ge
H(\Delta) - \log c$.\footnote{Proof: Consider the set $X=\{
t^*\cap t : t^*\in\Delta^*, t\in \Delta\}$. Each triangle of
$\Delta^*$ contributes at most $c$ pieces to $X$, so we have
$H(\Delta) \le H(X) \le H(\Delta^*) + \log c$.}  This observation
allows us to use a \kangulation\ $\Delta$ to prove a lower bound on
the entropy of a triangulation $\Delta^*$.  Unfortunately, the
condition that each triangle of $\Delta^*$ intersect at most $c$
triangles of $\Delta$ is too restrictive for our purposes.  Instead,
we require following stronger result:

\begin{lem}\lemlabel{pieces}
Let $D$ be a probability measure over $\R^2$.  Let $\Delta$ be a
\kangulation, let $\Delta^*$ be triangulation, and let
$\{\Delta_1,\ldots,\Delta_m\}$ be a partition of the \kangles\
in $\Delta$.  Suppose that, for all $i\in\{1,\ldots,m\}$ and for each
triangle $t^*\in\Delta^*$, $t^*$ intersects
at most $c$ \kangles\ in $\Delta_i$.  Then
\begin{eqnarray*}
   H(\Delta) \le 
%     \sum_{i=1}^k \Pr(\cup\Delta_i) \cdot H(\Delta_i|{\cup\Delta_i}) - \log c
	 H(\Delta^*) + H(\{\cup\Delta_1,\ldots,\cup\Delta_m\}) + \log c
 \enspace . 
\end{eqnarray*}
\end{lem}

Intuitively, \lemref{pieces} can be thought of as follows:  If we tell
an observer which of the $\Delta_i$ a point $p$ drawn according to $D$
occurs in then the amount of information we are giving the observer
about the outcome is $H(\{\cup\Delta_1,\ldots,\cup\Delta_m\})$.
However, after giving away this information, we are able to apply the
simple observation in the previous paragraph, since each triangle in
$\Delta^*$ intersects at most $c$ elements of each $\Delta_i$.  Thus,
\lemref{pieces} is really just $m$ applications of the simple
observation.  The following proof formalizes this:

\begin{proof} Help Vida: Justify the first line of the following please 
--- I'm stuck.
Intuition: LHS represents picking a point $p$ according to $D$ and
asking how much uncertainty there is about which triangle of
$\Delta^*$ contains $p$.  The RHS represents picking $p$, telling you
which of the $\Delta_i$ contains $p$ and then asking how much
uncertainty remains about which triangle of $\Delta^*$ contains $p$.
Surely knowing which of the $\Delta_i$ contains $p$ does not increase
the uncertainty.
\begin{eqnarray*}
   H(\Delta^*) 
     & \ge & \sum_{i=1}^m \Pr(\Delta_i)H(\Delta^*|\Delta_i) \\
     &  =  & \sum_{i=1}^m \Pr(\Delta_i)\sum_{t^*\in\Delta^*}
       \Pr(t^*|\Delta_i)\log(1/\Pr(t^*|\Delta_i)) \\
     &  =  & \sum_{i=1}^m \sum_{t^*\in\Delta^*}
       \Pr(t^*\cap\Delta_i)\log(\Pr(\Delta_i)/\Pr(t^*\cap\Delta_i)) \\
     &  =  & \sum_{i=1}^m \sum_{t^*\in\Delta^*}
       \Pr(t^*\cap\Delta_i)\log(1/\Pr(t^*\cap\Delta_i)) 
        + \sum_{i=1}^m \Pr(\Delta_i)\log(\Pr(\Delta_i)) \\
     &  =  & \sum_{i=1}^m \sum_{t^*\in\Delta^*}
       \Pr(t^*\cap\Delta_i)\log(1/\Pr(t^*\cap\Delta_i)) 
        - H(\{\cup\Delta_1,\ldots,\cup\Delta_m\}) \\
     & \ge  & \sum_{i=1}^m \sum_{t^*\in\Delta^*}\sum_{t\in\Delta}
       \Pr(t^*\cap t)\log(1/\Pr(t^*\cap t)) 
        -\log c - H(\{\cup\Delta_1,\ldots,\cup\Delta_m\}) \\
     & \ge  & \sum_{i=1}^m \sum_{t\in\Delta}
       \Pr(t)\log(1/\Pr(t)) 
        -\log c - H(\{\cup\Delta_1,\ldots,\cup\Delta_m\}) \\
     &  =  & H(\Delta) -\log c - H(\{\cup\Delta_1,\ldots,\cup\Delta_m\})
\end{eqnarray*}
\end{proof}


The remainder of our argument involves carefully partitioning the
triangles of $T$ into subsets that are compatible with \lemref{pieces}
and that give a lower-bound that matches the upper-bound in
\lemref{upper-bound}.
We begin by partitioning the internal nodes of $T$ in
\emph{groups} $G_1,G_2,\ldots$ where
\[
	G_i = \{v\in T : 1/2^{i} \le \Pr(v) < 1/2^{i-1} \} \enspace .
\]
In what follows, we fix some a real number $0< \alpha < 1$ that will be
specified later.   The following lemma gives the structure of the sets
that will be used in the application of \lemref{pieces}.

\begin{lem}\lemlabel{partition}
Each group $G_i$ can be partitioned into $r_i$ subgroups
$G_{i,1},\ldots,G_{i,r_i}$ such that
\begin{enumerate}
\item There is an integer $t_i$ such that $r_i-t_i\le 2^{\alpha i}$.

\item $|G_{i,j}| \ge 2^{\alpha i} / i$, for all $1\le j \le t_i$, and

\item for every $1\le j< r_i$ and every $u,v\in G_{i,j}$, $u\neq v$, 
node $u$ is not an ancestor of node $v$ in $T$. 
\end{enumerate}
\end{lem}

\begin{proof}
Assume that $|G_i|> 2^{\alpha i}$, otherwise simply set $t_i=0$ and
place each element of $G_i$ into a single group so that $r_i=|G_i|$.
Next, observe that all vertices in $G_i$ appear within the first $i$ levels
of $T$.  Thus, any node in $G_i$ has at most $i-1$ ancestors in $T$.  

We can obtain the first subgroup $G_{i,1}$ by first defining all nodes of
$G_i$ to be \emph{unmarked} and \emph{unselected}.  To obtain
$G_{i,1}$ we repeatedly \emph{select} any unselected and unmarked
node $v\in G_i$ that does not have any descendants in $G_i$ and
\emph{mark} the (at most $i-1$) ancestors of $v$ in $T$.  This
process selects at least
\[
   |G_i|/i \ge 2^{\alpha i}/i
\] 
elements to take part in $G_{i,1}$.  We can then apply this process
recursively on $G_i\setminus G_{i,1}$ to obtain the sets
$G_{i,2},\ldots,G_{i,t_i}$.  The recursion completes when there are
fewer than $2^{\alpha i}$ elements remaining, at which point the
remaining elements are placed into singleton groups.
\end{proof}

Next we apply \lemref{triangulate} and \lemref{partition} to obtain a
lower bound for triangulations.  For a triangle tree $T$, let
$\Delta(T)=\{\Xi_b(v) : v\in T, b\in\{0,1\}  \}$ be the \kangulation\
induced by $T$.  Note that each vertex in $T$ contributes at most 2
\kangles\ to $\Delta(T)$.  This implies that $H(\Delta(T))$ is closely
related to $\mu_D(T)$.  In particular, $\mu_D(T)=\Theta(H(\Delta(T))+1)$.

\begin{lem}\lemlabel{big-bugger}
Let $P$ be a convex polygon, let $D$ be a probability measure over
$\R^2$, and 
let $\Delta=\Delta(T)$ be the \kangulation\ induced by a triangle tree
$T=T(P,D)$. Then, for any triangulation $\Delta^*$ conforming to $P$,
\[
    H(\Delta) \le H(\Delta^*) + O(H(\Delta)^{2/3}+1) \enspace .
\]
\end{lem}

\begin{proof}
For any group $G_{i,j}$ defined by \lemref{partition}, Condition~3 and
\lemref{gen-indep} ensures that any triangle of $\Delta^*$ can
intersect at most 3 \kangles\ of $G_{i,j}$.  Therefore, applying
\lemref{pieces} with $c=3$, we obtain:
\[ 
 H(\Delta) \le 
   H(\Delta^*) + H(\{{\cup G_{i,j}} : i\in\N , j\in\{1,\ldots,r_i\}\}) 
   + O(1)  \enspace .
\]
Thus, all that remains is to show that the contribution of $\overline
H=H(\cup\{G_{i,j}\} : i\in\N , j\in\{1,\ldots,r_i\}\})$ is at most
$O(H(\Delta)^{2/3})$.
\begin{eqnarray*}
\overline H 
 & = & H(\cup\{G_{i,j}\} : i\in\N , j\in\{1,\ldots,r_i\}\}) \\
 & = & \sum_{i=1}^\infty
         \sum_{j=1}^{r_i}\Pr(\cup G_{i,j})\log(1/\Pr(\cup G_{i,j})) \\
 & = & \sum_{i=1}^\infty
         \sum_{j=1}^{t_i}\Pr(\cup G_{i,j})\log(1/\Pr(\cup G_{i,j})) 
         + \sum_{i=1}^\infty
         \sum_{j=t_i+1}^{r_i}\Pr(\cup G_{i,j})\log(1/\Pr(\cup G_{i,j})) \\
 & \le & \sum_{i=1}^\infty
         \sum_{j=1}^{t_i-1}\Pr(\cup G_{i,j})\log(1/\Pr(\cup G_{i,j})) 
         + \sum_{i=1}^{\infty} (2^{\alpha i}/ 2^{i-1})\log(2^i) \\
 &  =  & \sum_{i=1}^\infty
         \sum_{j=1}^{t_i-1}\Pr(\cup G_{i,j})\log(1/\Pr(\cup G_{i,j})) 
         + 2\cdot\sum_{i=1}^{\infty} i/2^{(1-\alpha)i} \\
 &  =  & \sum_{i=1}^\infty
         \sum_{j=1}^{t_i-1}\Pr(\cup G_{i,j})\log(1/\Pr(\cup G_{i,j}))
         + O(1/(1-\alpha)^2) \enspace ,
\end{eqnarray*}
where the last equality is obtained using Taylor series.
Continuing, we get 
\begin{eqnarray*}
\overline H
 &  =  & \sum_{i=1}^\infty
         \sum_{j=1}^{t_i-1}\Pr(\cup G_{i,j})\log(1/\Pr(\cup G_{i,j}))
         + O(1/(1-\alpha)^2) \\
 & \le  & \sum_{i=1}^\infty
         \sum_{j=1}^{t_i-1} \Pr(\cup G_{i,j})
              \log(i2^{i}/2^{\alpha i})
         + O(1/(1-\alpha)^2) \\
 & \le  & \sum_{i=1}^\infty
         \sum_{j=1}^{t_i-1} \Pr(\cup G_{i,j})((1-\alpha)i+\log i)
         + O(1/(1-\alpha)^2) \\
 & \le  & \sum_{i=1}^\infty
         \sum_{j=1}^{t_i-1} \Pr(\cup G_{i,j})((1-\alpha)\log(1/\Pr(\cup G_{i,j})) + \log\log(1/\Pr(\cup G_{i,j})))
         + O(1 + 1/(1-\alpha)^2) \\
 & \le &  (1-\alpha)H(\Delta) + \log(H(\Delta)) + O(1 + 1/(1-\alpha)^2) \\
 & \le &  O(H(\Delta)^{2/3}+ 1)
\end{eqnarray*} 
where the second last inequality follows from Jensen's Inequality and the
last inequality is obtained by setting $\alpha=1-1/H(\Delta)^{1/3}$.
\end{proof}

We now have all the tools we need to prove our lower bound.

\begin{lem}\lemlabel{lower-bound}
Let $\Delta=\Delta(T)$ denote the set of triangles in a triangle tree
$T=T(P,D)$.
Any linear decision tree $T^*$ that solves the problem of testing 
if any query point 
$p\in\R^2$ is contained in $P$ has
\[
   \mu_D(T^*) \ge H(\Delta) - O(H(\Delta)^{2/3}+1) \enspace .
\]
\end{lem}

\begin{proof}
Let $T^*$ be a linear decision tree for point location in $P$.  Apply
\lemref{triangulate} to $T^*$ to obtain a decision tree $T'$ for point
location in $P$, all of whose leaves have regions that are triangles.
Let $\Delta'$ be the set of triangles corresponding to the leaves of
$T'$, and let $\Delta=\Delta(T)$ be the \kangulation\ induced by
$T$.  The triangulation $\Delta'$ is a 
conforming triangulations of $P$, so by \lemref{big-bugger}, we have
\[
    H(\Delta) \le H(\Delta')  + O(H(\Delta)^{2/3} + 1) \enspace .
\]
Since $H(\Delta')$ is a lower bound on $\mu_D(T')$ this gives
\[
   \mu_D(T') \ge H(\Delta) - O(H(\Delta)^{2/3} + 1) \enspace ,
\]
and finally
\[
   \mu_D(T^*) \ge \mu_D(T') - O(\log\mu_D(T'))
            \ge H(\Delta) - O(H(\Delta)^{2/3} + 1) \enspace ,
\]
completing the proof.
\end{proof}

It is worth noting the differences between the lower bound in
\lemref{lower-bound} and the upper bound in \lemref{upper-bound}.  The
upper bound is stated in terms of the expected depth of a node in the
triangle tree $T$, while the lower bound is in terms of the entropy
$H(\Delta)$ of the triangles in $T$.  Although the higher order term
in these bounds match, a more precise inspection reveals that the
number of point/line comparisons required to reach a node of depth $i$
in $T$ is as high as $4(i+1)$, so the search time in $T$ only matches
the lower bound to within a constant factor.  However, as we will see
in the next section, a tighter upper bound can be obtained by using
the point-location structure of \thmref{ammw07} for triangulations and
applying it to $\Delta(T)$. 

\section{Convex Subdivisions}
\seclabel{subdivisions}

In this section we consider the problem of point location in convex
subdivisions.  This should be considered a warm-up exercise to
introduce the techniques that are used in \secref{simple}.

The data structure is simple.  For each face $F_i$ of the convex
subdivision we \kangulate\ $F_i$ using the \kangles, $\Delta(T_i)$,
defined by a triangle tree $T_i=(F_i,D_{|F_i})$.  Note that
$\Delta(T_i)$ contains \kangles\ not $\R^2$ that we obtain.  in $F_i$,
but we discard these.  Let $\Delta$ denote the \kangulation\ of $\R^2$
obtained this way and let $\Delta_i$ denote the subset of $\Delta$
that are contained in $F_i$.  Next, we preprocess $\Delta$ using
\thmref{ammw07}.  Note that the \kangles\ in $\Delta$ have at most 4
sides so that they can be split into two triangles without increasing
the entropy by more than 1. Thus, the expected number of point/line
comparisons performed when using this structure is $H(\Delta) +
O(H(\Delta)^{1/2} + 1)$.  The following theorem shows that this is
nearly optimal:

\begin{thm}
Given a convex subdivision $G$ with $n$ vertices and a probability
measure $D$ over $\R^2$, a data structure $T$ of size $O(n)$ can be
constructed in $O(n)$ time that answers point location queries in $G$.
The expected number of point/line comparisons performed by $T$, 
for a point $p$ drawn according to $D$ is 
\[
  \mu_D(T) \le \mu_D(T^*) + O(\mu_D(T)^{2/3}+1) \enspace , 
\] 
where $T^*$ is any linear classification tree that answers point
location queries in $G$.
\end{thm}

\begin{proof}
The space and preprocessing requirements follow from
\lemref{upper-bound} and \thmref{ammw07}.
To prove the bound on the expected query time, apply
\lemref{triangulate} to the tree $T^*$ and consider the resulting tree
$T'$, each of whose leaves have regions that are triangles and such
that
\[
     \mu_D(T') \le \mu_D(T^*) + O(\log \mu_D(T^*)) \enspace .
\]
Observe that each leaf of $T'$ corresponds to a triangle in $\R^2$
that is completely contained in one of the faces of $G$.  Let
$\Delta'$ denote this set of triangles and let $\Delta'_i$ denote the
subset of $\Delta'$ contained in $F_i$.
Consider the entropy $H(\Delta')$ of the distribution induced by the
triangles of $T'$:
\begin{eqnarray*}
   H(\Delta') 
     & = & \sum_{i=1}^f \sum_{t\in \Delta'_i}\Pr(t)\log(1/\Pr(t)) \\
     & = & \sum_{i=1}^f \Pr(F_i)\sum_{t\in \Delta'_i}
            \Pr(t|F_i)\log(1/\Pr(t)) \\
     & = & \sum_{i=1}^f \Pr(F_i)\sum_{t\in \Delta'_i}
            \Pr(t|F_i)
            \left(
              \log(1/\Pr(t|F_i))-\log(\Pr(F_i))
            \right) \\
     & = & \sum_{i=1}^f \Pr(F_i)\sum_{t\in \Delta'_i}
            \Pr(t|F_i)\log(1/\Pr(t|F_i)) 
          +
      \sum_{i=1}^f \Pr(F_i) \log(1/\Pr(F_i)) \\
     & = & \sum_{i=1}^f \Pr(F_i) H(\Delta'_i) + H(F) \enspace .
\end{eqnarray*}
(Recall that $F=\{F_1,\ldots,F_f\}$ is the set of faces in $G$.)
Similarly, the entropy of $\Delta$ is given by 
\begin{eqnarray*}
   H(\Delta) 
     & = & \sum_{i=1}^f \sum_{t\in \Delta_i}\Pr(t)\log(1/\Pr(t)) \\
     & = & \sum_{i=1}^f \Pr(F_i)\sum_{t\in \Delta_i}
            \Pr(t|F_i)\log(1/\Pr(t|F_i)) 
          + H(F) \\
     & = & \sum_{i=1}^f \Pr(F_i) H(\Delta_i) + H(F) \enspace .
\end{eqnarray*}
By \lemref{lower-bound}, the \kangles\ in $T_i$ form a nearly-minimum
entropy \kangulation\ of $F_i$.  More specifically, 
\[  
   H(\Delta_i) \le H(\Delta'_i) + O(H(\Delta_i)^{2/3}+1)  \enspace .
\]
Putting this all together, we have
\begin{eqnarray*}
  H(\Delta) 
    &  =  & \sum_{i=1}^f\Pr(F_i) H(\Delta_i) + H(F) \\ 
    & \le & \sum_{i=1}^f\Pr(F_i) (H(\Delta'_i) + O(H(\Delta_i)^{2/3}+1)) + H(F) \\ 
    &  =  & H(\Delta') + \sum_{i=1}^f\Pr(F_i) O(H(\Delta_i)^{2/3}+1) \\ 
    & \le & \mu_D(T') + \sum_{i=1}^f\Pr(F_i) O(H(\Delta_i)^{2/3}+1) \\ 
    & \le & \mu_D(T') + O(H(\Delta)^{2/3}+1) \\
    & \le & \mu_D(T^*) + O(H(\Delta)^{2/3}+1)
\end{eqnarray*}
Finally, since we preprocess $\Delta$ using \thmref{ammw07}, the
expected number of comparisons required to answer a query is
\begin{eqnarray*}
  H(\Delta) + O(H(\Delta)^{1/2} + 1)
   & = & \mu_D(T^*) + + O(\mu_D(T^*)^{1/2} + H(\Delta)^{2/3} + H(\Delta)^{1/3} + 1) \\
   & = & \mu_D(T^*) + O(\mu_D(T)^{2/3} + 1)
\end{eqnarray*}
and this completes the proof.
\end{proof}

\section{Simple Planar Subdivisions}
\seclabel{simple}

Next, we consider how to handle simple (possibly non-convex) planar
subdivisions.  We take the same approach used in the previous section.
Namely, we subdivide the input into a \kangulation\ and apply
\thmref{ammw07}  on the resulting \kangulation.  As before, the trick
is in arguing that the triangulation we find has near-minimum entropy.
Since the faces of $G$ are simple polygons, we first tackle the
problem of finding a (near) minimum-entropy \kangulation of a simple
polygon.

\subsection{Low-Entropy Triangulations of Simple Polygons}
\seclabel{simple-triangulation}

Let $P$ be a simple polygon with vertices, as encountered in a
counterclockwise traversal of its boundary are $p_1,\ldots,p_n$.  In
this section, we show how to \kangulate\ the interior of $P$ to obtain
a set of \kangles\ $\Delta=\Delta(P,D)$ such that $H(\Delta)$ is
nearly minimum over all possible triangulations of $P$.  Although our
only goal is to construct the subdivision $\Delta$, it will be helpful
when analyzing $H(\Delta)$ to impose a tree structure on its \kangles.
We refer to this tree as the \emph{\kangle\ tree} for $(P,D)$ and
denote it by $T=T(P,D)$.

A \emph{funnel polygon} is a polygon whose boundary consists of two
reflex chains $X=x_1,\ldots,x_k$ and $Y=y_1,\ldots,y_{k'}$ where $x_1$
and $y_{k'}$ are joined by a (possibly zero length) edge, as are $x_k$
and $y_{1}$.  A \emph{half-funnel} is a funnel polygon in which one of
the reflex chains, say $X$, consists of a single edge.  For two edges
$e$ and $e'$ of $P$ the \emph{funnel} between $e$ and $e'$ is the
union of all shortest paths in $P$ from a point on $e$ to a point on
$e'$.  Note that a funnel is either a single funnel polygon or two
funnel polygons joined by a path.

To \kangulate\ $P$ we fix a \emph{root edge} $e$ of $P$ and find the
edge $e'$ of $P$ such that the funnel $f$ between $e$ and $e'$ has
property that each connected component $C$ of $P\setminus f$ has
$\Pr(C)\le 1/2$. The existence of such an edge $e'$ is easily
established by a standard continuity argument \cite{geoham}.  The
funnel $f$ corresponds to the root of the kangle tree $T$.

Next, we proceed differently depending on the structure of $f$.  If
$f$ consists of two funnel polygons joined by a path, then we make
each edge of this path a child of the root and we make each of the two
funnel polygons a child of the root.  Otherwise $f$ consists of a
single funnel polygon, in which case we don't do anything special ($f$
is the root of $T$).

Next, for each child $v$ of the root of $T$ that is a funnel polygon,
we split this funnel polygon using a single edge into two half-funnel
polygons.  These two half-funnels become children of $v$.

Since the boundary of a half-funnel $h$ consists of at most one reflex
chain $C$ and at most 3 edges that are not in $C$  we can triangulate
a half-funnel in the following way:  Take the convex hull $H$ of $C$,
treat it as a convex polygon and compute the triangles of a triangle
tree for $(H,D_{\mid h})$ using the algorithm in \secref{convex}.
This yields a triangulation $\Delta(H)$ of $\R^2$ that conforms to
$H$.  From this \kangulation, we keep only the edges that intersect
$h$ and trim them so that they form a subdivision $\Delta(h)$ of the
interior of $h$. Note that this trimming results in polygons that have
at most 7 sides (the original \kangles\ have at most 4 sides and
trimming adds at most 3 more sides).  Furthermore, the triangles of
$\Delta(h)$ have a tree imposed on them by the triangle tree defined
in \secref{convex}.  The \kangles\ in $\Delta(h)$ are the descendents,
in $T$, of $h$ and are organized, according to the order imposed on
them by the triangle tree.

In this way, we obtain a tree $T_0$ whose leaves correspond to regions
$r_1,\ldots,r_k$ that share an edge $e_1,\ldots,e_k$ with the
components $P_1,\ldots,P_k$ of $P\setminus f$.  We will recursively
operate on each component $P_i$ using edge $e_i$ as the root edge to
obtain a subdivision of $P_i$ and a subtree $T_i$ that is rooted at
the leaf of $T_0$ whose region contains the edge $e_i$.  One small,
but important, detail is a special case that occurs when one of the
sub problems $P_i$ contains more than $n/2$ vertices of $P$.  In this
case, instead of recursing directly on $P_i$ we first subdivide $P_i$
using a funnel into pieces that each contain at most $n/2$ vertices of
$P$.  This completes the definition of the kangle tree $T$ and the
subdivision $\Delta=\Delta(P,D)$. 

We first consider the time required to construct a \kangle\ tree for
$(P,D)$.  

\begin{lem}\lemlabel{construction}
Let $P$ be a simple polygon with $n$ vertices and let $D$ be a
probability measure over $\R^2$.  The \kangulation\
$\Delta=\Delta(P,D)$ has size $O(n)$ and can be computed in $O(n\log n)$
time.
\end{lem}

\begin{proof}
The construction algorithm described above is a recursive algorithm in
which we find the root funnel, triangulate it, and then recurse on the
subpolygons incident to the root funnel.  Finding the root funnel can
be done in $O(n)$ time by computing the two shortest path trees
joining the endpoints of the root edge to all other vertices of $P$.
(See Bose \etal\ \cite{geoham} for a similar computation.)  However,
one of the subpolygons $P_i$ incident on the root funnel might have
many (more than $n/2$) vertices.  In this case, we compute a secondary
funnel in $P_i$ that splits $P_i$ into subpolygons each of which have
at most $n/2$ vertices.  In this way, we obtain an algorithm with a
recursion tree of depth $O(\log n)$ and at which the total work done
at each level is $O(n)$, for a total construction time of $O(n\log
n)$.

To see that the number of \kangles\ in $\Delta$ is $O(n)$, observe
that the \kangle\ tree has $O(n)$ nodes and each node contributes
$O(1)$ \kangles\ to $\Delta$.
\end{proof}

Each node $v$ of a \kangle\ tree has a region $\Delta(v)$ associated
with it.  
 For each node $v$ with children $v_1,\ldots,v_k$, we define
$\Xi(v)$ as 
\[
    \Xi(v) = \Delta_v\setminus(\Delta(v_1),\ldots,\Delta(v_k)) \enspace .
\]
and, as before, we define $\Pr(v)=\Pr(\Xi(v))$.  Note that $\Xi(v)$ is
a (possibly not connected) polygonal region with $O(1)$ edges.
An important property of the \kangle\ tree that is easily verified is
that a node $v$ at
distance $i$ from the root of has $\Pr(\Delta(v)) \le 1/2^{\floor{i/4}}$.

\begin{lem}
Let $v_1,\ldots,v_k$ be a set of nodes in a kangle tree $T$ such
$v_i$ is not an ancestor of $v_j$ for any $1\le i,j\le k$.  Then, for
any conforming triangle $t$ of $P$, there exists at most three indices
$i$ such that $t$ intersects $\Delta(v_i)$.
\end{lem}

\begin{proof}
Easy enough, but first prove \lemref{gen-indep}.
\end{proof}

Next, we partition the polygons of $\Delta$ into groups
$G_1,G_2,\ldots$ where
\[
	G_i = \{t\in \Delta : 1/2^{i} \le \Pr(t) < 1/2^{i-1} \} \enspace .
\]
Note that each polygon $v$ in $\Delta$ corresponds to a unique node
$v(t)$ in $T$.
In what follows, we fix some a real number $0< \alpha < 1$ that will be
specified later.   The following lemma gives the structure of the sets
that will be used in the application of \lemref{pieces}.

\begin{lem}\lemlabel{partition-w}
Each group $G_i$ can be partitioned into $r_i$ subgroups
$G_{i,1},\ldots,G_{i,r_i}$ such that
\begin{enumerate}
\item There is an integer $t_i$ such that $r_i-t_i\le 2^{\alpha i}$.

\item $|G_{i,j}| \ge 2^{\alpha i} / 4i$, for all $1\le j \le t_i$, and

\item for every $1\le j< r_i$ and every $t,s\in G_{i,j}$, $t\neq s$, 
node $v(t)$ is not an ancestor of node $v(s)$ in $T$. 
\end{enumerate}
\end{lem}

\begin{proof}
The proof is identical to the proof of \lemref{partition}, which
relies only on the fact that, for an element $t\in G_i$, there are at
most $4i$ elements of $s\in G_i$ such that $v(s)$ is an ancestor of
$v(t)$.
\end{proof}

\begin{lem}\lemlabel{big-bugger-w}
Let $P$ be a simple polygon with $n$ vertices and let $D$ be a
probability measure over $\R^2$.  Let $\Delta=\Delta(P,D)$ denote the
set of triangles in a kangle tree for $(P,D)$.  Then, for any
triangulation $\Delta^*$ conforming to $P$,
\[
    H(\Delta) \le H(\Delta^*) + O(H(\Delta)^{2/3}+1) \enspace .
\]
Furthermore, the triangulation $\Delta$ contains $O(n)$ vertices and
can be computed in $O(n\log n)$ time.
\end{lem}

\begin{proof}
The bound on the entropy is proven by applying \lemref{pieces} to the
partition of $\Delta$ given by \lemref{partition-w}. The computations
are identical to those in the proof of \lemref{big-bugger}.
\end{proof}



\subsection{Low Entropy Triangulations of Simple Subdivisions}

Given a simple, connected, planar subdivision $G$, we can subdivide
$G$ into polygons of constant size as follows.  We first compute the
convex hull $H$ of $G$.  Consider the planar subdivision $G'$ whose
edges consist of all the edges of $G$ and $H$.  Each of the bounded
faces $F_i$ of $G'$ is a simple polygon that can be \kangulated\ using the
algorithm given in \secref{simple-triangulation} on the distribution
$D_{\mid F_i}$.  The outer face $\overline{F}$ of $G'$ is the
complement of a convex polygon that can subdivided using the algorithm
of \secref{convex} using the distribution $D_{\mid \overline{F}}$.

\begin{lem}\lemlabel{lower-bound-w}
Let $\Delta=\Delta(G,D)$ denote the triangulation of $G$ obtained by
the above algorithm. 
Then, for any triangulation $\Delta^*$ conforming to $G$,
\[
    H(\Delta) \le H(\Delta^*) + O(H(\Delta)^{2/3}+1) \enspace .
\]
\end{lem}

\begin{proof}
The proof of this lemma is identical to the proof of
\lemref{lower-bound}, with the exception that the addition of the
convex hull edges in $H$ increases $H(\Delta)$ by at most 1 (since
each triangle in $\Delta^*$ intersects at most one edge of $H$).
\end{proof}

\subsection{Point Location in Simple Subdivisions}

Finally, the data structure for point location in planar subdivisions
comes as no surprise.  Given $G$ and $D$, construct the kangulation
$\Delta=\Delta(G,D)$ described in the previous section.  The data
structure of \thmref{ammw07} is then used to preprocess $\Delta$ and
$D$ for point location.  The performance of this data structure is
summarized by our main theorem.

\begin{thm}
Given a simple, connected, planar subdivision $G$ and a probability
measure $D$ over $\R^2$, there exists a data structure $T=T(G,D)$ for
answering point location queries in $G$ that can be
constructed in $O(n\log n)$ time and with expected query time
\[
   \mu_D(T) \le \mu_D(T^*) + O(\mu_D(T)^{2/3} + 1)
\]
where $T^*$ is any linear decision tree for point location in $G$.
\end{thm} 

\begin{proof}
The bound on the query time follows from \lemref{triangulate} and
\lemref{lower-bound-w}. The running time follows from
\lemref{construction} and by using, for
example, Melkman's
Algorithm to compute the convex hull of $G$ in $O(n)$ time.
\end{proof}

\bibliographystyle{plain}
\bibliography{entropy}



\end{document}
