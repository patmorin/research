\documentclass[charterfonts,lotsofwhite]{patmorin}
\usepackage[noend]{algorithmic}
\usepackage{amsmath,amsthm,amsopn}
\usepackage{graphicx}

\input{pat}
%\newcommand{\td}{\mathrm{depth}}
%\newcommand{\lp}{\mathrm{LP}}
\DeclareMathOperator{\td}{depth}
\DeclareMathOperator{\lp}{LP}

\title{\MakeUppercase{Output-Sensitive Algorithms for Tukey Depth and
        Related Problems}}
\author{David Bremner \and
        Dan Chen \and
	John Iacono \and
	Stefan Langerman \and
	Pat Morin}
\date{}
\begin{document}
\maketitle

\begin{abstract}
The \emph{Tukey depth} (Tukey 1975) of a point $p$ with respect to a
finite set $S$ of points is the minimum number of elements of $S$
contained in any closed halfspace that contains $p$.  Algorithms for
computing the Tukey depth of a point in various dimensions are
considered. The running times of these algorithms depend on the value
of the output, making them suited to situations, such as outlier
removal, where the value of the output is typically small.
\end{abstract}

\section{Introduction}

Let $S$ be a set of $n$ points in $\R^d$.
The \emph{Tukey depth}, or \emph{halfspace depth} of a point $p\in\R^d$ with
respect to $S$ can be defined in several equivalent ways \cite{t75}:
\begin{eqnarray}
\td(p,S) & = & \min\{ |h\cap S| :
	             \mbox{$h$ is a closed halfspace containing $p$} \} 
                       \eqlabel{tuk-orig} \\ 
            & = & \min\{ |h\cap S| :
                      \mbox{$h$ is a closed halfspace 
                            with $p$ on its boundary} \} 
                        \eqlabel{tuk-boundary} \\ 
            & = & \min\{ |S'| :
                      \mbox{$p$ is outside the convex hull of 
                           $S\setminus S'$} \}
                      \eqlabel{tuk-hull}
\end{eqnarray}

Algorithms for computing the point $p\in\R^d$ of maximum Tukey depth have a
rich history \cite{m91,ls03,c04} that has recently culminated in
Chan's $O(n\log n + n^{d-1})$ expected time algorithm.  A point of
maximum Tukey depth serves as a $d$-dimensional generalization of the
(1-dimensional) median and performs well as a robust estimate of the
``center'' of $S$ \cite{rr98,rr96,s90}.

In this paper we consider the simpler problem of computing the Tukey
depth of a given point $p$ with respect to a set $S$.  Our algorithms
have running times that are dependent on the value, $k$, of the
output.  These algorithms are thus particularly well-suited to
problems such as outlier-removal where the goal is to identify points
of small depth since they run quickly when the depth of $p$ is small.  
Specifically, we present the following results:

\begin{enumerate}
\item A simple $O(n + k\log k)$ time algorithm for points in $\R^2$
(\secref{2d}).  The most complicated data structure used in this
algorithm is a binary heap.

\item An $O(n + (n-k)\log(n-k))$ time algorithm to find the largest
clique in an interval graph, where $k$ is the size of the clique found
(\secref{max-clique}).  This problem is related to the Tukey
depth problem in $\R^2$.

\item An $O(n\log n + k^2\log n)$ time algorithm for points in $\R^3$
and an $O(n + k^{11/4}n^{1/4}\log^{O(1)}n)$ time algorithm for points
in $\R^4$ (\secref{3-4-d}).  These algorithms rely on results of Chan
on linear programming with violated constraints \cite{c05} which in
turn rely on sophisticated range searching data structures
\cite{m92,r99} and/or dynamic convex hull data structures \cite{bj02}.

\item A simple $O(d^k \lp(n,d-1))$ time algorithm for points in
$\R^d$, where $\lp(n,d)$ denotes the time required to determine the
feasibility of a linear program having $n$ constraints and $d$
variables (\secref{d-d}).  Not surprisingly, this algorithm is also
based on linear programming with violated constraints and is obtained
by presenting a fixed-parameter tractable algorithm for a
parameterization of the NP-hard \textsc{MaximumFeasibleSubsystem}
problem.
\end{enumerate}

For the remainder of this paper we use the following notations: For
points $p,q\in\mathbb{R}^d$, $p_i$ denotes the $i$th coordinate of
$p$, $\|p\|=(\sum_{i=1}^d p_i^2)^{1/2}$, and $p\cdot
q=\sum_{i=1}^d p_iq_i$.  The unit sphere in $\R^{d+1}$ is denoted by
$\Sp^d =\{ p\in\R^{d+1} : \|p\|=1\}$. The top side of this sphere is
denoted by $\Sp^d_+ =\{ p\in\Sp^{d} : p_{d+1} > 0 \}$, the bottom
side is denoted by $\Sp^d_- =\{ p\in\Sp^{d} : p_{d+1} < 0 \}$ and the
equator is denoted by $\Sp^d_0 =\{ p\in\Sp^{d} : p_{d+1} = 0 \}$ .

\section{An Algorithm for Points in $\R^2$}
\seclabel{2d}

In this section we give a simple $O(n + k\log k)$ time algorithm to
compute the Tukey depth of a point $p\in\R^2$ with respect to a set
$S$ of $n$ points in $\R^2$.  We first note that an $O(n\log n)$ time
\emph{sort-and-scan} algorithm is easily obtained by sorting the
points of $S$ radially about $p$ and then scanning the resulting
sorted list using two pointers \cite{m91}.  The main idea behind our
algorithm to is to reduce the problem to a \emph{kernel} of size
$O(k)$ on which we can apply this sort-and-scan algorithm.

The algorithm begins by partitioning $\R^2$ into 4 quadrants around
$p$ that, in counterclockwise order, we denote by $Q_0,\ldots,Q_3$.
The algorithm then simultaneously begins computing the 4 quantities
$\td_0(p,S),\ldots,\td_3(p,S)$ where 
\begin{equation}
     \td_i(p,S) = \min\{|h\cap S| : \mbox{$h$ is a closed halfspace containing $Q_i$} \} \enspace . \eqlabel{four-min}
\end{equation}
Clearly, $\td(p,S) = \min\{\td_i(p,S): 0\le i \le 3 \}$ since any
closed halfspace containing $p$ contains at least one of the four quadrants.
In the remainder of this section we will describe how to compute
$k_i=\td_i(p,S)$ in $O(n + k_i\log k_i)$ time.  Since the
computation can stop once $\td_i(p,S)$ has been computed for
the index $i$ that minimizes \eqref{four-min}, running the computation
of $k_0,\ldots,k_3$ in parallel yields an $O(n +
k\log k)$ time algorithm, where $k=\td(p,S)$.

\begin{figure}
\begin{center} \includegraphics{quadrants} \end{center}
\caption{Computing the quantity $\td_1(p,S)$.}
\figlabel{quadrants}
\end{figure}


Let $S_i=S\cap Q_i$. To compute $\td_i(p,S)$ we create two binary
heaps $H_{i-1}$ and $H_{i+1}$ that store the elements of $S_{i-1}$,
respectively $S_{i+1}$, in clockwise, respectively, counterclockwise,
order around $p$.\footnote{Here and in the remainder of this section
$S_i$ is treated implictly as $S_{i\bmod 4}$.} 
Creating these two heaps takes $O(n)$ time using the
standard bottom-up algorithm to construct a binary heap
\cite[Chapter~6]{clrs01}.
Next we extract elements one at a time from each of $H_{i-1}$ and
$H_{i+1}$ until either (a)~one of the heaps is empty or (b)~we extract
two elements $q$ from $H_{i-1}$ and $r$ from $H_{i+1}$ such that the
angle $\angle qpr > \pi$.  Suppose we have extracted $\ell$ elements
each from $H_{i-1}$ and $H_{i+1}$ when this occurs.  Then it is easy
to verify that 
\[  
  |S_i| + \ell - 1 \le \td_i(p,S) \le |S_i| + 2\ell-1 \enspace .
\]

Next, we continue to extract as many elements as possible from each of
$H_{i-1}$ and $H_{i+1}$ up to a maximum of an additional $\ell-1$
elements each. The total time required to extract these at most
$4\ell-2$ elements from the two heaps is $O(\ell\log n)$.  By sorting and
scanning all the elements extracted from the heaps plus the elements of
$S_i$ we can then compute $\td_i(p,S)$ in an additional
\[
     O((|S_i|+\ell)\log n) = O(k_i\log n)
\] 
time.  This yields an a total running time of 
\[  
   O(n + k_i\log n) = O(n + k_i\log k_i) \enspace ,
\]
as required.  This completes the proof of:

\begin{thm}\thmlabel{2-d}
The Tukey depth of a point $p$ with respect to a set $S$ of $n$ points
in $\R^2$ can be computed in $O(n + k\log k)$ time, where
$k$ is the value of the output.
\end{thm}

\section{An Algorithm for \textsc{Max-Clique} in Interval Graphs}
\seclabel{max-clique}

The problem of computing Tukey depth in $\R^2$ can be viewed as a
problem on a set of circular arcs.  By \eqref{tuk-boundary}, computing
the Tukey depth of $p$ is equivalent to finding a unit normal vector
$v$ such that the halfspace with $p$ on its boundary and having inner
normal $v$ contains as few points of $S$ as possible.  Note that the
set of unit normals in $\R^2$ is homeomorphic to the unit circle
$\Sp^1$ and that each point $q\in S$ defines an open circular arc of $\Sp^1$
such that all choices of $v$ in this circular arc yield a halfspace
that does not contain $q$.  Thus, the Tukey depth problem reduces to
the problem of finding a vector $v$ that is contained in the largest
number of circular arcs. The partitioning into 4 quadrants used in
the algorithm of \thmref{2-d} works because all the circular arcs are
actually half circles.

An obvious generalization of the Tukey depth problem is that of, given
a set of $n$ circular arcs of $\Sp^1$, finding a point $p\in\Sp^1$
contained in the largest number of arcs.  This problem is easily
solved in $O(n\log n)$ time by the sort-and-scan algorithm.
Unfortunately, it is not possible to obtain an algorithm whose running
time depends on the number $k$ of arcs containing $p$ or even on the
number $(n-k)$ of arcs not containing $p$.  This is because the
decision problem of testing whether a set of $n$ arcs covers $\Sp^1$
has an $\Omega(n\log n)$ lower-bound \cite{b-o83}.  This problem is
equivalent, by taking the complement of each arc, to the problem of
finding the point contained in the maximum number of arcs.  In
particular, the original set of arcs do not cover $\Sp^1$ if and only
if there is a point $p$ contained in every complementary arc.

Since we can not hope to solve the problem for circular arcs of
$\Sp^1$, we settle for the next best thing.  Let $I$ be a set of real
intervals.  Here we describe an $O(n+(n-k)\log (n-k))$ time algorithm
to find a point $p\in\R$ that is contained in the largest number of
intervals in $I$.  Here $k$ is the number of intervals in $I$ that
contain $p$.  Let $p_1,\ldots,p_{2n}$ denote the $2n$ endpoints of the
intervals in $I$, in increasing order.  For convenience we use the
convention that $p_i = -\infty$ for $i\le 0$ and $p_i=+\infty$ for $i
> 2n$.  Together, the following two observations imply that all the
points contained in many intervals are clustered together.

\begin{lem}\lemlabel{close}
Let $q\in[p_i,p_{i+1}]$ be a point contained in $k$ intervals of $I$.
Then, for any $0\le r\le n$, every point $q'\in[p_{i-r},p_{i+r+1}]$ is
contained in at least $k-r$ intervals of $I$.
\end{lem}

\begin{proof}
Without loss of generality, assume that $q'\in[q,p_{i+r+1}]$.  There are
at most $r$ endpoints of intervals in $I$ contained in the interval
$[q,q']$.  Therefore there are at most $r$ intervals that contain $q$
but not $q'$.
\end{proof}

\begin{lem}\lemlabel{far}
Let $q\in[p_i,p_{i+1}]$ be a point contained in $k$ intervals of $I$.
Then, for any $n-k \le r\le n$, every point $q'\notin[p_{i-r},p_{i+r+1}]$ 
is contained in at most $2n-k-r$ arcs of $C$.
\end{lem}

\begin{proof}
Without loss of generality, assume that $q'> p_{i+r+1}$.  Then, as we
walk from $q$ to $q'$ we encounter at least $r$ endpoints of intervals
in $I$.  At most $n-k$ of these endpoints are left endpoints of
intervals and at least $r-(n-k)$ of these are right endpoints.  Thus,
the number of intervals that contain $q'$ is at most
\[
      k + (n-k) - (r-(n-k)) = 2n - k - r \enspace ,
\]
as required.
\end{proof}

At a high level our algorithm is fairly simple.  Suppose we are given
a value $k$ and only wish to find a value $p\in\R$
contained in at least $k$ intervals of $I$.
We begin by taking a regular sample $s_1,\ldots,s_{2t}$ of
$p_1,\ldots,p_{2n}$ so that any interval $[s_i,s_{i+1}]$ between two
consecutive sample points contains at most $n/t$ points of
$p_1,\ldots,p_{2n}$.  We then compute, for each sample point $s_i$ the
number of intervals in $I$ that contain $s_i$.  By \lemref{close}, if
there exists any point $p\in \R$ contained in $k$ intervals of $I$
then the two sample points $s_j$ and $s_{j+1}$ on either side of $p$
are \emph{high depth samples} that are each contained in at least
$k-n/t$ intervals of $I$.  Furthermore, by \lemref{far}, the only high
depth samples are contained in the interval $[p_{i-r},p_{i+r}]$ for
$r=2(n-k)+n/t$.  

If we choose $t=\sqrt{n}$ then $r=O(n-k+\sqrt{n})$.  Thus, by
computing an interval $[p_a,p_b]$ that contains all high depth samples
we can find the point $p$ contained in the largest number of
intervals of $C$ by applying the standard sort-and-scan algorithm
on the $O(n-k+\sqrt{n})$ endpoints of the intervals of $C$ that fall
in the interval $[p_a,p_b]$.  The running time of the
sort-and-scan algorithm is $O(m\log m)$ where $m$ is the number of
points to be scanned.  In this case $m=O(n-k+\sqrt{n})$ for a running
time of 
\[
   O((n-k+\sqrt{n})\log(n-k+\sqrt n)) = O(n + (n-k)\log (n-k)) \enspace ,
\]
as required.

In implementating the above ideas, several issues arise:

\begin{enumerate}

\item The value of $k$ is not known in advance.  However, we do not
need the exactly value of $k$ and the value of $k$ can be estimated to
within an additive error of $\sqrt{n}$ by computing, for each sample
point $s_i$, the number of intervals of $I$ that contain $s_i$ (see
Issue~3, below) and using the maximum of these values as an estimate
for $k$.

\item We can not obtain a perfectly regular sample
$s_1,\ldots,s_{2\sqrt{n}}$ of $p_1,\ldots,p_{2n}$ in $O(n)$ time.
However, we do not require a perfectly regular sample.  By taking a
random sample of size $c\sqrt{n}\log n$ for an appropriate constant
$c$ we obtain a set of samples $s_1,\ldots,s_{c\sqrt{n}\log n}$ such
that, with high probability, no interval $[s_{i},s_{i+1}]$ contains
more than $\sqrt{n}$ endpoints of intervals of $I$ \cite{m98}.

\item We can not compute, in $O(n)$ time, for each sample point $s_i$,
the number of intervals of $I$ that contain $s_i$.  However, random
sampling helps again here.  Let $d(s_i)$ denote the number of elements
of $I$ that contain $s_i$. By taking a random sample $I'\subseteq I$,
$|I'|=\sqrt{n}$ we can determine for each $s_i$ a
number $d_i$ such that, with high probability,
\[
        d(s_i) - O(n^{4/5})\le d_i \le
           d(s_i) + O(n^{4/5}) \enspace .
\]
By storing the $\sqrt{n}$ elements of $I'$ in an interval tree \cite{ps85} and
then querying this interval tree with the $c\sqrt{n}\log n$ sample
elements the numbers $d_1,\ldots,d_{c\sqrt{n}\log n}$ can be computed
in $O(\sqrt{n}\log^2 n)$
time.  
\end{enumerate}

None of the above issues have any significant effect on the running
time of the overall algorithm, which is still dominated by the final
sort-and-scan step on a problem of size $O(n-k+\sqrt{n})$.  The
correctness of the resulting output depends on the success of the
random sampling steps described in points 2 and 3, above.  However,
\lemref{far} implies that this final sort-and-scan step allows us
to check the correctness of the output and restart the algorithm from
scratch if necessary.  This yields:

\begin{thm}
There exists a randomized algorithm that, given a set $I$ of $n$ real
intervals, finds a value $p\in\R$ contained in the largest number of
intervals of $I$ and that runs in $O(n+(n-k)\log (n-k))$ expected time.
\end{thm}


\section{Algorithms for Points in $\R^3$ and $\R^4$}
\seclabel{3-4-d}

The previous section showed how the problem of computing the Tukey
depth of a point in $\R^2$ is equivalent to the problem of finding a
point contained in the largest number of halfcircles on the unit
circle $\Sp^1$.  A similar statement is true in $\R^d$:  Each point
$q\in S$ defines an open halfsphere $q^*=\{ v\in\Sp^{d-1} : v\cdot q <
0\}$.  That is, all vectors in $q^*$ are the inner normals of
halfspaces that contain $p$ but do not contain $q$.  Thus, the problem
of determining the Tukey depth of $p$ reduces to the problem of
finding the point contained in the largest number of halfspheres in
$S^*=\{q^* : q\in S\}$.

We observe that this problem can be solved by solving two
\textsc{MaximumFeasibleSubsystem} problems in $\R^{d-1}$.  Each open
halfsphere $q^*\in S^*$ is the intersection of an open halfspace
$q^\#$ with $\Sp^{d-1}$.  Consider the intersection of $q^\#$ with the
hyperplane $H_+=\{(x_1,\ldots,x_d):x_d=1\}$.  By central projection,
there is a bijection between points in $\Sp^{d-1}_+$ and $H_+$ and
this bijection has the property that $r\in\Sp^{d-1}_+$ is in $q^*$ if
and only if the projection of $r$ is in $q^\#\cap H_+$.  Thus, finding
the point in $\Sp^{d-1}_+$ contained in the largest number of
halfspheres is equivalent to finding a point in $H_+$ contained in the
largest number of halfspaces.  A similar statement holds regarding
$\Sp^{d-1}_-$ using the hyperplane $H_-=\{(x_1,\ldots,x_d):x_d=-1\}$.
Finally, we observe that we do not need to consider solutions on the
equator $\Sp^{d-1}_0$ because our input consists of \emph{open}
halfspheres.\footnote{A solution to the Tukey depth problem is
obtained when we find a set $n-k$ open halfspheres that have a
non-empty common intersection.  The intersection of a set of open
halfspheres is either an empty set or a set with positive measure.
Since $\Sp^{d-1}_0$ has measure 0 any solution to the Tukey depth
problem contains points not in $\Sp^{d-1}_0$.}

The above discussion shows that computing the Tukey depth of a point
in $\R^d$ reduces to two instances of the problem
\textsc{MaximumFeasibleSubsystem} problem in $\R^{d-1}$: Given a set
$K$ of $n$ halfspaces in $\mathbb{R}^{d-1}$, find the subset $K'$ of
$K$ of minimum cardinality such that $\cap (K\setminus K')$ is
non-empty.  The current best results for
\textsc{MaximumFeasibleSubsystem} in small dimensions are due to Chan
\cite{c05}.  Using two instances of his algorithm for
\textsc{MaximumFeasibleSubsystem} in $\R^2$, respectively, $\R^3$, and
running them in parallel gives:

\begin{thm}
The Tukey depth of a point $p$ with respect to a set $S$ of $n$ points
in $\R^3$ can be computed in $O(n\log n + k^2\log n)$ time, where
$k$ is the value of the output.
\end{thm}


\begin{thm}
The Tukey depth of a point $p$ with respect to a set $S$ of $n$ points
in $\R^4$ can be computed in $O(n\log n + k^{11/4}n^{1/4}\log^{O(1)}
n)$ time, where $k$ is the value of the output.
\end{thm}

\section{An Algorithm for Points in $\R^d$}
\seclabel{d-d}

Finally, we consider the general case of point sets in $\R^d$.  In the
previous section we showed that computing the Tukey depth of a point
$p$ with respect to a set $S$ of $n$ points in $\R^d$ can be reduced
to two instances of \textsc{MaximumFeasibleSubsystem} in $\R^{d-1}$.
In this section we give a fixed-parameter tractable \cite{df98}
algorithm for \textsc{MaximumFeasibleSubsystem}.  

The algorithm uses linear programming as a subroutine in the following
way:  Given a collection $K$ of halfspaces in $\R^{d-1}$, an algorithm
for linear programming can be used to either
\begin{enumerate}
\item Determine a point $p\in\cap K$ if such a point exists or,
\item report a subset $B\subseteq K$, $|B|\le d$, such that 
	$\cap B=\emptyset$.
\end{enumerate}
The set $B$ reported in the latter case is called a \emph{basic
infeasible subsystem}.  Standard combinatorial algorithms for linear
programming, including algorithms for linear programming in small
dimensions \cite{c95,d84,m83,m84,s91,sw92} as well as the simplex
method (c.f., \cite{chvatal80}), can easily be made to report a basic
infeasible subsystem.  A method of finding basic infeasible subsystems
from interior point linear-programming methods is described in
\appref{bis}.

Let $\textsc{BIS}(K)$ denote a routine that outputs a basic infeasible
subsystem of $K$ if $K$ is infeasible, and that outputs the empty set
otherwise.  The following algorithm solves the
\textsc{MaximumFeasibleSubsystem} decision problem:

\noindent$\textsc{MFS}(K,k)$
\begin{algorithmic}[1]
\STATE{\COMMENT{$\star$ determine if there exists $K'\subseteq K$, $|K'|\le
k$, such that $\cap(K\setminus K')\neq\emptyset$ $\star$} }
\STATE{$B\gets\textsc{BIS}(K)$}
\IF{$B=\emptyset$}
   \STATE{\textbf{return} true}
\ENDIF
\IF{$k=0$}
   \STATE{\textbf{return} false}
\ENDIF
\FOR{each $h\in B$}
   \IF{$\textsc{MFS}(K\setminus\{h\},k-1)=\mathrm{true}$}
      \STATE{\textbf{return} true}
   \ENDIF
\ENDFOR
\STATE{\textbf{return} false}
\end{algorithmic}

The correctness of the above algorithm is easily established by
induction on the value of $k$.  The running time of the algorithm is
given by the recurrence
\[
    T(n,k) \le \lp(n,d-1)+ dT(n-1,k-1) \enspace ,
\]
where $\lp(n,d)$ denotes the running time of an algorithm for finding
a basic infeasible system in a linear program with $n$ constraints and
$d$ variables.  This recurrence readily resolves to
$O(d^k\lp(n,d-1))$.  
Using this as a subroutine for Tukey depth computation we obtain our
final result:

\begin{thm}\thmlabel{d-d}
The Tukey depth of a point $p$ with respect to a set $S$ of $n$ points
in $\R^d$ can be computed in $O(d^k\lp(n,d-1))$ time, where $k$ is
the value of the output and $\lp(n,d)$ is the time to solve a linear
program with $n$ constraints and $d$ variables.
\end{thm}

\bibliographystyle{plain}
\bibliography{tukey}

\appendix
\section{Computing a Basic Infeasible Subsystem}
\applabel{bis}


For any matrix $M$, let $M_J$ denote the set of rows indexed by $J$.
Given a system of linear inequalities $Mx \geq b$, $M \in \R^{m \times
d}$, a \emph{basic infeasible subsystem} is a subset of $\{ 1 \dots
m\}$ such that the system $M_I x \geq b_I$ is infeasible, and $|I|
\leq d+1$.  We consider the standard first stage simplex problem (see
e.g.~\cite{chvatal80}, p.~39). Let  $e$ denote the $m$-vector of all
ones, $c$ the length $d+1$ binary vector with exactly one one in the
last position and let $A=[M e]$. We can write the first stage LP for
our system as

\begin{align*}
    \min c^Tx &= x_{d+1}\\\tag{P}\eqlabel{P}
    \text{subject to}\\
    Ax &\geq b
\end{align*}
  In the case of an
infeasible system, the optimal value of this LP will be strictly
positive. 
The dual LP of \eqref{P} is
\begin{align*}
  \max b^T y\\\tag{D}
  \eqlabel{D}
  \text{subject to}\\
  yA &= c\\
  y & \geq 0
\end{align*}
In what follows, we generally follow the notation used by Megiddo 
\cite{megiddo91},
except that we interchange the definitions of the primal and dual LPs.
Define a \emph{basic partition} (or just \emph{basis}) $(\beta,\eta)$
as a partition of the row indices of $A$ such that $A_\beta$ is
nonsingular.  For each basic partition, we define a \emph{primal basic
  solution}
\begin{equation*}
  x^*=A_\beta^{-1} b_\beta
\end{equation*}
and a \emph{dual basic solution}
\begin{equation*}
  y^*= c A^{-1}_\beta
\end{equation*}
We say that a basis is \emph{primal feasible} (resp.\ \emph{dual
  feasible}) if $x^*$ is feasible for \eqref{P} (respectively $y^*$
is feasible for \eqref{D}).  It is a standard result of linear
programming duality that a basis which is both primal and dual feasible
defines a pair $(x^*,y^*)$ of optimal solutions to the primal and dual
LP's; such a partition is called an \emph{optimal basis partition}.

In general LP algorithms (either directly in the case 
of Simplex type algorithms, or via postprocessing using
e.g.~\cite{megiddo91,vavasis96}) provide an optimal basis partition
$(\beta,\eta)$.  Consider the relaxed LP
\begin{align*}
  \min c^T x \\\tag{R}
  \eqlabel{R}
  \text{subject to}\\
  A_\beta x \leq b_\beta
\end{align*}
It is easy to verify that an optimal basis partition for \eqref{P}
is also primal and dual feasible for \eqref{R}.  This implies that
the system $M_\beta \geq b_\beta$ is infeasible, and provides a a
basic infeasible system.

\end{document}

