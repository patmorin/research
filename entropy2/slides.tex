\documentclass{beamer}

\mode<presentation>{
\definecolor{cured}{rgb}{.8,0,.2}
\usecolortheme[named=cured]{structure}
%\usefonttheme{structurebold}
\usetheme{split}
}

\usepackage{amsopn}

\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\average}{average}
\DeclareMathOperator{\support}{support}
\DeclareMathOperator{\boundary}{\partial}

\input{pat-slides}

\newcommand{\pin}{p_{\mathsf{in}}}
\newcommand{\pout}{p_{\mathsf{out}}}
\DeclareMathOperator{\cost}{cost}
\DeclareMathOperator{\depth}{depth}

\title{Minimum-Entropy Conforming Triangulations}
\author{S\'ebastien~Collette 
	\and Vida~Dujmovi\'c 
	\and John~Iacono
	\and Stefan~Langerman
	\and Pat~Morin}
\date{November 13, 2008 \\ NICTA Algorithms Seminar}

\begin{document}

\frame{\titlepage}

\section[Outline]{}
\frame{\tableofcontents}

\section{Introduction}
\subsection{Planar Point Location}
\frame
{
  \frametitle{Planar Point Location}
  \begin{itemize}
  \item Preprocess a planar subdivision $G$ so that, for any query
point $p\in\R^2$, the we can quickly determine the face of $G$
that contains $p$
  \begin{center}
  \only<1>{\includegraphics{pl-1}}%
  \only<2>{\includegraphics{pl-2}}%
  \only<3>{\includegraphics{pl-3}}%
  \end{center}
  \end{itemize}
}

\subsection{History}

\frame
{
  \frametitle{The 1980s}
  \begin{itemize}
  \item<1-> After a few initial attempts, optimal $O(n)$ space, $O(\log
n)$ query time solutions were obtained by
  \begin{itemize}
   \item<2-> Kirkpatrick (1983) --- Independent set removal
   \item<3-> Edelsbrunner, Guibas, and Stolfi (1986) --- Fractional cascading
   \item<4-> Sarnak and Tarjan (1986) --- Persistence
   \item<5-> Mulmuley (1990) --- Randomized incremental construction
  \end{itemize}
  \end{itemize}
}

\frame
{
  \frametitle{The 1990s}
  \begin{itemize}
  \item<1-> Then we started carefully studying the constants:
   \item<2-> Goodrich, Orletsky, and Ramaiyer (1997)
   \begin{itemize}
      \item<3-> $O(n)$ space, $2\log n + o(\log n)$ standard comparisons
      \item<4-> Conjectured that $2\log n$ is optimal for linear space structures
   \end{itemize}
   \item<5-> Adamy and Seidel (1998)
   \begin{itemize}
      \item<6-> $O(n)$ space, $\log n + \sqrt{2}\log\log n +
o(\log\log n)$ standard comparisons
      \item<7-> Optimal up to the second term
   \end{itemize}
  \end{itemize}
}

\frame
{
  \frametitle{The New Millenium: A New Era of Sensitivity}
  \begin{itemize}
  \item<1-> Adamy and Seidel's result closes the problem of the
	worst-case complexity of planar point location
  \item<2-> What about the average case?
  \item<3-> Assume the point $p$ is selected according to a
	probability measure $D$ over $\R^2$
  \item<4-> Given $(G,D)$ can we optimize the expected cost of
	locating $p$?
  \end{itemize}
}

\frame
{
  \frametitle{Defining Optimality}
  \begin{itemize}
   \item<1-> We need a way of knowing when a point location structure
	is optimal
   \item<2-> $D$ induces a distribution over the faces of $G$
   \item<3-> Let $p_i=\Pr\{\mbox{$p$ is contained in face $i$}\}$
   \item<4-> \textbf{Theorem (Shannon 1948):}
     For any data structure $X$ that makes only binary decisions, the
expected number of decisions is at least
   \[
      \mu_D(X) \ge H(p_1,\ldots,p_f) = \sum_{i=1}^f p_i\log(1/p_i)  \enspace .
   \]
  \end{itemize}
}

\frame
{
   \frametitle{Distribution-Sensitive Point Location: Results}
   \begin{itemize}
   \item<1-> Arya, Cheng, Mount, and Ramesh (2000)
    \begin{itemize}
      \item<1->For convex subdivisions: $O(n^2)$ space, $2H+o(H)$ comparisons
      \item<1->For convex subdivisions: $O(n)$ space, $4H+o(H)$ comparisons
      \item<1->Requires that $x$ and $y$ coordinates of $p$ are independent
    \end{itemize}
   \item<2-> Arya, Malamatos, and Mount (2000,2001a,2001b,2007)
    \begin{itemize}
      \item<2->For triangulations: $O(n)$ space, $H+O(H^{1/2}+1)$ comparisons
%    \end{itemize} 
%    \item<3-> Arya, Malamatos, and Mount (2001a)
%    \begin{itemize}
%      \item<3->For triangulations: $O(n\log^* n)$ space, $H+o(H)$ comparisons
%    \end{itemize} 
%    \item<4-> Arya, Malamatos, and Mount (2001b)
%    \begin{itemize}
      \item<3->For triangulations: $O(n)$ space, $O(H)$ comparisons
(randomized --- Mulmuley)
    \end{itemize} 
    \item<4-> Iacono (2001)
    \begin{itemize}
      \item<4->For triangulations: $O(n)$ space, $O(H)$ comparisons
(deterministic --- Kirkpatrick)
    \end{itemize} 
    \item<5-> Collette, Dujmovi\'c, Iacono, Langerman, Morin (2007)
    \begin{itemize}
      \item<5->For convex subdivisions: $O(n)$ space, $\tilde{H} + O(\tilde{H}^{2/3} + 1)$ comparisons
(deterministic --- Kirkpatrick)
    \end{itemize} 
   \end{itemize}



}

\frame
{
   \frametitle{What's Left}
   \begin{itemize}
   \item<1-> It looks like distribution-sensitive point location is 
	well understood, but\ldots\\
   \begin{center}
     \only<2->{\includegraphics[height=1.5in]{australia_regions}}
   \end{center}
   \item<2-> This isn't a triangulation or even a convex subdivision
   \end{itemize}
}

\section{Triangulations, Entropy, and Linear Decision Trees}

\frame
{
   \frametitle{Minimum-Entropy Triangulations}
   \begin{itemize}
   \item<1-> Recall the result of Arya et al 2007:
     \begin{itemize}
      \item For triangulations: $O(n)$ space, $H+O(H^{1/2}+1)$ comparisons
     \end{itemize}
   \item<2-> Idea: Triangulate the subdivision and apply the above result
   \end{itemize}
}

\frame
{
   \frametitle{Difficulties}
   \begin{itemize}
   \item<1-> Not enough choice when triangulating
   \begin{center}
     \only<1>{\includegraphics{unique1}}
     \only<2->{\includegraphics{unique2}}
   \end{center}
   \item<2-> The only triangulation may increase $H$ too much
   \end{itemize}
}

\frame
{
   \frametitle{Linear Decision Trees}
   \begin{itemize}
     \item<1-> We need to fix a model of computation if we want to
       prove any data structure is optimal
     \item<2-> A \emph{linear decision tree} is a binary tree where
       \begin{itemize}
          \item Each internal node is labelled with a linear inequality
          \item Each leaf is labelled with the name of a face of $G$
          \item For every $p\in\R^2$ the search path for $p$ finishes at 
           a leaf labelled $F_i$ if and only if $p\in F_i$
       \end{itemize}
     \item<3-> Every point location structure in the
	introduction can be expressed as a linear decision tree
   \end{itemize}
}




\frame
{
	\frametitle{Linear Decision Tree Example}
        \only<1>{\includegraphics{ld-a}}%
        \only<2>{\includegraphics{ld-b}}%
        \only<3>{\includegraphics{ld-c}}%
        \only<4>{\includegraphics{ld-d}}%
        \only<5>{\includegraphics{ld-e}}%
}

\frame
{
   \frametitle{Linear Decision Trees and Triangulations}
   \begin{itemize}
     \item<1->
     \textbf{Lemma 1:}
       For every linear decision tree $T$ there exists a linear
decision tree $T'$ in which every leaf of $T'$ is a triangle and 
$\mu_D(T') \le \mu_D(T) + O(\log(\mu_D(T))$.
     \item<2-> \textit{Proof:}
      \begin{itemize}
        \item<3-> If $T$ has a leaf $\ell$ with $k$ sides then
          the depth of $\ell$ in $T$ is at least $k$ 
        \item<4-> Triangulate $\ell$ using a balanced triangulation,
increasing its depth only by $O(\log k)$
        \item<5-> Apply Jensen's Inequality
        \item<6-> ???
        \item<7-> Profit! (QED)
     \end{itemize}
   \end{itemize}
}

\frame
{
   \frametitle{Review}

   \begin{itemize}
     \item<1-> If we find a triangulation with entropy $H$ then we get a
point location structure with query time $H+O(H^{1/2}+1)$
     \item<2-> If the minimum entropy of \emph{any} triangulation is $H$
then the expected cost of \emph{any} linear decision tree is at least
$H-O(\log H)$
     \item<3-> Finding a (nearly) optimal point location structure is
(nearly) equivalent to finding a (nearly) minimum-entropy triangulation
   \end{itemize} 
}

\frame
{
    \frametitle{A Simple(ton) Plan}

    \begin{itemize}
      \item<1-> We want to find a triangulation $\Delta$ of $G$ and compare it to
a minimum entropy triangulation $\Delta^*$ of $G$
      \item<2-> But we have no idea what $\Delta^*$ looks like
      \item<3-> \textbf{Lemma 2:} If each triangle in $\Delta^*$ intersects
at most $c$ triangles in $\Delta$ then $H(\Delta) \le H(\Delta^*)+\log c$
      \item<4-> Idea (wrong): Find a triangulation $\Delta$ such that \emph{no}
 triangle intersects more than $c$ triangles in $\Delta$
    \end{itemize}	
}


\frame
{
    \frametitle{A Better Plan}

    \begin{itemize}
      \item<1-> \textbf{Lemma 3:} Let $\Delta_1,\ldots,\Delta_m$ be a
partition of the triangles in $\Delta$ such that \emph{no triangle
contained in a single face of $G$}
intersects more than $c$ triangles of $\Delta_i$ for any $1\le i\le
m$. Then, for any triangulation $\Delta^*$
\[
      H(\Delta) \le H(\Delta^*) + H(\cup\Delta_1,\ldots,\cup\Delta_m) + \log c
\]
\[
      H(\Delta^*) \ge H(\Delta) - H(\cup\Delta_1,\ldots,\cup\Delta_m) - \log c
\]
     \item<2-> \textit{Proof:} 
      \begin{itemize}
       \item<3-> Give away information about which $\Delta_i$ occurs
(gives up $H(\cup\Delta_1,\ldots,\cup\Delta_m)$ bits of entropy)
       \item<4-> Apply Lemma~2 to each $\Delta_i$ (gives up
           $\Pr(\Delta_i)\log c$ bits of entropy)
       \item<5-> QED
      \end{itemize} 
    \end{itemize}
}

\section{A Low-Entropy Triangulation}
\frame
{
   \frametitle{A Low-Entropy Triangulation}

   \begin{itemize}
     \item<1->We triangulate each face of $G$ (a simple polygon)
recursively
     \begin{center}
       \only<1>{\includegraphics[height=1in]{x1}}
       \only<2>{\includegraphics[height=1in]{x2}}
       \only<3>{\includegraphics[height=1in]{x3}}
       \only<4->{\includegraphics[height=1in]{x4}}
     \end{center}
     \begin{itemize}
     \item<2->Split $P$ into pieces $P_1,\ldots,P_k$ with a funnel $F$, so
that $\Pr(P_i) \le \Pr(P)/2$. 
     \item<3->Triangulate $F$
     \item<4->Recursively triangulate each of $P_1,\ldots,P_k$
     \end{itemize}
     \item<5->Notice: Any triangle contained in $P$ intersects at most
3 pieces of $P_1,\ldots,P_m$
   \end{itemize}
}



\frame
{
   \frametitle{Triangulating a Funnel}
   \begin{center} 
     \only<1>{\includegraphics[height=1in]{funnel1}}
     \only<2>{\includegraphics[height=1in]{funnel2}}
     \only<3>{\includegraphics[height=1in]{funnel3}}
     \only<4>{\includegraphics[height=1in]{funnel4}}
   \end{center}
   \begin{itemize}
     \item<2->Split into two half-funnels (one reflex chain each)
     \item<3->Treat each half-funnel like a convex polygon using Dujmovi\'c et al 2007
     \item<4->Finish off by trimming and triangulating any non-triangles
   \end{itemize}
}

\frame
{
  \frametitle{Analysis}
  \begin{itemize}
    \item<1-> Treat the triangulation as a tree
      \item<2-> The root is the funnel $F$
      \item<3-> $P_1,\ldots,P_k$ are subtrees of the root
      \item<4-> The triangulation of $F$ is a subtree of the root
      \item<5-> Notice: If $t$ is a triangle at depth $i$, then $\Pr(t)\le
1/2^i$
      \item<5-> Notice: If $t_1,\ldots,t_m$ are genetically
independent then any triangle in $P$ intersects at most $3\times O(1)$ of
$t_1,\ldots,t_m$ 
  \end{itemize}
}

\frame
{
  \frametitle{Finishing Up}
  \begin{itemize}
    \item<1-> We now have a triangulation $\Delta$ whose triangles are
oganized into a tree
    \item<2-> Partition $\Delta$ into genetically independent subsets
$\Delta_1,\ldots,\Delta_m$ 
    \item<3-> Show that $H(\Delta_1,\ldots,\Delta_m)$ is small
  \end{itemize}
}

\frame
{
	\frametitle{Picking the $V_i$s}

	\begin{itemize}
	\item<1-> Let
	\[
		\Delta_i=\{t\in \Delta :1/2^{i-1} \le \Pr(\Delta) \le 1/2^i \}
	\]
	\item<2-> But these $\Delta_i$ are not genetically independent!
 	\item<3-> While $|\Delta_i|\ge 2^{\alpha i}$  [constant $0< \alpha
< 1$]
	   \begin{itemize}
		\item $\Delta_{i,j}$ is a genetically independent subset of
			$\Delta_i$ with size $2^{\alpha i}/i$
		\item $\Delta_i\gets \Delta_i\setminus \Delta_{i,j}$
		\item $j\gets j+1$
	   \end{itemize}
	\item<4-> With the right $\alpha$ and some work, we can show that
	\begin{eqnarray*}
	H(\{\cup\Delta_{i,j}\}) \le O(H(\Delta)^{2/3}+1)
	\end{eqnarray*}
	\end{itemize}
}


\frame
{
  \frametitle{Finishing Up}
  \begin{itemize}
    \item<1-> By Lemma~3, 
     \[ H(\Delta) \le H(\Delta^*) + O(H(\Delta^*)^{2/3}+1) \]
    \item<2-> By Lemma~1, the expected query time of any linear decision tree is at least
     \[ \mu_D(T^*) \ge H(\Delta^*) + O(\log (H(\Delta^*)) \]
    \item<3-> Using the algorithm of Arya et al, we get a data
structure whose expected cost is at most
     \[ \mu_D(T) \le H(\Delta) + O(H(\Delta)^{1/2}+1) \le \mu_D(T^*) + O(\mu_D(T^*)^{2/3}) \]
  \end{itemize}
}

\section{Theorem 1}
\frame
{
	\frametitle{Theorem 1}
	\begin{itemize}
	\item<1-> For any simple planar subdivision $G$ of size $n$ and
any probability measure $D$ over $\R^2$, a point location structure
$T$ for $G$ can be
constructed in $O(n\log n)$ time and
\[
	\mu_D(T) \le \mu_D(T*) + O(\mu_D(T^*)^{2/3}) \enspace ,
\]
where $T^*$ is any linear decision tree for point location in $G$.
         \item<2-> \begin{center}\Huge THE END\end{center}
         \end{itemize}
}


\end{document}

