\documentclass{DIKU-article}
%\usepackage{charter}
\usepackage[dvips]{graphicx}

\input{hyphenation.txt}

%\renewcommand{\baselinestretch}{1.3}
%\input{pat.txt}

\newcommand{\seclabel}[1]{\label{sec:#1}}
\newcommand{\secref}[1]{\mbox{Section~\ref{sec:#1}}}
\newcommand{\thmref}[1]{Theorem~\ref{theorem:#1}}
\newcommand{\comment}[1]{}

\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceils}[1]{\lceil #1 \rceil}
\newcommand{\sequence}[1]{\left\langle#1\right\rangle}
\newcommand{\set}[1]{\left\{#1\right\}}

\newcommand{\Findmin}{\mbox{$\mathit{find}$}\textnormal{-}\allowbreak{}\mbox{$\mathit{min}$}}
\newcommand{\Findmax}{\mbox{$\mathit{find}$}\textnormal{-}\allowbreak{}\mbox{$\mathit{max}$}}
\newcommand{\Member}{\mbox{$\mathit{find}$}\textnormal{-}\allowbreak{}\mbox{$\mathit{member}$}}
\newcommand{\Predecessor}{\mbox{$\mathit{find}$}\textnormal{-}\allowbreak{}\mbox{$\mathit{predecessor}$}}
\newcommand{\Successor}{\mbox{$\mathit{find}$}\textnormal{-}\allowbreak{}\mbox{$\mathit{successor}$}}
\newcommand{\Insert}{\mbox{$\mathit{insert}$}}
\newcommand{\Erase}{\mbox{$\mathit{erase}$}}
\newcommand{\Extractmin}{\mbox{$\mathit{extract}$}\textnormal{-}\allowbreak{}\mbox{$\mathit{min}$}}
\newcommand{\Extractmax}{\mbox{$\mathit{extract}$}\textnormal{-}\allowbreak{}\mbox{$\mathit{max}$}}
\newcommand{\Merge}{\mbox{$\mathit{merge}$}}
\newcommand{\Split}{\mbox{$\mathit{split}$}}
\newcommand{\Decrease}{\mbox{$\mathit{decrease}$}}
\newcommand{\Begin}{\mbox{$\mathit{begin}$}}
\newcommand{\End}{\mbox{$\mathit{end}$}}
\newcommand{\Borrow}{\mbox{$\mathit{borrow}$}}
\newcommand{\Search}{\mbox{$\mathit{search}$}}

\newcommand{\Null}{\mbox{$\mathit{null}$}}
\newcommand{\Operator}{\mbox{$\mathit{operator}$}}
\newcommand{\Pred}{\mathrm{pred}}
\newcommand{\Succ}{\mathrm{succ}} 
\newcommand{\Elem}{\mathrm{elem}} 
\newcommand{\Size}{\mathrm{size}}

\titlehead{Putting your data structure on a diet}
\authorhead{Herv\'e Br\"onnimann, Jyrki Katajainen, and Pat Morin}

\title{Putting your data structure on a diet}

\author{Herv\'e Br\"onnimann\inst{1}
\and
Jyrki Katajainen\inst{2}\fnmsep\thanks{%
Partially supported by the Danish Natural Science
Research Council under contract 272-05-0272
(project ``Generic programming---algorithms and tools'').%
}
\and
Pat Morin\inst{3}%
}

\institute{Department of Computer and Information Science, Polytechnic University\\
Six Metrotech, Brooklyn NY 11201, USA
%\email{hbr@poly.edu}
\and
Department of Computing, University of Copenhagen\\
Universitetsparken 1, 2100 Copenhagen East, Denmark
%\email{jyrki@diku.dk}
\and
School of Computer Science,
Carleton University\\
1125 Colonel By Drive,
Ottawa, Ontario, Canada K1S 5B6 
%\email{morin@cs.carleton.ca}
}

\dates{Copyright \copyright{} 2006 by Performance Engineering
Laboratory and the authors. This paper was presented at the 6th STL
Workshop held in Copenhagen on October 30th, 2006. Reproduction of
this paper is permitted for academic, non-profit purposes provided
that this text is included. Version: \today }

\begin{document}
\maketitle
\begin{abstract}
Consider a data structure $\mathcal{D}$ that stores a dynamic
collection of elements. Assume that $\mathcal{D}$ uses a linear number
of words in addition to the elements stored.  In this paper several
data-structural transformations are described that can be used to
transform $\mathcal{D}$ into another data structure $\mathcal{D}'$
that has the same functionality as $\mathcal{D}$, has considerably
smaller memory overhead than $\mathcal{D}$, and performs the supported
operations by a small constant factor or a small additive term slower
than $\mathcal{D}$, depending on the data structure and operation in
question. The compaction technique has been successfully applied for
linked lists, dictionaries, and priority queues.
\end{abstract}

\begin{keywords}
Data structures, lists, dictionaries, priority queues, space efficiency
\end{keywords}

\section{Introduction}
\seclabel{intro}

In this paper we consider the space efficiency of data structures that
can be used for maintaining a dynamic collection of
elements. Earlier research in this area has concentrated on the space
efficiency of some specific data structures or on the development of
implicit data structures. Our focus is on data-structural
transformations that can be used to transform a data structure
$\mathcal{D}$ into another data structure $\mathcal{D}'$ that has the
same---or augmented---functionality as $\mathcal{D}$, has about the same efficiency as
$\mathcal{D}$, but uses significantly less space than $\mathcal{D}$.
In particular, we consider a compaction technique that can be
applied with minor variations to several different data structures.

One particular aspect of concrete implementations of element containers
that has received much attention is the issue of \emph{memory
overhead}, which is the amount of storage used by a data
structure beyond what is actually required to store the elements manipulated.
The starting point for our research was the known implicit data
structures where the goal is to reduce the memory overhead to $O(1)$ words
or $O(\lg n)$ bits, where $n$ denotes the number of elements stored.  A
classical example is the binary heap of Williams \cite{Wil64}, which is a
priority queue supporting the methods \Findmin{}, \Insert{}, and
\Extractmin{}. Another example is the searchable heap of Franceschini
and Grossi \cite{FG03}, which is an ordered dictionary supporting
methods \Member{}, \Predecessor{}, \Successor{}, \Insert{}, and
\Erase{}. A searchable heap is a complicated data structure and
because of the bit encoding techniques used it can only store a set,
not a multiset.

One should observe that implicit data structures are
designed on the assumption that there is an infinite array available
to be used for storing the elements. In other words, it is assumed
that the whole memory is used for the data structure alone and no
other processes are run simultaneously. To relax this assumption,
a resizable array could be used instead.  It is known that any
realization of a resizable array requires at least $\Omega(\sqrt{n})$
space for pointers and/or elements \cite{BCDMS99}, and realizations
exist that only require $O(\sqrt{n})$ extra space \cite{BCDMS99,KM01}.
In \cite{BCDMS99}, it was shown that a space-efficient resizable array
can perform well under a realistic model of dynamic memory
allocation. In particular, the waste of memory due to internal
fragmentation is $O(\sqrt{n})$ even though external
fragmentation can be large \cite{LL85}.

We call a data structure \emph{elementary} if it only allows
key-based access to elements. In particular, all
implicit data structures are elementary.  An important requirement often
imposed by modern libraries (e.g.~C\texttt{++} standard library
\cite{ISO}) is to provide location-based access to elements, as well as
to provide iterators to iterate over a collection of elements. In
general, for efficiency reasons it might be advantageous to allow
application programs to maintain references, pointers, or iterators to
elements stored inside a data structure. For the sake of correctness,
it is important to keep these external references valid at all times. We call
this issue \emph{referential integrity} (the corresponding term used
in the C\texttt{++} standard \cite{ISO} is \emph{iterator validity}).
For non-elementary priority queues,
location-based access is essential since without it methods \Erase{}
or \Decrease{} cannot be provided. For 
dictionaries, a finger indicates a location, so to
support finger search location-based access must be possible.  In the
widely-used textbook
\cite{CLRS01}, \emph{handles} are proposed as a general solution to
achieve referential integrity but this technique 
alone increases the memory overhead of the data structure in
question by $2n$ pointers.

For fundamental data structures, like dictionaries and
priority queues studied in this paper, many implementations
are available, each having own advantages and disadvantages.  In
earlier research, the issues considered include the repertoire of
methods supported, complexity of implementation, constants involved
in running times, worst-case versus average-case versus amortized
performance guarantees, memory overhead, and so on.  Many
implementations are also special because they provide certain
\emph{properties}, not provided by implicit data structures.  Examples
of such properties include the working-set property \cite{i01,st85b},
the queueish property \cite{il02}, the unified property
\cite{i01,st85b}, the ability to \Insert{} and \Erase{} in constant
time \cite{Fle96,hm82,LO88}, and the (static or dynamic) finger
property \cite{as96,bt80,c95,gmpr77,hm82}.  These special properties
are a large part of the reason that so many implementations of
fundamental data structures exist.  Many algorithmic applications require
data structures that have one or more of these special properties to ensure the
bounds on their running times or storage requirements.  Thus, it is
not always possible to substitute one implementation for another.

Several dictionary implementations, including variants of
height-balanced search trees \cite{Bro79}, self-adjusting search trees
\cite{And99,gr93,st85b}, and randomized search trees \cite{as96,p90},
have been proposed that reduce the extra storage to two pointers per
element. However, when a dictionary has to support
location-based access to elements, additional pointers (like parent
pointers) are often needed. By using the child-sibling representation
of binary trees (see, e.g.~\cite[Section 4.1]{Tar83}) and by
compacting the bits describing the type and colour of nodes in pointers (as proposed, for example, in
\cite{BK06}), red-black trees supporting location-based access are
obtained that only use $2n + O(1)$ words of memory in addition to the
$n$ elements stored.

In contrast with fully implicit binary heaps, many priority-queue
implementations rely on pointers. Actually, pointer-based
implementations are often necessary to provide referential integrity,
and to support \Erase{} and constant-time \Decrease{}. Of the known
implementations, all need at least $2n$ extra words \cite{DW93,MP05},
$3n$ extra words \cite{EJK05}, or more \cite{DGST88,KST02}.  We note
that the implicit priority queue structure of \cite{MP05}, which is
used to study the complexity of implicit priority queues that support
the \Decrease\ operation, avoids the issue of referential integrity
by forcing the calling application to maintain a mapping between keys
and their location within the structure.  Thus, although the
structure uses only a constant number of words of overhead, any
application that wishes to use \Decrease\ operations is responsible
for keeping track of the locations of these keys in the data
structure.

In this paper we elaborate upon a technique that can be used to make a
pointer-based data structure that uses $cn$ words of extra storage,
for a positive constant $c$, into an equivalent data structure that
uses either $\varepsilon n$ or $(1 + \varepsilon) n$ words of extra
storage, for any $\varepsilon>0$ and sufficiently large $n >
n(\varepsilon)$.  By applying this technique, any application program
can employ the data structure (and its properties) without worrying
about the memory overhead. In some special cases, we can even make the
amount of extra space needed as small as $O(n/\lg n)$.%
\comment{Observe that here the extra space means words plus elements.}
Under reasonable assumptions, the data-structural transformations
increase the running times of the operations on the data structure only by a
small constant factor independent of $\varepsilon$ and/or an additive
term of $O(1/\varepsilon)$.

The compaction technique used by us is simple, and similar to the idea
used in degree-balanced search trees \cite{hm82} where each node
contains at least $a$ elements and at most $b$ elements for
appropriately chosen $a$ and $b$. That is, instead of operating on
elements themselves, we operate on groups of elements that we call
\emph{chunks}.% 
\comment{Each chunk has a \emph{representative} and the
underlying data structure operates on these representatives. The
elements in a chunk determine an \emph{interval} of values.} When the
compaction technique is applied to different data structures, there
are minor variations in the scheme depending on how chunks are
implemented (as a singly-linked list or an array), how elements are
ordered within chunks (in sorted or in unsorted or in some other
order), and how the chunks are related to each other.

The technique itself is not new; it has been explicitly used, for
example, in the context of AVL-trees \cite{Mun86} and in the context
of run-relaxed heaps \cite{DGST88}.  However, in the latter paper,
referential integrity was not an issue because the elements in the
structure are placed in a memory location at the time of insertion and
are never moved again.  In particular, the space allocated to these
elements is never released until the priority queue is destroyed.
While this is sufficient for some applications (e.g., in an
implementation of Dijkstra's algorithm) it is obviously undesirable in
a general-purpose priority queue.  A similar idea has also been
applied by to general binary search trees by Jung and Sahni
\cite{js03}, but their application of the idea does not guarantee an
arbitrarily small memory overhead and may increase the running times
of operations binary search trees by more than a constant factor.

Our purpose in this paper is to show that, carefully implemented, this
technique is widely applicable and general.  Also, preliminary tests
suggest that the technique seems to be of practical value so our plan
is to verify this experimentally.

The remainder of the paper is organized as follows.  In
\secref{sec:def}, the data structures studied are defined to fix the
notation used throughout the paper.  In
\secref{elementary-dictionaries}, we show how the compaction technique
can be used for reducing the memory overhead of any elementary
dictionary. More specifically, we extend an old result of Munro
\cite{Mun86} by proving that any elementary dictionary can be
converted into an equivalent data structure that requires at most
$O(n/\lg n)$ extra space without affecting the asymptotic running
times of algorithms for searching, inserting, and erasing.  In
\secref{iterator-dictionaries}, we describe two more applications of
the technique that are able to transform any linear-size
dictionary providing iterator support into a more space-efficient data
structure. In \secref{iterator-queues} we prove similar results for
priority queues.  Finally, in \secref{discussion} we mention some
other applications of the compaction technique and conclude with a few
final remarks.

\section{Data structures studied}%
\seclabel{sec:def}

When defining the data structures considered, we have used the
interfaces specified in the C\texttt{++} standard \cite[Clause
23]{ISO} as a model, but we use our own conventions when naming the
methods. The main reason for using generic method names is brevity and
our aim at using a unified notation independent of whether a container
stores a set or a multiset.

\subsection{Elementary dictionaries}

An \emph{ordered dictionary}, or simply a \emph{dictionary}, is a container that stores a
collection of elements drawn from an ordered universe and supports
insertion and erasure of elements and search of elements with respect
to a given element.  We define the methods in a generic form, in which
they can take parameters, $\propto$ and $\mathit{r}$, specified at
compile time and parameters, $\mathcal{S}$ and $x$, specified at run
time. Parameter $\propto \in \set{\diamond,<,>,=}$ is a binary
predicate used for specifying an interval of interest in
$\mathcal{S}$. Binary predicate $\diamond$ is special since $x
\diamond y$ is defined to be true for any two elements $x$ and $y$. We
only allow $r$ to take two values $1$ and $-1$, meaning the first and
the last element on an interval, respectively.

In its elementary form, a realization of a dictionary should
provide (a subset of) the following methods:
\begin{description}

\item[$\Search\sequence{\propto,\mathit{r}}(\mathcal{S}, x).$] Return
a reference to the $r$th element among all elements $v$ in container
$\mathcal{S}$ for which predicate $v \propto x$ is true.  If no such
element exists, return a reference to a special \Null{} element.
%The
%function used in element comparisons is to be specified at the
%construction time of $\mathcal{S}$.

\item[$\Insert{}(\mathcal{S}, x).$] Insert a copy of element $x$ into container $\mathcal{S}$.

\item[$\Erase\sequence{\propto,\mathit{r}}(\mathcal{S}, x).$] Remove
the $r$th element among all elements $v$ in container $\mathcal{S}$
for which predicate $v \propto x$ is true.  If no such element exists,
do nothing.
\end{description}
For these methods argument $x$ can be
partially constructed; i.e.~if an element is a pair of a key and some
satellite data, only the key needs to be specified before a search.

The generic form of \Search{} is an archetype for 8 different methods
depending on how the compile-time parameters are set. If $\mathcal{S}$
stores a set, generic parameter $r$ would be superfluous.
Colloquially, invocations of various forms of \Search{} are called
\emph{searches}, and invocations of \Insert{} and
\Erase{} \emph{updates}. 
Conventional
names for methods
$\Search\sequence{=,1}$,
$\Search\sequence{<,-1}$,
$\Search\sequence{>,1}$,
$\Search\sequence{\diamond,1}$, 
$\Search\sequence{\diamond,-1}$,
$\Erase\sequence{\diamond,1}$, and
$\Erase\sequence{\diamond,-1}$
would be
\Member{},
\Predecessor{}, 
\Successor{}, 
\Findmin{}, 
\Findmax{},
\Extractmin{}, and
\Extractmax{},
respectively.
In addition to the above-mentioned methods, there should be methods
for constructing and destroying elements and sets, and methods for
examining the cardinality of sets, but these are algorithmically less
interesting and therefore omitted in our discussion.

\subsection{Semi-implicit dictionaries}

Assuming that at a given point in time an elementary dictionary
contains $n$ elements, an \emph{implicit dictionary} stores the
elements in the first $n$ locations of an infinite array and uses at
most $O(1)$ additional words for book-keeping purposes.  Because
implicit dictionaries can be complicated \cite{FG03}, our goal is to
develop a \emph{semi-implict dictionary}, as discussed by Munro
\cite{Mun86}, which also stores the data elements compactly at the
beginning of the infinite array and only uses $o(n)$ words of extra
memory for book-keeping purposes.  The main motivation for considering
semi-implicit dictionaries is provide a space-efficent dictionary data
structure that can store multisets, which is not known to be possible
for implicit dictionaries. Of course, in a practical implementation a
resizable array would be used instead of an infinite array.

\subsection{Dictionaries providing iterator support}

In modern libraries, a dictionary data structure is supposed to
support both key-based and location-based access to elements. A
location can be specified by a reference, a pointer, or an object
specially designed for this purpose---like a locator or an iterator to
be defined next.

A \emph{locator} is a mechanism for maintaining the association
between an element and its location in a data structure \cite[Section
6.4]{GT98}. A locator follows its element even if the element changes
its location inside the data structure. In terms of the C\texttt{++}
programming language, a locator is an object that provides
default constructor, copy constructor, unary operator * (prefix), and
binary operators \texttt{==} and \texttt{!=}. That is, for
locator $p$, *$p$ denotes the element stored at the specified
location.  An \emph{iterator} is a generalization of a locator that
captures the concepts \textit{location} and \textit{iteration} in a
container of elements. In addition to the operations supported for a
locator, a \emph{bidirectional iterator} provides
unary operators \texttt{++} and \texttt{-}\texttt{-} (both prefix and
postfix). That is,
for iterator $p$, \texttt{++}$p$ returns an iterator pointing to the
element following the current element according to some
specific order. An error occurs if $p$ points to the past-the-end
element of the corresponding container.

For an ordered dictionary providing location-based access to elements
by means of iterators, the methods \Search{}, \Insert{}, and
\Erase{} are overloaded as follows:
\begin{description}
\item[$\Search\sequence{\propto, r}(\mathcal{S}, p, x).$] 
Start the search from the element pointed to by iterator $p$ and return
an iterator to the $r$th element among all elements $v$ in container
$\mathcal{S}$ for which predicate $v \propto x$ is true. If no such
element exists,  return an iterator to the past-the-end element.

\item[\Insert{}$(\mathcal{S}, p, x).$] Insert a copy of element $x$
\emph{immediately before} the element pointed to by iterator $p$ in
container $\mathcal{S}$, and return an iterator pointing to the newly inserted element.

\item[\Erase{}$(\mathcal{S}, p).$] Remove the element pointed to by iterator $p$
from container $\mathcal{S}$.
\end{description}

To iterate over a collection of elements, it is also possible to
access some special locations:
\begin{description}
\item[\Begin{}$(\mathcal{S}).$] Return an iterator pointing to the first element
in container $\mathcal{S}$.

\item[\End{}$(\mathcal{S}).$] Return an iterator pointing to the past-the-end
element for container $\mathcal{S}$.
\end{description}
Since \Begin$(\mathcal{S})$ points to the minimum element of $\mathcal{S}$ and
\texttt{-}\texttt{-}\End$(\mathcal{S})$ to the maximum element (if any),
methods \Findmin{} and \Findmax{} become superfluous.

\subsection{Dictionaries providing random-access iterators}

An ordered dictionary stores its elements in sorted order. Therefore,
it is natural to augment a dictionary (as discussed, for example, in
\cite[Section 14.1]{CLRS01}) to support \emph{order-statistic queries}
returning the $i$th element stored in a dictionary, and \emph{rank
queries} returning the position (i.e.~index) of a given element in a
dictionary.  Instead of proving separate methods to carry out these
queries, one could simply upgrade the iterators to provide random
access.  A \emph{random-access iterator} \cite[Clause 24]{ISO} is as a
bidirectional iterator, but also allows iterator additions, iterator
substractions, and iterator comparisons. That is, for integer $i$ and
iterators $p$ and $q$, $p+i$, $p-i$, $p-q$, and $p < q$ would be valid
expressions.  For example, $p+i$ advances $i$ positions from the
element pointed to by $p$ and returns an iterator to that element,
provided that the past-the-end element is not passed.

\subsection{Elementary priority queues}

In its elementary form, a \emph{priority queue} stores a collection of
elements and supports operations \Findmin{}, \Insert{}, and
\Extractmin{}.  We want to point out that we do not consider
elementary priority queues since a fully implicit priority queue
exists that can support \Findmin{} and \Insert{} in $O(1)$ worst-case
time, and \Extractmin{} in $O(\lg n)$ worst-case time \cite{CMP88},
$n$ being the number of elements stored. As mentioned in
\secref{intro}, in a realistic model of computation, the amount of
extra space used by such a data structure is only $O(\sqrt{n})$.  In
\cite{CMP88}, it was shown that there even exists a fully implicit
double-ended priority queue, i.e.~a priority queue that supports
\Findmax{} and \Extractmax{} as well.

\subsection{Priority queues providing locator support}

In a general form, a \emph{priority queue} is an element container
that provides methods \Findmin{} and location-based \Erase{} as well as (a
subset of) the following methods:
\begin{description}
\item[\Insert{}$(\mathcal{Q}, x).$] Insert a copy of element $x$ into
container $\mathcal{Q}$ and return a locator pointing to the newly inserted
element.

\item[\Borrow{}$(\mathcal{Q}).$] Remove an \emph{arbitrary} element from
container $\mathcal{Q}$ and return a locator pointing to that element. If $\mathcal{Q}$
is empty, return a locator pointing to the past-the-end element.

\item[\Decrease{}$(\mathcal{S}, p, x).$] Replace the element pointed to by
locator $p$ with element $x$, which is assumed to be no greater than
the original element pointed to by $p$.
\end{description}
Of these methods, \Borrow{} is non-standard, but according to our
earlier research \cite{EJK04,EJK07} it is a valuable tool, for
instance, in data-structural transformations. Our point to include it
in the repertoire of priority-queue operations is to show that our
compaction scheme can handle it quite easily. If the original priority
queue provides it, the compact version will do the same at about equal
efficiency.

As dictionaries, priority queues could also provide iterator
support. However, in a priority queue there is no natural order of
storing the elements. In a basic form, an iteration starting from
\Begin{} and ending at \End{} may just be guaranteed to visit all
elements. Iterator operations could be realized in the same way as for
dictionaries, so in this paper we concentrate on priority
queues that support the above-mentioned priority-queue
operations.

\section{Compaction of elementary dictionaries}%
\seclabel{elementary-dictionaries}

In this section we describe transformations that can be applied to
elementary ordered dictionaries to make them more space-efficient.
Our approach is to partition the data into chunks and store these
chunks in the ordered dictionary under transformation (skiplist,
2-3 tree, red-black tree, etc).  To avoid confusion, we will refer the
data structure we are describing as ``the dictionary'' and the data
structure we are using as ``the data structure.''  Similarly, we will
use the terms chunks and elements to refer to groups of elements and
individual elements, respectively.  

We study two different schemes based on this idea.  Both schemes have
the following in common:  Each chunk has a header containing a
constant amount of data, the elements of each chunk are stored in an
array, the elements stored within each chunk are kept in sorted order,
and the chunks themselves are stored in a doubly-linked list (the
pointers for which are stored in the header).  At all times we
maintain the \emph{global invariant} that every element in chunk $A$
is less than or equal to every element in chunk $B$ if $A$ appears
before $B$ in this list.  Thus, a traversal of this \emph{chunk list}
is sufficient to output all elements in sorted order.

In addition to being linked into a list, the chunks themselves are
stored in the data structure.  Notice that, because of the above
invariant, any two chunks can be compared in $O(1)$ time.  To test,
for example, if chunk $A$ is less than chunk $B$ we simply check if
the last element of $A$ is less than the first element of $B$.  It is
this comparison function that we use to store the chunks in the
dictionary.  Similarly, we can compare a chunk to an item $x$.  To
test if $x$ is less than $A$ we test if $x$ is less than the first
element of $A$.  To test if $x$ is greater than $A$ we test if $x$ is
greater than the last element of $A$.  If $x$ is neither greater than
nor less than $A$ then $x$ is equal to $A$.  

Observe also in Figure 1 an innocent pointer back to S. This means
that the data structure transformed must guarantee referential
integrity. This is not a problem since we can use handles to achieve
this. However, if we are maintaining a multiset, the data structure
transfered should also support iterator-based delete. Otherwise, we
cannot be sure that we delete the right chunk from the upper store
when the representative of a chunk changes.

With this representation, it is easy to see how \Search{} can be
implemented by performing at most one search in the dictionary
followed by one (linear or binary) search in a chunk.  The special
searches  \Findmin{} and \Findmax{} are even easier;  we simply return
the first element of the first chunk or the last element of the last
chunk, respectively.  Where our two implementations differ is in how
they implement the \Insert{} and \Erase{} operations.

\noindent\textbf{Jyrki: I am unsure whether these schemes can work
without the representatives. The reason I used them was that I was
afraid of that the top structure can take copies of the
representatives, and the bottom structure can change the
representatives. However, I was not able to construct a data structure
where this would be a problem, but the contruction should work for all
possible data structures. Pat: In such a case, can we just use one
extra level of indirection, as described in the next section (see
footnote 2)?  Also: doesn't this also handle the problem of deleting
the ``wrong'' element in the case of a multiset?}

\subsection{Fixed-sized chunks}

The simplest realization of the above ideas is to store the chunks as
fixed-size arrays, each of size $b+1$.  With the exception of at most
one chunk, each chunk will store either $b-1$, $b$, or $b+1$ data
elements.

To insert $x$ into the data structure we first search for $x$ in the
data structure.  The result of this is some chunk $A$ that is either
the first chunk, the last chunk, or a chunk in which $x$ is greater
than the first element and less than the last element.  Next, by
traversing the chunk list we examine up to $b$ consecutive chunks
including $A$.

If any of these $b$ consecutive chunks contains fewer than $b+1$
elements then we can reassign the elements to chunks so as to place
$x$ into some chunk and restore our global invariant.  Otherwise, each
of the $b$ chunks contains $b+1$ elements.  In this case we create a
new chunk and reassign these $b(b+1)$ elements across the $b+1$ chunks
so that each chunk contains exactly $b$ elements.  Finally, we place
$x$ in the appropriate chunk and insert the newly created chunk into
the data structure.

Erasing an element is similar to insertion. To erase $x$, we first
search for the chunk $A$ containing $x$. Then, in the $b$ consecutive
chunks including $A$ either: (1)~there is some chunk containing $b$ or
$b+1$ elements, in which case we remove $x$ and redistribute elements
so as to maintain the global invariant, or (2)~all $b$ chunks contain
exactly $b-1$ elements, in which case we delete one chunk from the
data structure and redistribute the $b(b-1)-1$ elements among the $b-1$
remaining chunks so that each chunk stores $b$ elements, except one,
that contains $b-1$ elements.

It is clear that both the insertion and deletion procedures require
one search in the data structure, $O(b^2)$ work, and at most one
insertion or deletion, respectively, in the data structure.  Indeed a
slightly more careful amortized analysis shows that a sequence of $n$
\Insert/\Erase{} operations on the dictionary requires only $O(n/b)$
\Insert/\Erase{} operations on the data structure.  

\begin{theorem}
\label{theorem:elementary-a}
Given an ordered dictionary that requires $S(n)$, $I(n)$, and $E(n)$
time to search, insert, and erase (respectively) an element from a
set of $n$ elements and that has a memory overhead of $cn$ for a
positive constant $c$, we can construct an equivalent data structure
that requires
\begin{enumerate}
\item $S'(n)=O(S(n/b)+\lg b)$, $I'(n)=O(S(n/b)+I(n/b)+b^2)$,
and $E'(n)=O(S(n/b)+E(n/b)+b^2)$ worst-case time to search, insert,
and erase (respectively), 
\item $S''(n)=O(S(n/b)+\lg b)$,
$I''(n)=O(S(n/b)+\frac{1}{b}I(n/b)+b^2)$, and
$E'(n)=O(S(n/b)+\frac{1}{b}E(n/b)+b^2)$ amortized time to search,
insert, and erase (respectively), and 
\item a memory overhead of $O(n/b)$.
\end{enumerate}
\end{theorem}

One issue that several researchers have been concerned with is that of
fragmentation. \emph{Internal fragmentation} is the amount space
within the data structure that is not used.  \emph{External
fragmentation} is the space that cannot be used because the allocation
of memory segments was disadvantageous.  Using the above scheme, in
combination with simple techniques for maintaining a pool of buffers
for the chunks, the entire dictionary described above can be made to
live in one resizable array that contains $n$ data elements and
$O(n/b)$ additional information.  In this case, the internal and
external fragmentation is no worse than the fragmentation required in
an implementation of a resizable array.

\subsection{Variable-sized chunks}

One drawback of the scheme described in the previous subsection is the
$O(b^2)$ term in the running time for the \Insert{} and \Erase{}
operations.  In this subsection we describe an alternative that avoids
this problem but requires more extensive use of memory management and,
consequently, for which it is more difficult to avoid fragmentation.
In this scheme, all but one chunk contains somewhere between $b/2$ and
$2b$ elements, inclusive.\footnote{Throughout this section we assume
$b$ is even so that $b/2$ is an integer.}  However, unlike the
previous scheme, each chunk is allocated to the precise size required
to store the number of elements it currently contains.

Since chunks will be reallocated frequently and are addressed by the
nodes of the data structure, each chunk also has an associated
\emph{handle}.  This is a memory location that stores a pointer to the
chunk and the header of each chunk stores a pointer to this handle.
The data structure, rather than maintain direct pointers to the
chunk, maintains pointers to the handle.  In this way, when a chunk
is reallocated it only need update its handle.\footnote{This extra
level of indirection adds some overhead to the running time.  It is
unnecessary for dictionary implementations that store, or refer to,
each data element only once, as is the case with most dictionary
implementations.} From this point on, updating of handles is implicit
when we say that a chunk is resized.

An insertion of the element $x$ in the data structure proceeds as
follows: The chunk $A$ that should contain $x$ is located.  If $A$
contains fewer than $2b$ elements then $A$ is resized to make room for
$x$ and $x$ is added to $A$.  Otherwise, if $A$ already contains $2b$
elements then $A$ is split into two chunks, each containing $b$
elements, one of the new chunks is inserted into the dictionary, and
$x$ is added to the appropriate chunk.

The \Erase{} procedure is similar to the \Insert{} procedure.  If the
chunk $A$ containing $x$ contains more than $b/2$ elements then $A$ is
resized and $x$ is removed.  Otherwise, we check one of the
neighbouring chunks, $B$, of $A$. If $B$ contains more than $b/2$
elements then an element is removed from $B$ and used to replace $x$
in $A$ and then $B$ is resized. Otherwise, each of $A$ and $B$
contains exactly $b/2$ elements, in which case we merge them into a
single chunk containing $b$ elements and delete one of the chunks from
the data structure.

Again, it is clear that both \Insert{} and \Erase{} can be done using at
most one search, $O(b)$ time to operate on the at most 2 chunks
involved, and at most one insertion and extraction.  Again, a simple
amortization argument shows that a sequence of $n$ \Insert{} and
\Erase{} operations on the dictionary results in only $O(n/b)$ \Insert\
and \Erase{} operations in the data structure.

\begin{theorem}
\label{theorem:elementary-b}
Given an ordered dictionary that requires $S(n)$, $I(n)$, and $E(n)$
time to search, insert, and erase (respectively) an element from a
set of $n$ elements and that has a memory overhead of $cn$ for a
positive constant $c$, we can construct an equivalent data structure
that requires 
\begin{enumerate}
\item $S'(n)=O(S(n/b)+\lg b)$, $I'(n)=O(S(n/b)+I(n/b)+b)$,
and $E'(n)=O(S(n/b)+E(n/b)+b)$ worst-case time to search, insert,
and erase (respectively), 
\item $S''(n)=O(S(n/b)+\lg b)$,
$I''(n)=O(S(n/b)+\frac{1}{b}I(n/b)+b)$, and
$E'(n)=O(S(n/b)+\frac{1}{b}E(n/b)+b)$ amortized time to search,
insert, and erase (respectively), and 
\item a memory overhead of
$O(n/b)$.
\end{enumerate}
\end{theorem}

\comment{
\comment{The components used by this compaction scheme are
illustrated in Figure~\ref{fig:array}.

\begin{figure}
\begin{center}
\input{array.pstex_t}
\end{center}
\caption{A chunk storing seven integers in an array.\label{fig:array}}
\end{figure}
}

We call the objects containing a pointer to the beginning of a chunk,
the \emph{head} of that chunk.  A head $f$ contains two pointers
$\Pred(f)$ and $\Succ(f)$ that point to other heads, a pointer to an
array $\Elem(f)$ that contains anywhere between $b$ and $2b$ elements,
and an integer $\Size(f)$ that tells the number of elements in
$\Elem(f)$.  The $\Pred$ and $\Succ$ fields are used to link all heads
together into a doubly-linked list.  The elements of $\Elem(f)$ are
always sorted in increasing order, and the elements of $\Elem(f)$ are
all less than the elements of $\Elem(g)$ if $f$ appears before $g$ in
the doubly-linked list.  In this way, the list of heads gives the
elements of the dictionary in sorted order.  Additionally, two special
pointers are maintained that point to the first and the last head,
respectively.  The data structure, denoted $\mathcal{S}$, stores
\emph{handles} to the heads.  Each head contains an iterator pointing
back to the corresponding node inside the data structure.  If the
iterators of the data structure were normal pointers, the extra
indirection could be avoided and the data structure could store the
heads directly.

Note that the comparison of two heads is a constant time operation,
since to test if $f<g$ we compare the last element of $\Elem(f)$ to
the first element of $\Elem(g)$.  Similarly, given an element $x$ we
can test if $x<f$, respectively $f<x$, by comparing $x$ with the
first, respectively last, element in $\Elem(f)$.  Therefore, to reduce
the storage requirements of a dictionary, we make a data structure on
a set of heads whose $\Elem$ arrays contain all the elements in our
dictionary.

\paragraph{Searching.}
If we search for some element $x$ in the data structure, we find the
head $f$ such that $\Elem(f)$ contains the smallest key greater
than or equal to $x$.  An additional search in $\Elem(f)$ finds the
actual element we are looking for at a cost of $O(b)$, or $O(\lg b)$
if we use binary search.  If the data structure takes $S(n)$ time to
perform a search on a set of $n$ elements, then the dictionary takes
$S'(n)=O(S(n/b)+\lg b)$ time to perform the search.

\Insert{}$(\mathcal{S}, x)$.
Assume we are given a pointer to the dictionary node containing the
head $f$ we would find if we searched for $x$.  To insert $x$, we
proceed as follows.  If $\Elem(f)$ contains fewer than $2b$ elements
then we simply reallocate $\Elem(f)$ to increase its size by 1 and add
$x$ to $\Elem(f)$.  Otherwise ($\Elem(f)$ contains $2b$ elements), we split
$\Elem(f)$ into two heads, one that contains $b$ elements and one
that contains $b+1$ elements and insert the newly created head
into our data structure.  Therefore, if the data structure takes
$I(n)$ time to perform an insertion on a set of $n$ elements then
insertion in the dictionary takes $I'(n)=O(I(n/b)+b)$ time.

\Erase{}$(\mathcal{S}, x)$.
To remove element $x$ from the dictionary, we assume we are given a
pointer to the dictionary node containing the head $f$ such that
$\Elem(f)$ contains $x$.  If $\Elem(f)$ has size greater than $b$, then
we simply reallocate $\Elem(f)$ to decrease its size by one and exclude
$x$.  Otherwise ($\Elem(f)$ has size $b$), we examine a head $g$
that is a neighbour of $f$ in the linked list.  If $\Elem(g)$ contains
more than $b$ elements then we take the first or last element in
$\Elem(g)$ (depending on whether $g=\Succ(f)$ or $g=\Pred(f)$) and use it
to replace $x$ in $g$.  Otherwise ($\Elem(f)$ and $\Elem(g)$ both have
size $b$), we merge $f$ and $g$ into a single chunk and remove one
of them from the data structure.  If $E(n)$ is the time it takes to
remove element $x$ from a data structure of size $n$, then erasure
in the modified data structure takes $E'(n)=O(E(n/b)+b)$ time.

A data structure containing $m$ heads contains at least $n=bm$
elements.  Each head has a constant amount of overhead, and the data
structure has an overhead of $cm$ for some constant $c$, so the
overhead is $O(n/b)$. The above discussion can be summarized as
follows.

\begin{theorem}
\label{theorem:elementary}
Given an ordered dictionary that requires
$S(n)$, $I(n)$, and $E(n)$ time to search, insert, and erase
(respectively) an element from a set of $n$ elements and that has a memory
overhead of $cn$ for a positive constant $c$, we can construct an equivalent
data structure that requires $S'(n)=O(S(n/b)+\lg b)$,
$I'(n)=O(I(n/b)+b)$, and $E'(n)=O(E(n/b)+b)$ time to
search, insert, and erase (respectively) and that has a memory
overhead of $O(n/b)$.
\end{theorem}
}

Knowing that element-based operations on an ordered dictionary take at
least logarithmic time in the worst case, it is tempting to choose $b
= \lg n$.  The problem with this choice is that $n$, the number of
elements stored, is varying whereas $b$ must be kept fixed. To avoid
this problem, we use Frederickson's partitioning scheme \cite{Fre83}
and partition the data structure into $O(\lg\lg n)$ separate
\emph{portions} of exponentially increasing sizes $2^{2^{1}}$,
$2^{2^{2}}$, $2^{2^{3}}$, and so on, except that the last portion can
be smaller.  Inside each portion we can use a fixed chunk size
$b_i=2^i$ in the $i$th portion. Insertions are done in the last
portion, searches visit all structures but the overall cost is
dominated by the cost incurred by the last portion [all $O(\lg\lg n)$
searches still take $O(\lg n)$ time if the data structure supports
searches in $O(\lg n)$ time], and deletions are performed by borrowing
an element from the last portion. 

\begin{theorem}
\label{theorem:elementary-c}
Given an ordered dictionary that requires $S(n)$, $I(n)$, and $E(n)$
time to search, insert, and erase (respectively) an element from a
set of $n$ elements and that has a memory overhead of $cn$ for a
positive constant $c$, we can construct an equivalent data structure
that requires 
\begin{enumerate}
\item $S'(n)=O(S(n/\lg n)+\lg\lg n)$, $I'(n)=O(S(n/\lg n)+I(n/\lg n)+\lg n)$,
and $E'(n)=O(S(n/\lg n)+E(n/\lg n)+\lg n)$ worst-case time to search, insert,
and erase (respectively), 
\item requires $S''(n)=O(S(n/\lg n)+\lg\lg n)$,
$I''(n)=O(S(n/\lg n)+\frac{1}{\lg n}I(n/\lg n)+\lg n)$, and
$E'(n)=O(S(n/\lg n)+\frac{1}{\lg n}E(n/\lg n)+\lg n)$ amortized time to search,
insert, and erase (respectively), and 
\item that has a memory overhead of
$O(n/\lg n)$.
\end{enumerate}
\end{theorem}

As an example, we can apply Theorem \ref{theorem:elementary-c} to any
worst-case efficient balanced search tree such as a red-black tree to
obtain the following result:

\begin{corollary}
\label{corollary:red-black}
There exists an ordered dictionary that supports searches and updates
in $O(\lg n)$ worst-case time, \Findmin{} and \Findmax{} in $O(1)$
worst-case time, and has a memory overhead of $O(n/\lg n)$.
\end{corollary}

\subsection{A compact semi-implicit dictionary}

\noindent\textbf{Discuss issues with fragmentation; as a starter, here
is my letter dated June 20th 2006.}

The paper has two parts: nonreferential part and referential part. The
referential integrity part includes new material and I cannot see any
problems there. However, the nonreferential part is more problematic
because of the earlier literature on the topic. We cannot just recap
Munro's old result. Therefore I want to add a new discussion about
fragmentation [which can be avoided altogether if we accept Munro's
assumptions and which is unavoidable if we do not accept his
assumptions] and I would also like to improve the space bound to
O(n/lg n) (i.e. show that the construction works for k = lg n).

I have not read your material and I do not quite understand your
problem. I can also see a minor problem with my solution; I need n +
O(n/k) space for the data segment [actually n elements and n/k
pointers], not $n + k^2$ elements as Munro, and my insertion/deletion
cost is still proportional to k, not to $k^2$. By maintaining the
elements and pointers separately, gaps are avoided inside the data
segment.

Let us try to look at the construction in greater detail. Assume that we are using a Munro-like way of handling overloads. The data strcucture has the following parts:

1) The tree nodes that have a pointer to a full data block (of size k), a pointer to a potential overflow block (of size  1..k), and the size of the overflow block. These nodes also store normal pointers and balancing information like a colour.

2) An array of k pointers to the data block categories; each pointer indicates the beginning of the segment containing data blocks of size $i$ ($i \in \set{1,\ldots,k}$). [Probably, some information is needed for the maintanance of the split blocks.]

3) The data blocks themselves are maintained in a contiguous memory segment. Each data block is of size i + O(1); it stores i elements and a pointer back to the corresponding tree node. (These back pointers are needed to keep the references from the tree nodes valid, since data blocks are moved.)

There are at most O(n/k) tree nodes (and data blocks) so the extra space used by this construction is O(k + n/k) words. [Observe that space is allocated for only n elements, assuming that we can make arrays resizable for free.]

To insert an element x, we search for the proper node using the search tree and the proper position of x inside the data blocks. An insertion of x to is done by moving the particular overflow block as the last block in its category (which involves a block swap and some pointer updates). Then this block is interchanged with the last half-full block (if any). [Of course, proper tests should be carried out to see whether the  current overflow block is  the split block in its category. This special case can be handled easily.] Thereafter, the higher categories are rotated one element to the right.  [In this process the category boundaries must updated all along the way.] [Also, if a split data block becomes complete, a pointer from the tree node has to be updated.] Finally, the overflow block is updated to include one new element [which involves O(k) element moves]. Of course, the ordering of elements is maintained inside the data block and the overflow block.

Deletion is symmetric. Searching involves tree searching and binary search inside the relevant data block.

An interesting point here is that referential integrity is a problem also in this data structure; now we are only concerning internal integrity---i.e. only pointers inside the data structure should be kept valid since we do not allow any external references to the data structure.

I was just too lazy to read Munro's construction. Probably, he can provide something that I have missed. Now the fun starts. How to extend this and allow k be lg n?



\section{Compaction of dictionaries providing iterator support}
\seclabel{iterator-dictionaries}

The data structural transformations described in the previous sections
can be used to reduce the memory overhead of dictionary data
structures.  However, they break down when the interface to those data
structures allows access to elements using iterators or even locators.
The reason for this is that, as updates are performed, individual data
elements migrate between chunks.  Thus, without some special mechanism
the iterators for these elements will become invalid.

In this section we show two ways in which these data structures can be
extended to allow iterator access.  The first method is preferable
when the total number of iterators to elements in the data structure
is small and when unused iterators are cleaned up. The second method
is preferable when there are many iterators in use simultaneously. 

\subsection{Lists of element handles}

In this section we present a method of maintaining iterators where the
memory overhead at any given time is proportional to the total number
of iterators in use at that time.  This method augments either of the
two schemes described in the previous section in the following way:
The header of each chunk is augmented so that it maintains a pointer
to an ordered dictionary with iterator support that allows constant
time insertion and deletion.\footnote{In practice, this is probably
overkill and a sorted doubly-linked list would suffice, but this will
increase the search time in \thmref{iterator-a} to $O(S(n/b)+d)$.}
This dictionary, which we call the \emph{element handle list} for the
chunk stores element handles. Each \emph{element handle} maintains a
reference count, a pointer to the chunk that contains the element
referenced, and the index within the chunk of the element referenced.
The iterators themselves are pointers to element handles.

When a new iterator for an element $x$ is created, the element handle
list for the chunk containing $x$ is searched.  If there is already an
element handle for $x$ then its reference count is incremented and a
pointer to this handle is used as the iterator.  Otherwise, a new
element handle for $x$ is created, added to the list, and initialized
with a reference count of 1.  Since an element handle list stores at
most $b$ element handles, this process takes $O(\log b)$ time.

When an iterator for an element $x$ is destroyed, the reference count
of the element handle is decremented. If the reference count drops to
zero, then the element handle is deleted from the list and the memory
associated with it is freed.  This process takes $O(1)$ time.

When the algorithms of the previous section operate on chunks by
either growing them, splitting them, merging or moving elements
between them, the element handles and element handle lists associated
with them must be updated appropriately to reflect the changes.
However, these changes only require changing the contents and/or
pointers of the element handles and therefore, once they are updated,
the iterators remain valid.  This updating of element handles is
easily done without increasing the ($O(b)$) running time of each of
the above operations.

It is clear that the iterators defined this way offer constant time
access to the element which they locate.  They can be created in
$O(\log b)$ time and destroyed in $O(1)$ time.  Furthermore, because
the element handle lists support iteration and constant time insertion
and deletion, iterators can be incremented and decremented in $O(1)$
time.  If $k$ is the total number of elements in the dictionary
referenced by iterators at any point in time then the overhead of the
dictionary, at that point in time, is $O(n/b+k)$.  Note that the
$O(k)$ term is in some sense necessary since, if a program using the
dictionary maintains $k$ active iterators, then these must be stored
somewhere within the program and thus there is already $\Omega(k)$
space used within the program to keep track of these iterators.
Applying this augmentation to the data structure of
\thmref{elementary-b} we obtain the following result:

\begin{theorem}\label{theorem:iterator-a}
Given an ordered dictionary with iterator support that requires 
$S(n)$, $I(n)$, and $E(n)$
time to search, insert, and erase (respectively) an element from a
set of $n$ elements and that has a memory overhead of $cn$ for a
positive constant $c$, we can construct an equivalent data structure
that requires 
\begin{enumerate}
\item $S'(n)=O(S(n/b)+\lg b)$, $I'(n)=O(I(n/b)+b)$,
and $E'(n)=O(E(n/b)+b)$ worst-case time to search, insert,
and erase (respectively), 
\item $S''(n)=O(S(n/b)+\lg b)$,
$I''(n)=O(\frac{1}{b}I(n/b)+b)$, and
$E'(n)=O(\frac{1}{b}E(n/b)+b)$ amortized time to search,
insert, and erase (respectively), and 
\item a memory overhead of
$O(k+n/b)$ where $k$ is the number of elements of the $S$ currently 
referenced by iterators.
\end{enumerate}
\end{theorem}

\subsection{Storing chunks as linked lists}

An alternative method of implementing ordered dictionaries with
iterator support is to apply any of the schemes used in
\secref{elementary-dictionaries} but, instead of using a sorted array
for each chunk, to use a sorted singly-linked list.  In this case, an
iterator for item $x$ is implemented simply as a pointer to the list
node containing $x$ and the iterator operations are trivial to
implement.  All the other operations remain the same except that a
search within a chunk must now be a linear search as opposed to a
binary search.  Using the data structure of \thmref{elementary-b} we
obtain the following result:

\begin{theorem}
\label{theorem:iterator-b}
Given an ordered dictionary with iterator support that requires 
$S(n)$, $I(n)$, and $E(n)$
time to search, insert, and erase (respectively) an element from a
set of $n$ elements and that has a memory overhead of $cn$ for a
positive constant $c$, we can construct an equivalent data structure
that requires 
\begin{enumerate}
\item $S'(n)=O(S(n/b)+b)$, $I'(n)=O(I(n/b)+b)$,
and $E'(n)=O(E(n/b)+b)$ worst-case time to search, insert,
and erase (respectively), 
\item $S''(n)=O(S(n/b)+b)$,
$I''(n)=O(\frac{1}{b}I(n/b)+b)$, and
$E'(n)=O(\frac{1}{b}E(n/b)+b)$ amortized time to search,
insert, and erase (respectively), and 
\item a memory overhead of
$n$ pointers plus $O(n/b)$ additional storage.
\end{enumerate}
\end{theorem}

As a concrete example of the usage of Theorem
\ref{theorem:iterator-b}, let us apply it to the ordered dictionary
presented in \cite{BLMTT03}. This gives us the following corollary.

\begin{corollary}
\label{corollary:dictionary}
There exists an ordered dictionary that supports iterator-based
updates in $O(1/\varepsilon)$ worst-case time, finger searches in
$O(\lg d + 1/\varepsilon)$ worst-case time, $d$ being the distance to
the target, and requires at most $(1 + \varepsilon)n$ words of
extra storage, for any $\varepsilon > 0$ and sufficiently large
$n > n(\varepsilon)$.
\end{corollary}

\subsection{Adding random-access iterator support}

*** To be written ***

Add a balanced tree above the chunks as in \cite[Section
14.1]{CLRS01}. Use this to provide random-access iterators. The memory
overhead is only $O(n/b)$ words. The point is that the construction
works for any dictionary. That is, the idea of augmenting a data
structure is not good; this general transformation is stronger.

\section{Compaction of priority queues providing locator support}\seclabel{iterator-queues}

Any ordered dictionary can be used as a priority queue as well.
However, two methods, \Insert{} and \Decrease{}, have turned out to be
special since there exist priority queues that support all of them in
constant time.  When our previous transformations are applied to such
a priority queue, the resulting data structure is not necessarily
optimal with respect to these operations. Therefore, in this section
we describe a transformation that is suitable for priority queues.

We again use the idea of partitioning the elements into chunks.
However, this time the chunks are not sorted but the minimum element
within each chunk is stored as the first element of the chunk.  The
minimum (first) element within each chunk is referred to as the
\emph{representative} of the chunk.  Furthermore, the chunks are not
linked into a list, and there is no special relationship between the
elements of one chunk and the elements of another chunk. One special
chunk, denoted by $B$ and called the \emph{buffer}, is treated
differently.  All chunks except $B$ are stored in a priority queue
where the key of a chunk is its representative.  As before, we refer
to the data structure we are describing as ``the priority queue'' and
the data structure we are transforming as the ``the data structure.''

Either of the two methods described in \secref{iterator-dictionaries}
for adding locator support can be used.  That is, we can either store
each chunk as an array and create, for each chunk, a locator handle
list or we can store each chunk as a singly-linked list. In either
case, locators for an element can be created in constant time if the
element is the first or last element of its chunk. 

Next we consider how the various priority-queue operations can be
realized.

\Findmin{}$(\mathcal{Q})$. The minimum is stored either in buffer $B$
or the minimum representative in the data structure. A locator for
the minimum of the two elements can be returned. Since both the
relevant elements are the first elements in their respective chunks,
this operation can be accomplished in $O(1)$ worst-case time.

\Insert{}$(\mathcal{Q}, x)$. Insert $x$ into buffer $B$. If $x$ is
smaller than the current minimum of $B$, store it as the first
element; otherwise, store it somewhere convenient. If the size of $B$
is equal to $2b$, insert $B$ into the data structure and create a new
empty buffer.  In addition to a single invocation of \Insert{} for the
data structure, only a constant amount of work is done (under the
assumption that chunks know their size).

\Borrow{}$(\mathcal{Q})$. If $B$ is non-empty, extract a convenient
element and return (an locator pointing to) it.  Otherwise, $B$ is
empty, extract one chunk from the data structure and make that a new
buffer $B$. If, after this, $B$ is still empty, return an locator to
the past-the-end element and stop.  Otherwise, take a convenient
element from $B$ and return (an locator pointing to) that element.
Again one invocation of \Borrow{} for the data structure may be
necessary, but otherwise only a constant amount of work is done.

\Decrease{}$(\mathcal{Q}, p, x)$. Access the head of the chunk to
which $p$ points. Thereafter, carry out the element replacement and
make the replaced element the representative of the chunk if it is
smaller than the previous representative. If the chunk is not buffer
$B$ and if the representative of the chunk changes, invoke \Decrease{}
on the data structure. The work done in addition to one invocation of
\Decrease{} depends on the method being used.  If arrays are being
used to store chunks then the element handle contains a pointer to the
head of the chunk so finding the head takes $O(1)$ time.  If the
chunks are stored as lists, it may be necessary to traverse the entire
list to find the head of the chunk and adjust the appropriate
pointers, so the time is $O(b)$.

\Erase{}$(\mathcal{Q}, p)$. Access the head of the chunk to which
$p$ points. If the size of the chunk becomes smaller than $b$ or if
the representative of the chunk is removed, remove the chunk from the
data structure. Then merge the removed chunk with buffer $B$ to form a
new buffer. If the size of $B$ is larger than $2b$, cut off a piece of
size $2b$ from $B$, make the rest a buffer and insert the chunk cut
off into the data structure. To sum up, one invocation of \Erase{}
and \Insert{} for $Q$ may be necessary, but the other work done is
proportional to $b$.

The above discussion is summarized in the following two theorems, that
come from applying each of the two methods in
\secref{iterator-dictionaries}, respectively.  Let
$\mathcal{D}$ be a data structure and let $n$ denote the number of
elements stored in $\mathcal{D}$ prior to each operation.  We use the
notation that $\mathcal{D}$ executes \Findmin{}, \Insert{},
\Borrow{}, \Decrease{}, and \Erase{} in 
$F^{\mathcal{D}}(n)$,
$I^{\mathcal{D}}(n)$, 
$B^{\mathcal{D}}(n)$, 
$D^{\mathcal{D}}(n)$, and
$E^{\mathcal{D}}(n)$ time, respectively. To get the claimed result, we
should just make $b$ large enough to get the amount of extra storage
used down from $O(n)$ to $\varepsilon n$.

\begin{theorem}
\label{theorem:queue-a}
A priority queue $\mathcal{D}$ that uses $O(n)$ words of extra storage
can be transformed into an
equivalent data structure that, when referenced by at most $k$
iterators,
uses
$O(k)+\varepsilon n$ words of extra storage, for an arbitrary
$\varepsilon > 0$ and sufficiently large $n > n(\varepsilon)$, and performs 
\Findmin{}, \Insert{},
\Borrow{}, \Decrease{}, and \Erase{} in
$F^{\mathcal{D}}(\varepsilon n) + O(1)$,
$I^{\mathcal{D}}(\varepsilon n) + O(1)$,
$B^{\mathcal{D}}(\varepsilon n) + O(1)$,
$D^{\mathcal{D}}(\varepsilon n) + O(1/\varepsilon)$,
$E^{\mathcal{D}}(\varepsilon n) + I^{\mathcal{D}}(\varepsilon n) + O(1/\varepsilon)$ time, respectively.
\end{theorem}



\begin{theorem}
\label{theorem:queue-b}
A priority queue $\mathcal{D}$ that uses $O(n)$ words of extra storage
can be transformed into a equivalent data structure that
uses
$(1+\varepsilon)n$ words of extra storage, for an arbitrary
$\varepsilon > 0$ and sufficiently large $n > n(\varepsilon)$, and performs 
\Findmin{}, \Insert{},
\Borrow{}, \Decrease{}, and \Erase{} in
$F^{\mathcal{D}}(\varepsilon n) + O(1)$,
$I^{\mathcal{D}}(\varepsilon n) + O(1)$,
$B^{\mathcal{D}}(\varepsilon n) + O(1)$,
$D^{\mathcal{D}}(\varepsilon n) + O(1/\varepsilon)$,
$E^{\mathcal{D}}(\varepsilon n) + I^{\mathcal{D}}(\varepsilon n) + O(1/\varepsilon)$ time, respectively.
\end{theorem}

\noindent\textbf{Is it possible to get amortized bounds like those we
get for dictionaries, where only one out of every $b$ operations
results in an operation on the underlying priority queue?}

To illustrate the power of Theorem \ref{theorem:queue-b}, let us apply
it to the priority queue described in \cite{EJK06}. As an outcome we
get the following result:

\begin{corollary}
\label{corollary:queue}
There exists a priority queue that supports \Findmin{} and \Insert{}
in $O(1)$ worst-case time, \Decrease{} in $O(1/\varepsilon)$
worst-case time, \Erase{} in $O(\lg n + 1/\varepsilon)$ worst-case
time including at most $\lg n + 3\lg\lg n + O(1/\varepsilon)$ element
comparisons, and requires at most $(1 + \varepsilon)n$ words of extra
storage, for any $\varepsilon > 0$ and sufficiently large $n > n(\varepsilon)$.
\end{corollary}

\section{Discussion}\seclabel{discussion}

We showed that many data structures of size $O(n)$ can be put on a
diet, so that the memory overhead is reduced to $O(n/\lg n)$,
$\varepsilon n$, or $(1 + \varepsilon)n$ words for any $\varepsilon >0$
and sufficiently large $n > n(\varepsilon)$. This diet does not change
the running times of the operations on the data structures except by a
small constant factor independent of $\varepsilon$ and/or an additive
term of $O(1/\varepsilon)$.

We considered ordered dictionaries and single-ended priority queues,
but the compaction technique can be used for slimming down other data
structures as well. Other applications, where the technique is known
to work, include positional sequences (doubly-linked lists), unordered
dictionaries (e.g.~hash tables relying on linear hashing
\cite{Lit80}), and double-ended priority queues.  The details follow
closely the guidelines given in this paper, so we leave them
to the interested reader.

\comment{
\begin{theorem}
A positional sequence, which provides location-based updates and
bidirectional iterators, can be transformed into an equivalent data
structure that uses $(1 + \varepsilon)n$ words of memory, for any
$\varepsilon > 0$ and sufficiently large $n > n(\varepsilon)$,
excluding the space used by the $n$ elements stored. This
transformation only slows down the running time of the supported
operations by an additive term of $O(1/\varepsilon)$.
\end{theorem}

\begin{theorem}
An unordered dictionary, which relies on chaining and provides member
searches and updates, can be transformed into an equivalent data
structure that uses $(1 + \varepsilon)n$ words of extra storage, for
any $\varepsilon > 0$ and sufficiently large $n > n(\varepsilon)$.
This transformation slows down the running time of the supported
operations by a \emph{multiplicative factor} of $O(1/\varepsilon)$,
except that the slowdown is an additive term of $O(1/\varepsilon)$ for
\Insert{} when a multiset is stored and for location-based \Erase{}.
\end{theorem}

\begin{theorem}
A doubly-ended priority queue, which uses $O(n)$ words of extra
storage and provides methods \Findmin{}, \Findmax{}, \Insert{},
\Borrow{}, and \Erase{}, can be transformed into an equivalent data
structure that uses $(1 + \varepsilon)n$ words of extra storage, for
any $\varepsilon > 0$ and sufficiently large $n > n(\varepsilon)$.
This transformation only slows down the running time of \Erase{} by an
additive term of $O(1/\varepsilon)$ and that of the other operations
by an additive term of $O(1)$.
\end{theorem}
}

The practical value of the compaction technique is that users of data
structures need not worry about the memory overhead associated with
the data structure they choose.  If it becomes a problem, they can
simply apply the technique and the problem is solved.  The theoretical
value of the compaction technique is that designers of data structures
need no longer make unnecessarily complicated algorithms and
representations for the sake of reducing the memory overhead.  They
can design data structures that have
fast running times and useful properties, and put less emphasis on the
memory overhead of the data structures.

%\section*{Acknowledgement}
%We thank Nicolai Esbensen for pointing out that the compaction technique
%can also be used for slimming down a doubly-linked list.

\bibliography{diet}
\bibliographystyle{DIKU}

\end{document}
