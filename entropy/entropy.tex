\documentclass[charterfonts,lotsofwhite]{patmorin}
\usepackage{amsopn}
\input{pat}

\newcommand{\defequals}{\stackrel{\mathrm{def}}{=}}
\DeclareMathOperator{\depth}{depth}

\title{\MakeUppercase{Distribution-Sensitive Point-in-Polygon Testing}}
\author{ULB Algorithms \and John Iacono \and Pat Morin}
\date{}

\begin{document}
\maketitle

\section{Introduction}


For a set $X$, itself containing sets, we denote the union of its
elements by $\cup X=\bigcup_{x\in X} x$.

For a distribution $D$ and an event $X$, we denote by $D_{|X}$ the
distribution $D$ conditioned on $X$.  That is, the distribution where 
the probability of an even $Y$ is
$\Pr(Y|X)=\Pr(Y\cap X)/\Pr(X)$.

A \emph{decision problem} $\mathcal{P}$ over a domain $\mathcal{D}$ is
a function $\mathcal{P}:\mathcal{D}\mapsto \{0,1\}$.

A \emph{decision tree} is a full binary tree\footnote{A full binary
tree is a rooted ordered binary tree in which each non-leaf node has
exactly two children.} in which each internal node $n$ is labelled
with a predicate $P_n(\cdot)$ and for which each leaf $\ell$ is
labelled with a boolean value $d(\ell)\in\{0,1\}$. The \emph{search
path} of an input $p$ in a comparison tree $T$ starts at the root of
$T$ and, at each internal node $n$, proceeds to the left child of $n$
if $P_n(p)$ is false and proceeds to the right child of $n$ otherwise.
We denote by $T(p)$ the label of the final (leaf) node in the search
path for $p$.  We say that the decision tree $T$ solves a decision
problem $\mathcal{P}$ over the domain $\mathcal{D}$ if, for every $p\in
\mathcal{D}$, $\mathcal{P}(p)=T(p)$.

When there is a probability measure $D$ over $\mathcal{D}$, we define
the \emph{expected search time} of a decision tree $T$ as the expected
length of the search path for $p$ when $p$ is drawn at random from
$\mathcal{D}$ according to the distribution $D$.  Note that, for each
leaf $\ell$ of $T$ there is a subset of $r(\ell)\subseteq \mathcal{D}$
such that the search path for any $p\in r(\ell)$ ends at $\ell$.
Thus, the expected search time of $T$ can be written as
\[
     \mu_D(T) = \sum_{\ell\in L(T)} \Pr(r(\ell))\times \depth(\ell)
	\enspace ,
\]
where $L(T)$ denotes the leaves of $T$ and $\depth(\ell)$ denotes the
length of the path from the root of $T$ to $\ell$.


\section{Collections of Intervals}

Before considering point-in-polygon testing we warm up by studying a
related one-dimensional problem on a set of intervals.  Our motivation
for studying this problem is that it gives intuition into how to
characterize the complexity of point-in-polygon testing under random
distributions.

Let $I=\{[a_i,b_i]: i=1,\ldots,n\}$ be a set of $n$ disjoint closed
real intervals, with $b_i < a_{i+1}$ for all $1\le i\le n$.  We are
interested in the problem of constructing a decision tree for $I$ that
can answer the decision problem: Given $p\in\R$, is $p\in \cup I$?
The type of decision trees we consider are \emph{comparison trees}.
These are decision trees in which each internal node $v$ is labelled
with a comparison of the form $P_v(x) = [x > n(v)]$ where $n(v)$ is
some real number called the \emph{label} of $v$.\footnote{Here and
throughout, we use Iverson's notation \cite{kXX} where, for a
predicate $X$, $[X]$ evaluates to 1 if $X$ is true and 0 otherwise.}
In particular, we are interested in the case when the query point $p$
is drawn according to some probability distribution $D$.   Throughout
the remainder of this section, all references to probabilities are
implicitly with reference to $D$ and the query domain is the support
of $D$.  We call such a decision tree a membership tester for $(I,D)$.

To study the expected search time of comparison trees for this problem
we need to define a slightly different set of intervals than $I$.  Let
$J$ be the set of intervals obtained after the following two
operations:

\begin{enumerate}

\item Remove from $I$ all intervals $\ell$ such that $\Pr(\ell) = 0$.

\item Repeat the following operation as many times as possible:  If
$\Pr([b_i,a_{i+1}])=0$ then replace the two intervals $(a_i,b_i)$ and
$(a_{i+1},b_{i+1})$ with the single interval $(a_i,b_{i+1})$
\end{enumerate}

\begin{lem}\lemlabel{intervals}
Let $\overline{J}$ be the set of maximal open intervals in
$\R\setminus\cup J$ and let $S=J\cup \overline{J}$.  Then the
expected search time of any membership tester $T$ for $(I,D)$ is at least
\[
    \mu_D(T) \ge H(I,D) \defequals 
	-\sum_{x\in S} \Pr(x)\log \Pr(x) \enspace .
\]
\end{lem}

\begin{proof}
Consider two distinct intervals $i,j\in S$ and suppose $p\in i$ and
$q\in j$.  Note that the search paths for $p$ and $q$ must finish in
different leaves of $T$ since, otherwise, there is some point
$r\in[p,q]$ that is incorrectly classified by $T$, i.e., such
that $T(p)\neq [p\in\cup I]$.  Now, for every interval $i\in S$,
select a point $p_i\in i$ such that the length of the search path for
$p$ is minimum.   Using the standard encoding of paths in a binary
search tree, this gives a code for the intervals of $S$ where the
length of the codeword for $i$ is the depth of $p_i$ in $T$.  In this
way we obtain

\[
   \mu_D(T)
    = \sum_{\ell\in L(T)} \Pr(\ell)\depth(\ell)
   \ge \sum_{i \in S} \Pr(i)\depth(p_i)
   \ge H(I,D)
   \enspace , 
\]
where the second inequality follows from Shannon's source coding
theorem \cite{X}.
\end{proof}

Though not the focus of the current paper, it is worth noting that the
lower bound in \lemref{intervals} can be matched to within an additive
constant by storing the intervals of $J$ and $\overline{J}$ at the
leaves of a biased binary search tree \cite{kXX,mXX}.  In particular,
for every $I$ and $D$ there exists a membership tester for $(I,D)$
whose expected search time is $H(I,D)+O(1)$.

\section{Point In Convex Polygon Testing}

Let $P$ be a convex $n$-gon whose boundary is denoted by $\partial P$
and consider a probability measure $D$ over $\R^2$.  In this section
we are interested in the problem of testing whether a query point $p$,
drawn according to $D$, is contained in $P$.   

In particular, we are interested in algorithms that can be described
as \emph{linear decision trees}.  These are decision trees such that
each internal node $v$ contains a linear inequality $P_v(x,y)=[ax+by
\ge c]$.  Again, we require that, for every $p$ in the support of $D$,
the leaf reached in the search path for $p$ is labelled with a 1 if
and only if $p\in P$.  Geometrically, each internal node of $T$ is
labelled with a directed line and the decision to go to the left or
right child depends on whether p is to the left or right (or on) this
line.  

\subsection{Nice Distributions}

In this section we assume throughout that the support of $D$ is a
convex set that includes $P$.  The following lemma is crucial in
proving the optimality of our data structure:

\begin{lem}\lemlabel{lower-bound-a}
Let $C=P_1\setminus P_2$ where $P_1$ and $P_2$ are closed compact
subsets of $\R^2$ with $P_2\subseteq P_1$.  Let $C_1,\ldots,C_k$ be
the connected components of $C\setminus\partial P$.  Then for any
linear decision tree $T$
\[
	\mu_{D_{|C}}(T) \ge \frac{1}{2}\times H(P,D_{|C},C) 
		= -\frac{1}{2}
			\times \sum_{i=1}^k \Pr(C_i|C)\log \Pr(C_i|C)
		\enspace .
\]
\end{lem}

\begin{proof} 
We first prove a lower bound for  a slightly different type of
decision tree.  A \emph{chord decision tree} for $C$ is defined in
exactly the same manner as a linear decision tree except that the
internal nodes of the tree are labelled with line segments (chords)
whose endpoints are on the bounary of $C$ and whose interiors are
disjoint from the boundary of $C$.  A chord partitions $C$ into two
parts and each of these corresponds to one of the children of $C$.
The following claim relates chord decision trees to linear decision
trees and follows easily from the fact that any line intersects $C$ in
at most 2 connected components.

\begin{clm}\clmlabel{two-approx}
Given a linear decision tree $T$ for $(P,D_{|C})$ with expected cost $k$,
there exists a chord decision tree $T_C$ for $(P,D_{|C})$ with expected cost
at most $2k$. 
\end{clm}

Let $T$ be any linear decision tree and let $T_C$ be the chord
decision tree guaranteed by \clmref{two-approx}.  As in the proof of
\lemref{intervals}, the search paths for a point $p\in C_i$ and a
point $q\in C_j$ with $i\neq j$ must finish at different leaves of the
chord decision tree.  Thus, we can assign a code to $C_1,\ldots,C_k$
such that the length of the codeword for $C_i$ is the length of the
shortest path from the root of $T$ to a leaf corresponding to some
point in $C_i$.  Then we again obtain
\[
  \mu_{D_{|C}}(T_C)
	= \sum_{\ell\in L(T_C)} \Pr(\ell|C)\depth(\ell) 
	\ge -\sum_{i=1}^k \Pr(C_i|C)\log\Pr(C_i|C) \enspace .
\]
Then, from \clmref{two-approx}, $\mu_{D_{|C}}(T) \ge
\frac{1}{2}\mu_{D_{|C}}(T_C)$, completing the proof.
\end{proof}

Next we describe a data structure that uses \lemref{lower-bound-a} to
guide its construction. In particular, our data structure consists of
a hierarchy of progressively finer approximations $A_0,A_1,\ldots,A_k$
of $\partial P$ with $\R=A_0\supseteq A_{1}\supseteq\cdots\supseteq
A_k$.  These approximations are designed so that:

\begin{enumerate}

\item for any query point $p$ we can, in $O(i)$ time, find the largest
value $i$ such that $p\in A_i$ and simultaneously determine if $p\in
P$, and 

\item if we condition on $A_i$, the expected cost of any decision tree
for $(P,D_{|A_i})$ is $\Omega(i)$.
\end{enumerate}

To see that this is sufficient, observe that the first point above
shows that the query time of our data structure is 
\[
     \mu_D(T) = \sum_{i=0}^k O(i)(\Pr(A_i)-\Pr(A_{i+1})) =
	\sum_{i=1}^k O(1)\Pr(A_i) \enspace .
\]




\comment{
To see that we also have a matching lower bound, we use the
\emph{little-birdie principle}.  That is, we give the algorithm extra
information that it may (or may not) use to speed up its search.
Suppose that we allow a stronger model in which an algorithm
precomputes a collection $T_0,\ldots,T_m$ of linear decision trees.
Now, suppose we draw the point $p$ according to $D$.  Let
$i=\max\{j:p\in A_j\}$.  Now, we report to the algorithm some value
$j$ chosen uniformly at random from $\{i,\ldots,k\}$.  Using this
value $j$, the algorithm then selects one of the trees
$T_0,\ldots,T_m$ that it uses to determine if $p\in P$.


 and
we tell the algorithm that $p\in A_i$ for some value $i$.  Point 2,
above, says that, even given this information, the expected time to
determine if $p\in P$ is $\Omega(i)$.

while the query time of any data structure is at least
\[
     \sum_{i=1}^k \Omega(i)(\Pr(A_i)-\Pr(A_{i-1}))  \enspace .
\]
}


\section{Conclusion}

\end{document}
