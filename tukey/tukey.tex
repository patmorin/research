\documentclass[charterfonts,lotsofwhite]{patmorin}
\usepackage[noend]{algorithmic}
\usepackage{amsmath,amsthm,amsopn}
\usepackage{graphicx}
%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% FIXME: We must mention Matousek's $O(k^{d+1}*LP(n,d))$ time algorithm
% for LP with violations. 
% Actually, our FPT algorithm is a stupider version of the same 
% algorithm.
%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
\input{pat}
%\newcommand{\td}{\mathrm{depth}}
%\newcommand{\lp}{\mathrm{LP}}
\DeclareMathOperator{\td}{depth}
\DeclareMathOperator{\lp}{LP}

\title{\MakeUppercase{Output-Sensitive Algorithms for Tukey Depth and
        Related Problems}\thanks{This research was partly funded by
	the NSERC Canada.}}
\author{David Bremner \\ University of New Brunswick \and
        Dan Chen \\ University of New Brunswick \and
	John Iacono \\ Polytechnic University \and
	Stefan Langerman \\ Universit\'e Libre de Bruxelles \and
	Pat Morin \\ Carleton University}
\date{}
\begin{document}
\maketitle

\begin{abstract}
The \emph{Tukey depth} (Tukey 1975) of a point $p$ with respect to a
finite set $S$ of points is the minimum number of elements of $S$
contained in any closed halfspace that contains $p$.  Algorithms for
computing the Tukey depth of a point in various dimensions are
considered. The running times of these algorithms depend on the value
of the output, making them suited to situations, such as outlier
removal, where the value of the output is typically small.
\end{abstract}

\noindent\textbf{Keywords:}
Tukey depth, halfspace depth, algorithms, computational statistics, 
computational geometry, fixed-parameter tractability

\section{Introduction}

Let $S$ be a set of $n$ points in $\R^d$.
The \emph{Tukey depth}, or \emph{halfspace depth} of a point $p\in\R^d$ with
respect to $S$ can be defined in several equivalent ways \cite{t75}:
\begin{eqnarray}
\td(p,S) & = & \min\{ |h\cap S| :
	             \mbox{$h$ is a closed halfspace containing $p$} \} 
                       \eqlabel{tuk-orig} \\ 
            & = & \min\{ |h\cap S| :
                      \mbox{$h$ is a closed halfspace 
                            with $p$ on its boundary} \} 
                        \eqlabel{tuk-boundary} \\ 
            & = & \min\{ |S'| :
                      \mbox{$p$ is outside the convex hull of 
                           $S\setminus S'$} \}
                      \eqlabel{tuk-hull}
\end{eqnarray}

A point of maximum Tukey depth serves as a $d$-dimensional
generalization of the (1-dimensional) median that has many nice
statistical properties including being robust against outliers,
invariant under affine transformations, and monotone.  The contours of
the Tukey depth function\footnote{The \emph{$\ell$-contour} of the
tukey depth function is defined as $\Gamma_\ell(S)=\{p\in\R^d :
\td(p,S)\ge \ell\}$.}  are generalizations of 1-dimensional
percentiles that also have many nice properties including convexity,
robustness, and monotonicity \cite{rr98,rr96,s90}.  Algorithms for
computing a point $p\in\R^d$ of maximum Tukey depth have a rich
history \cite{m91,ls03,c04} that has recently culminated in Chan's
$O(n\log n + n^{d-1})$ expected time algorithm.  

In this paper we consider the simpler, but still very difficult,
problem of computing the Tukey depth of a given point $p$ with respect
to a set $S$. If the dimension $d$ of the problem is part of the
input, then this problem is NP-hard \cite{jp78}, and is even APX-hard
\cite{ak95}.  The current paper presents algorithms whose running
times are dependent on the dimension $d$ and the value, $k$, of the
output.  In some applications, such as outlier removal, the goal is to
identify quickly the data points of small depth (so they can be
removed).  Our algorithms are particularly well-suited to such
applications since they run quickly when the depth of $p$ is small.
Specifically, we present the following results:

\begin{enumerate}
\item A simple $O(n + k\log k)$ time algorithm to compute the Tukey
depth of a point in $\R^2$
(\secref{2d}).  The most complicated data structure used in this
algorithm is a binary heap.

\item An $O(n + (n-k)\log(n-k))$ time algorithm to find the largest
clique in an interval graph, where $k$ is the size of the clique found
(\secref{max-clique}).  This problem is related to the Tukey depth
problem in $\R^2$.

\item An $O(n\log n + k^2\log n)$ time algorithm to compute the Tukey
depth of a point in $\R^3$ and an $O(n + k^{11/4}n^{1/4}\log^{O(1)}n)$
time algorithm to compute the Tukey depth of a point in $\R^4$
(\secref{3-4-d}).  These algorithms rely on results of Chan on linear
programming with violated constraints \cite{c05} which in turn rely on
sophisticated range searching data structures \cite{m92,r99} and/or
dynamic convex hull data structures \cite{bj02}.

\item A simple $O(d^k \lp(n,d-1))$ time algorithm to compute the Tukey
depth of a point in $\R^d$, where $\lp(n,d)$ denotes the time required
to determine the feasibility of a linear program having $n$
constraints and $d$ variables (\secref{d-d}).  Not surprisingly, this
algorithm is also based on linear programming with violated
constraints and is obtained by presenting a fixed-parameter tractable
algorithm for a parameterization of the NP-hard
\textsc{MaximumFeasibleSubsystem} problem.
\end{enumerate}

For the remainder of this paper we use the following notations: For
points $p,q\in\mathbb{R}^d$, $p_i$ denotes the $i$th coordinate of
$p$, $\|p\|=(\sum_{i=1}^d p_i^2)^{1/2}$, and $p\cdot
q=\sum_{i=1}^d p_iq_i$.  The unit sphere in $\R^{d+1}$ is denoted by
$\Sp^d =\{ p\in\R^{d+1} : \|p\|=1\}$. The top side of this sphere is
denoted by $\Sp^d_+ =\{ p\in\Sp^{d} : p_{d+1} > 0 \}$, the bottom
side is denoted by $\Sp^d_- =\{ p\in\Sp^{d} : p_{d+1} < 0 \}$ and the
equator is denoted by $\Sp^d_0 =\{ p\in\Sp^{d} : p_{d+1} = 0 \}$ .

\section{An Algorithm for Points in $\R^2$}
\seclabel{2d}

In this section we give a simple $O(n + k\log k)$ time algorithm to
compute the Tukey depth of a point $p\in\R^2$ with respect to a set
$S$ of $n$ points in $\R^2$.  We first note that an $O(n\log n)$ time
\emph{sort-and-scan} algorithm is easily obtained by sorting the
points of $S$ radially about $p$ and then scanning the resulting
sorted list using two pointers \cite{m91}.  The main idea behind our
algorithm to is to reduce the problem to a \emph{kernel} of size
$O(k)$ on which we can apply this sort-and-scan algorithm.

The algorithm begins by partitioning $\R^2$ into 4 quadrants around
$p$ that, in counterclockwise order, we denote by $Q_0,\ldots,Q_3$.
The algorithm then simultaneously begins computing the 4 quantities
$\td_0(p,S),\ldots,\td_3(p,S)$ where 
\begin{equation}
     \td_i(p,S) = \min\{|h\cap S| : \mbox{$h$ is a closed halfspace containing $Q_i$} \} \enspace . \eqlabel{four-min}
\end{equation}
Clearly, $\td(p,S) = \min\{\td_i(p,S): 0\le i \le 3 \}$ since any
closed halfspace containing $p$ contains at least one of the four quadrants.
In the remainder of this section we will describe how to compute
$k_i=\td_i(p,S)$ in $O(n + k_i\log k_i)$ time.  Since the
computation can stop once $\td_i(p,S)$ has been computed for
the index $i$ that minimizes \eqref{four-min}, running the computation
of $k_0,\ldots,k_3$ in parallel yields an $O(n +
k\log k)$ time algorithm, where $k=\td(p,S)$.

\begin{figure}
\begin{center} \includegraphics{quadrants} \end{center}
\caption{Computing the quantity $\td_1(p,S)$.}
\figlabel{quadrants}
\end{figure}


Let $S_i=S\cap Q_i$. To compute $\td_i(p,S)$ we create two binary
heaps $H_{i-1}$ and $H_{i+1}$ that store the elements of $S_{i-1}$,
respectively $S_{i+1}$, in clockwise, respectively, counterclockwise,
order around $p$.\footnote{Here and in the remainder of this section
$S_i$ is treated implictly as $S_{i\bmod 4}$.} 
Creating these two heaps takes $O(n)$ time using the
standard bottom-up algorithm to construct a binary heap
\cite[Chapter~6]{clrs01}.
Next we extract elements one at a time from each of $H_{i-1}$ and
$H_{i+1}$ until either (a)~one of the heaps is empty or (b)~we extract
two elements $q$ from $H_{i-1}$ and $r$ from $H_{i+1}$ such that the
angle $\angle qpr > \pi$.  Suppose we have extracted $\ell$ elements
each from $H_{i-1}$ and $H_{i+1}$ when this occurs.  
Then, any closed halfspace containing $Q_i$ contains $\ell-1$
elements extracted from $H_{i-1}$ or it contains $\ell-1$ elements
extracted from $H_{i+1}$.  Therefore,
\[  
  |S_i| + \ell - 1 \le \td_i(p,S) \enspace .
\]

On the other hand, the closed halfspace containing $Q_i$ and bounded by the line through
$p$ and the last element extracted from $H_{i-1}$ contains at most
$|S_i|+2\ell -1$ points of $S$, so
\[
  \td_i(p,S) \le |S_i| + 2\ell-1  \enspace .
\]

Next, we continue to extract as many elements as possible from each of
$H_{i-1}$ and $H_{i+1}$ up to a maximum of an additional $\ell-1$
elements each. The total time required to extract these at most
$4\ell-2$ elements from the two heaps is $O(\ell\log n)$.  By sorting and
scanning all the elements extracted from the heaps plus the elements of
$S_i$ we can then compute $\td_i(p,S)$ in an additional
\[
     O((|S_i|+\ell)\log n) = O(k_i\log n)
\] 
time.  This yields an a total running time of 
\[  
   O(n + k_i\log n) = O(n + k_i\log k_i) \enspace ,
   \footnote{This equation follows from the analysis of two cases. If
$k_i \le n/\log n$ then $k_i\log k_i \le n$.  On the other hand, if $k_i >
n/\log n$ then $\log k_i > \log n - \log\log n$ so $k_i\log n <
2k_i\log k_i$ for all $n\ge 16$.}
\]
as required.  This completes the proof of:

\begin{thm}\thmlabel{2-d}
The Tukey depth of a point $p$ with respect to a set $S$ of $n$ points
in $\R^2$ can be computed in $O(n + k\log k)$ time, where
$k$ is the value of the output.
\end{thm}

\section{An Algorithm for \textsc{Max-Clique} in Interval Graphs}
\seclabel{max-clique}

The problem of computing Tukey depth in $\R^2$ can be viewed as a
problem on a set of circular arcs.  By the second definition of Tukey
depth (\Eqref{tuk-boundary}), computing the Tukey depth of $p$ is
equivalent to finding a unit normal vector $v$ such that the halfspace
with $p$ on its boundary and having inner normal $v$ contains as few
points of $S$ as possible.  Note that the set of unit normals in
$\R^2$ is homeomorphic to the unit circle $\Sp^1$ and that each point
$q\in S$ defines an open circular arc of $\Sp^1$ such that all choices
of $v$ in this circular arc yield a closed halfspace with $p$ on its
boundary that does not contain $q$.  In fact, all the open circular
arcs obtained this way are half-circles.  Thus, the Tukey depth problem
reduces to the problem of finding a vector $v$ that is contained in
the largest number of half-circles. Although it is not immediately
apparent, the correctness of the algorithm
in \thmref{2-d} relies crucially on the fact that the arcs of $\Sp^1$
defined by the Tukey depth problem are all half-circles and not arcs
of arbitrary length.

An obvious generalization of the Tukey depth problem is that of, given
a set of $n$ circular arcs of $\Sp^1$, finding a point $p\in\Sp^1$
contained in the largest number of arcs.  Note that, in this
generalization, the arcs can have arbitrary and different lengths.
Like the Tukey depth problem, this problem is easily solved in
$O(n\log n)$ time by the sort-and-scan algorithm.  Unfortunately, it
is not possible to obtain an algorithm whose running time depends on
the number $k$ of arcs containing $p$ or even on the number $(n-k)$ of
arcs not containing $p$. More precisely, the decision problem of
testing whether a set of $n$ circular arcs covers $\Sp^1$ has an
$\Omega(n\log n)$ lower-bound \cite{b-o83}.  This problem is
equivalent, by taking the complement of each arc, to the problem of
finding the point contained in the maximum number of arcs.  In
particular, the original set of arcs do not cover $\Sp^1$ if and only
if there is a point $p$ contained in every complementary arc.

Since we can not hope to find an output-sensitive algorithm for
circular arcs of $\Sp^1$, we settle for the next best thing.  Let $I$
be a set of real intervals.  Here we describe an $O(n+(n-k)\log
(n-k))$ time algorithm to find a point $p\in\R$ that is contained in
the largest number of intervals in $I$.  Here $k$ is the number of
intervals in $I$ that contain $p$.  

Let $p_1,\ldots,p_{2n}$ denote the $2n$ endpoints of the intervals in
$I$, in increasing order. For convenience we use the convention that
$p_i = -\infty$ for $i\le 0$ and $p_i=+\infty$ for $i > 2n$.  Note
that the $p_1,\ldots,p_{2n}$ notation is for convenience only and the
algorithm we describe does not require that the endpoints be given in
sorted order, nor will it sort them into this order.  Together, the
following two observations imply that all the points contained in many
intervals are clustered together. (Refer to \figref{deep-intervals}.)

\begin{figure}
\begin{center}
\includegraphics{deep-intervals}
\end{center}
\caption{An illustration of \lemref{close} and \lemref{far}.  The point $q\in
[p_6,p_7]$ is contained in 6 intervals.  Therefore, by \lemref{close}, the point $q'\in
[p_{10},p_{11}]$ is contained in at least $6-4 = 2$ intervals and by
\lemref{far} $q'$ is contained in at most $14-6-4=4$ intervals.}
\figlabel{deep-intervals}
\end{figure}

\begin{lem}\lemlabel{close}
Let $q\in[p_i,p_{i+1}]$ be a point contained in $k$ intervals of $I$.
Then, for any $0\le r\le n$, every point $q'\in[p_{i-r},p_{i+r+1}]$ is
contained in at least $k-r$ intervals of $I$.
\end{lem}

\begin{proof}
Without loss of generality, assume that $q'\in[q,p_{i+r+1}]$.  There are
at most $r$ endpoints of intervals in $I$ contained in the interval
$[q,q']$.  Therefore there are at most $r$ intervals that contain $q$
but not $q'$.
\end{proof}

\begin{lem}\lemlabel{far}
Let $q\in[p_i,p_{i+1}]$ be a point contained in $k$ intervals of $I$.
Then, for any $n-k \le r\le n$, every point $q'\notin[p_{i-r},p_{i+r+1}]$ 
is contained in at most $2n-k-r$ arcs of $C$.
\end{lem}

\begin{proof}
Without loss of generality, assume that $q'> p_{i+r+1}$.  Then, as we
walk from $q$ to $q'$ we encounter at least $r$ endpoints of intervals
in $I$.  At most $n-k$ of these endpoints are left endpoints of
intervals and at least $r-(n-k)$ of these are right endpoints.  Thus,
the number of intervals that contain $q'$ is at most
\[
      k + (n-k) - (r-(n-k)) = 2n - k - r \enspace ,
\]
as required.
\end{proof}

We first explain our algorithm at a high level in an idealized way in
which we deliberately ignore many details implementation details that
are discussed later.  Refer to \figref{interval-alg}. Suppose we are
given a value $k$ and only wish to find a value $p\in\R$ contained in
at least $k$ intervals of $I$.  We begin by taking a regular sample
$s_1,\ldots,s_{2t}$ of $p_1,\ldots,p_{2n}$ so that any interval
$[s_i,s_{i+1}]$ between two consecutive sample points contains at most
$n/t$ points of $p_1,\ldots,p_{2n}$.  We then compute, for each sample
point $s_i$ the number of intervals in $I$ that contain $s_i$.  By
\lemref{close}, if there exists any point $p\in \R$ contained in $k$
intervals of $I$ then the two sample points $s_j$ and $s_{j+1}$ on
either side of $p$ are \emph{high-depth samples} that are each
contained in at least $k-n/t$ intervals of $I$.  Furthermore, by
\lemref{far}, the only high-depth samples are contained in the
interval $[p_{i-r},p_{i+r}]$ for $r=2(n-k)+n/t$.  

\begin{figure}
\begin{center}\includegraphics{interval-alg2}\end{center}
\caption{An illustration of the algorithm for interval graphs
using sampling at regular intervals (above) and using random samples
(below).  
The rows of dots show
the endpoints of the intervals with samples drawn as circles labelled with the number
of intervals that contain them.  High-depth samples are indicated as
filled circles.}
\figlabel{interval-alg}
\end{figure}

If we choose $t=\sqrt{n}$ then $r=O(n-k+\sqrt{n})$.  Thus, by
computing an interval $[p_a,p_b]$ that contains all high-depth samples
we can find the point $p$ contained in the largest number of
intervals of $C$ by applying the standard sort-and-scan algorithm
on the $O(n-k+\sqrt{n})$ endpoints of the intervals of $C$ that fall
in the interval $[p_a,p_b]$.  The running time of the
sort-and-scan algorithm is $O(m\log m)$ where $m$ is the number of
points to be scanned.  In this case $m=O(n-k+\sqrt{n})$ for a running
time of 
\[
   O((n-k+\sqrt{n})\log(n-k+\sqrt n)) = O(n + (n-k)\log (n-k)) \enspace ,
\]
as required.  Note that the value of $k$ has very little effect on the
execution of the algorithm other than to bound the number of endpoints
in the interval $[p_a,p_b]$ on which the sort-and-scan algorithm is
run.
In implementating the above ideas, several complications arise:

\begin{enumerate}

\item The value of $k$ is not known in advance.  However, we do not
need the exactly value of $k$ and the value of $k$ can be estimated
using \lemref{close}.  In particular, since the number of endpoints
between any two consecutive samples is at most at most $\sqrt{n}$ the
number of intervals containing $s_i$ estimates, to within an additive
error of $\sqrt{n}$, the number of intervals containing any point $q\in
[s_{i-1},s_{i+1}]$. In particular, we can estimate the value of $k$ by
computing, for each sample point $s_i$, the number of intervals of $I$
that contain $s_i$ (see Issue~3, below) and use the maximum of these
values as an estimate for $k$.

\item We can not obtain a perfectly regular sample
$s_1,\ldots,s_{2\sqrt{n}}$ of $p_1,\ldots,p_{2n}$ in $O(n)$ time.
However, we do not require a perfectly regular sample.  By taking a
random sample of size $c\sqrt{n}\log n$ for an appropriate constant
$c$ we obtain a set of samples $s_1,\ldots,s_{c\sqrt{n}\log n}$ such
that, with probability $1-n^{-\Omega(c)}$, no interval
$[s_{i},s_{i+1}]$ contains more than $\sqrt{n}$ endpoints of intervals
of $I$ \cite{m98}.  Such a sample still guarantees that the number of
intervals containing a point $q\in [s_i,s_{i+1}]$ is estimated, to
within an additive error of $\sqrt{n}$, by the number of intervals
containing $s_i$.

\item We can not compute, in $O(n)$ time, for each sample point $s_i$,
the number of intervals of $I$ that contain $s_i$.  However, random
sampling helps again here.  Let $d(s_i)$ denote the number of elements
of $I$ that contain $s_i$. By taking a random sample $I'\subseteq I$,
$|I'|=c\sqrt{n}$ we can determine for each $s_i$ a
number $d_i$ such that, with probability $1-n^{-\Omega(c)}$
\[
        d(s_i) - O(n^{4/5})\le d_i \le
           d(s_i) + O(n^{4/5}) \enspace .
\]
By storing the $\sqrt{n}$ elements of $I'$ in an interval tree \cite{ps85} and
then querying this interval tree with the $c\sqrt{n}\log n$ sample
elements the numbers $d_1,\ldots,d_{c\sqrt{n}\log n}$ can be computed
in $O(\sqrt{n}\log^2 n)$
time.  
\end{enumerate}

Note that each of the above steps can be accomplished in $o(n)$ time,
determining the endpoints contained in $[p_a,p_b]$ can be done in
$O(n)$ time, and the final sort-and-scan step takes $O(n + (n-k)\log
(n-k))$ time.  The correctness of the resulting output depends on the
success of the random sampling steps described in points 2 and 3,
above.  However, \lemref{far} implies that this final sort-and-scan
step allows us to check the correctness of the output (by counting the
number of intervals containing $p_a$ and $p_b$ and comparing this to
the computed value of $k$) and restarting the algorithm from scratch
if necessary.  This yields:

\begin{thm}
There exists a randomized algorithm that, given a set $I$ of $n$ real
intervals, finds a value $p\in\R$ contained in the largest number of
intervals of $I$ and that runs in $O(n+(n-k)\log (n-k))$ expected
time, where $k$ is the number of intervals containing $p$.
\end{thm}


\section{Algorithms for Points in $\R^3$ and $\R^4$}
\seclabel{3-4-d}

The previous section showed how the problem of computing the Tukey
depth of a point in $\R^2$ is equivalent to the problem of finding a
point contained in the largest number of halfcircles on the unit
circle $\Sp^1$.  A similar statement is true in $\R^d$:  Each point
$q\in S$ defines an open halfsphere $q^*=\{ v\in\Sp^{d-1} : v\cdot q <
0\}$.  That is, all vectors in $q^*$ are the inner normals of
halfspaces that contain $p$ but do not contain $q$.  Thus, the problem
of determining the Tukey depth of $p$ reduces to the problem of
finding the point contained in the largest number of halfspheres in
$S^*=\{q^* : q\in S\}$.

We observe that this problem can be solved by solving two
\textsc{MaximumFeasibleSubsystem} problems in $\R^{d-1}$. Refer to 
\figref{central}.  Each open
halfsphere $q^*\in S^*$ is the intersection of an open halfspace
$q^\#$ with $\Sp^{d-1}$.  Consider the intersection of $q^\#$ with the
hyperplane $H_+=\{(x_1,\ldots,x_d):x_d=1\}$.  By central projection,
there is a bijection between points in $\Sp^{d-1}_+$ and $H_+$ and
this bijection has the property that $r\in\Sp^{d-1}_+$ is in $q^*$ if
and only if the projection of $r$ is in $q^\#\cap H_+$.  Thus, finding
the point in $\Sp^{d-1}_+$ contained in the largest number of
halfspheres is equivalent to finding a point in $H_+$ contained in the
largest number of halfspaces.  A similar statement holds regarding
$\Sp^{d-1}_-$ using the hyperplane $H_-=\{(x_1,\ldots,x_d):x_d=-1\}$.
Finally, we observe that we do not need to consider solutions on the
equator $\Sp^{d-1}_0$ because our input consists of \emph{open}
halfspheres.\footnote{A solution to the Tukey depth problem is
obtained when we find a set $n-k$ open halfspheres that have a
non-empty common intersection.  The intersection of a finite set of open
halfspheres is either an empty set or a set with positive measure.
Since $\Sp^{d-1}_0$ has measure 0 any solution to the Tukey depth
problem contains points not in $\Sp^{d-1}_0$.}

\begin{figure}
\begin{center}\includegraphics{central}\end{center}
\caption{Computing the Tukey depth of a point in $\R^d$ reduces to two
\textsc{MaximumFeasibleSubsystem} problems in $\R^{d-1}$.}
\figlabel{central}
\end{figure}

The above discussion shows that computing the Tukey depth of a point
in $\R^d$ reduces to two instances of the problem
\textsc{MaximumFeasibleSubsystem} problem in $\R^{d-1}$: Given a set
$K$ of $n$ halfspaces in $\mathbb{R}^{d-1}$, find the subset $K'$ of
$K$ of minimum cardinality such that $\cap (K\setminus K')$ is
non-empty.  The current best results for
\textsc{MaximumFeasibleSubsystem} in small dimensions are due to Chan
\cite{c05}.  Using two instances of his algorithm for
\textsc{MaximumFeasibleSubsystem} in $\R^2$, respectively, $\R^3$, and
running them in parallel gives:

\begin{thm}
The Tukey depth of a point $p$ with respect to a set $S$ of $n$ points
in $\R^3$ can be computed in $O(n\log n + k^2\log n)$ time, where
$k$ is the value of the output.
\end{thm}


\begin{thm}
The Tukey depth of a point $p$ with respect to a set $S$ of $n$ points
in $\R^4$ can be computed in $O(n\log n + k^{11/4}n^{1/4}\log^{O(1)}
n)$ time, where $k$ is the value of the output.
\end{thm}

\section{An Algorithm for Points in $\R^d$}
\seclabel{d-d}
\seclabel{fpt}

Finally, we consider the general case of point sets in $\R^d$.  In the
previous section we showed that computing the Tukey depth of a point
$p$ with respect to a set $S$ of $n$ points in $\R^d$ can be reduced
to two instances of \textsc{MaximumFeasibleSubsystem} in $\R^{d-1}$.
In this section we give a fixed-parameter tractable \cite{df98}
algorithm for \textsc{MaximumFeasibleSubsystem}.   

The algorithm uses linear programming as a subroutine in the following
way:  Given a collection $K$ of halfspaces in $\R^{d-1}$, an algorithm
for linear programming can be used to either
\begin{enumerate}
\item Determine a point $p\in\cap K$ if such a point exists or,
\item report a subset $B\subseteq K$, $|B|\le d$, such that 
	$\cap B=\emptyset$.
\end{enumerate}
The set $B$ reported in the latter case is called a \emph{basic
infeasible subsystem}.  Standard combinatorial algorithms for linear
programming, including algorithms for linear programming in small
dimensions \cite{c95,d84,m83,m84,s91,sw92} as well as the simplex
method (c.f., \cite{chvatal80}), can generate a basic infeasible
subsystem given an infeasible linear program.  A method of finding
basic infeasible subsystems from interior point linear-programming
methods is described in \appref{bis}.

Let $\textsc{BIS}(K)$ denote a routine that outputs a basic infeasible
subsystem of $K$ if $K$ is infeasible, and that outputs the empty set
otherwise.  The following algorithm solves the
\textsc{MaximumFeasibleSubsystem} decision problem:

\noindent\begin{minipage}{\textwidth}
\noindent$\textsc{MFS}(K,k)$
\begin{algorithmic}[1]
\STATE{\COMMENT{$\star$ determine if there exists $K'\subseteq K$, $|K'|\le
k$, such that $\cap(K\setminus K')\neq\emptyset$ $\star$} }
\STATE{$B\gets\textsc{BIS}(K)$}
\IF{$B=\emptyset$}
   \STATE{\textbf{return} true}
\ENDIF
\IF{$k=0$}
   \STATE{\textbf{return} false}
\ENDIF
\FOR{each $h\in B$}
   \IF{$\textsc{MFS}(K\setminus\{h\},k-1)=\mathrm{true}$}
      \STATE{\textbf{return} true}
   \ENDIF
\ENDFOR
\STATE{\textbf{return} false}
\end{algorithmic}
\end{minipage}

The correctness of the above algorithm is easily established by
induction on the value of $k$.  The running time of the algorithm is
given by the recurrence
\[
    T(n,k) \le \lp(n,d-1)+ dT(n-1,k-1) \enspace ,
\]
where $\lp(n,d)$ denotes the running time of an algorithm for finding
a basic infeasible system in a linear program with $n$ constraints and
$d$ variables.  This recurrence readily resolves to
$O(d^k\lp(n,d-1))$.  
Using this as a subroutine for Tukey depth computation we obtain our
final result:

\begin{thm}\thmlabel{d-d}
The Tukey depth of a point $p$ with respect to a set $S$ of $n$ points
in $\R^d$ can be computed in $O(d^k\lp(n,d-1))$ time, where $k$ is
the value of the output and $\lp(n,d)$ is the time to solve a linear
program with $n$ constraints and $d$ variables.
\end{thm}

\noindent\textbf{Remark:}
The algorithm described above is closely related to Matou\v{s}ek's
algorithm for \textsc{MaximumFeasibleSubsystem} which, in our setting, 
has a running
time of $O(k^{d}\lp(n,d))$ \cite{m95}.  In the language of fixed-parameter
tractability, the primary difference between the two algorithms is
that Matou\v{s}ek's algorithm explores the search tree in
breadth-first order and uses a dictionary to ensure that identical
subtrees are not explored, whereas the current algorithm explores the
search tree in depth-first order.  The two algorithms can, of course,
be combined to obtain an algorithm with running time $O(\min\{k^d +
d^k\}\lp(n,d))$.


\section*{Ackowledgement}

Dan Chen, David Bremner, and Pat Morin are grateful to Diane~Souvaine
and the Radcliffe Institute for hosting the \emph{Workshop on
Computational Aspects of Statistical Data Depth Analysis}, July 7--10,
2006.  In particular the work in \secref{fpt} is the result of
discussions that began at the workshop.

\bibliographystyle{plain}
\bibliography{tukey,bis}

\appendix
\section{Computing a Basic Infeasible Subsystem}
\applabel{bis}

This appendix explains how, given an infeasible linear program, to
find a basic infeasible subsystem of that linear program.  This
routine is required as part of the algorithm for solving
\textsc{MaximumFeasibleSubsystem} described in \secref{d-d}.

For any matrix $M$, let $M_J$ denote the set of rows indexed by $J$.
Given a system of linear inequalities $Mx \geq b$, $M \in \R^{m \times
  d}$, a \emph{basic infeasible subsystem} is a subset of $\{ 1 \dots
m\}$ such that the system $M_I x \geq b_I$ is infeasible, and $|I|
\leq d+1$. 
 We consider the standard first stage simplex
problem (see e.g.~\cite{chvatal80}, p.~39). Let  $e$ denote the
$m$-vector of all ones, $c$ the length $d+1$ binary vector with
exactly one one in the last position and let $A=[M e]$. We can write the first stage LP for our system as
\begin{align*}
    \min c^Tx &= x_{d+1}\\\tag{P}\eqlabel{P}
    \text{subject to}\\
    Ax &\geq b
\end{align*}
  In the case of an
infeasible system, the optimal value of this LP will be strictly
positive. 
The dual LP of \eqref{P} is
\begin{align*}
  \max b^T y\\\tag{D}
  \eqlabel{D}
  \text{subject to}\\
  yA &= c\\
  y & \geq 0
\end{align*}
In what follows, we generally follow the notation of \cite{megiddo91},
except that we interchange the definitions of the primal and dual LPs.
Define a \emph{basic partition} (or just \emph{basis}) $(\beta,\eta)$
as a partition of the row indices of $A$ such that $A_\beta$ is
nonsingular.  For each basic partition, we define a \emph{primal basic
  solution}
\begin{equation*}
  x^*=A_\beta^{-1} b_\beta
\end{equation*}
and a \emph{dual basic solution}
\begin{equation*}
  y^*= c A^{-1}_\beta
\end{equation*}
We say that a basis is \emph{primal feasible} (resp.\ \emph{dual
  feasible}) if $x^*$ is feasible for \eqref{P} (respectively $y^*$
is feasible for \eqref{D}).  It is a standard result of linear
programming duality that a basis which is both primal and dual feasible
defines a pair $(x^*,y^*)$ of optimal solutions to the primal and dual
LP's; such a partition is called an \emph{optimal basis partition}

In general LP algorithms (either directly in the case of Simplex type
algorithms, or via postprocessing using
e.g.~\cite{megiddo91,vavasis96,BelingMegiddo98}) provide an optimal
basis partition $(\beta,\eta)$.  Consider the relaxed LP
\begin{align*}
  \min c^T x \\\tag{R}
  \eqlabel{R}
  \text{subject to}\\
  A_\beta x \geq b_\beta
\end{align*}
It is easy to verify that an optimal basis partition for \eqref{P}
is also primal and dual feasible for \eqref{R}.  This implies that
the system $M_\beta x \geq b_\beta$ is infeasible, and provides a
basic infeasible system.  Using interior point algorithms
(see~\cite{gonzaga87,renegar88,vaidya90,Roos89,RoosVial92}), a
solution to the first stage LP can be found $O(m^3L)$ time, where $L$
is the number of bits in the input.  Using the algorithm of Beling and
Megiddo~\cite{BelingMegiddo98}, an optimal basis partition can be
computed in $O(m^{1.594}d)$ time (where the bound is based on the best
known methods for fast matrix multiplication).

\end{document}

