\documentclass[lotsofwhite]{patmorin}
\usepackage{amsthm,amsopn}

\newcommand{\CH}{\mathrm{conv}}
%\newcommand{\argmax}{\mathrm{argmax}}
%\newcommand{\argmax}{\operatorname{argmax}}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\drop}{\!\!\downarrow\!\!}
\newcommand{\dual}{\varphi}

\input{pat}

%\title{An Optimal Randomized Algorithm for $d$-Variate Zonoid Depth}
\title{\MakeUppercase{An Optimal Randomized Algorithm
	for} $d$\MakeUppercase{-Variate Zonoid Depth}}
\author{Pat Morin \\ 
	School of Computer Science \\
	Carleton University \\
	\texttt{morin@scs.carleton.ca}}
\date{}

\begin{document}
\maketitle
\begin{abstract}
A randomized linear expected-time algorithm for computing the zonoid
depth (Dyckerhoff \etal\ 1996, Mosler 2002) of a point with respect to a fixed
dimensional point set is presented.
\end{abstract}

\section{Introduction}

Let $S$ be a set of $n$ points in $\R^d$.
For a real number $k\ge 1$, the \emph{$k$-zonoid} of $S$ is defined as 
\[
      Z_k(S) = \left\{\sum_{p\in S}\lambda_p p
	: \mbox{$0\le \lambda_p \le 1/k$ for all $p\in S$  
	   and $\sum_{p\in S}\lambda_p = 1$}  \right\} 
\] 
\cite{dkm96,m02}.
Notice that, for $k=1$ the $1$-zonoid of $S$ is the convex hull of
$S$,  i.e., $Z_1(S)=\CH(S)$.
As $k$ increases, $Z_k(S)$ becomes smaller and smaller until
the limiting case $k=n$, for which $Z_n(S)$ consists of a single point,
the mean of $S$.  The \emph{zonoid depth} of a point
$p\in\CH(S)$ with respect to $S$ is defined as
\[
     Z(p,S) = \sup\{k : p\in Z_k(S) \} \enspace ,
\]
and is a real number in the interval $[1,n]$.

Dyckerhoff \etal\ \cite{dkm96} give an algorithm to compute $Z(p,S)$
by solving a linear program in the variables $\{\lambda_p:p\in S\}$.
To obtain an efficient algorithm they make use of the fact that most
of the constraints on the $\lambda$'s are independent of $S$.  The
worst-case running time of their algorithm is unclear.

Bern and Eppstein \cite{be01} study zonoids (also called \emph{reduced
convex hulls}) in the context of support vector machines used in
machine learning.  Among other things they solve a more general
problem than that of zonoid depth: Given two sets $S_1$ and $S_2$ in
$\R^d$, compute the minimum value $k$ such that $Z_k(S_1)\cap
Z_k(S_2)$ is non-empty.  Their algorithm has a running time of
$O(n(Ld\log n)^{O(1)})$, where $L$ is the number of bits used to
describe the points in $S_1$ and $S_2$.  Their algorithm uses
Kachiyan's ellipsoid method for linear programming \cite{k79} to
exploit the fact that, for a given direction $v$, it is easy (see
\secref{dual}) to test if there is a hyperplane orthogonal to $v$ that
separates $Z_k(S_1)$ and $Z_k(S_2)$.

Gopala and Morin \cite{gm06} consider algorithms for bivariate ($d=2$)
zonoid depth and give a randomized $O(n)$ expected time algorithm for
computing $Z(p,S)$ when $p$ and $S$ are in $\R^2$.  Their algorithm is
a combination of two techniques, namely a prune-and-search algorithm
due to Lo \etal\ \cite{lms94} for searching the $k$-level of a line
arrangement and an optimization method due to Chan \cite{c99} for
efficiently converting decision algorithms into optimization
algorithms.   While the latter technique extends efficiently into
arbitrary (constant) dimensions \cite{c04} the former technique,
unfortunately, does not.  Note that this algorithm is combinatorial in
that it makes all decisions based on the results of evaluating a few
different kinds of predicates. Other than evaluating these predicates,
the running time does not depend on the input precision.

The current paper extends and bridges the above results by giving an
$O(n)$ time algorithm to compute $Z(p,S)$ when $p$ and $S$ are in
$\R^d$ for any constant dimension $d$.  The algorithm uses a recent
method, due to Chan \cite{c04}, of solving linear programs with many
constraints that are defined implicitly by a small number of objects.
Again, the algorithm is combinatorial, and the input precision affects
only some low-level predicates.

Besides being the first linear-time algorithm for solving the zonoid
depth problem in constant dimensions, the current results are
interesting for two other reasons:

\begin{enumerate}
\item Zonoid depth is one of many definitions of depth proposed in the
robust statistics literature \cite{lps99}.  Perhaps the gold
standard in this regard is \emph{Tukey (halfspace) depth} \cite{t74}:
\[
      T(p,S) = \min\{ |h\cap S| : \mbox{$h$ is a closed halfspace
containing $p$} \} \enspace .
\]
Tukey depth and zonoid depth have an interesting feature in common;
under duality, the combinatorial structure of the depth $k$ contour is
determined by the $k$-level and the $(n-k+1)$-level of a set of
hyperplanes. The structure of $k$-levels has been extensively studied
by combinatorial geometers \cite[Chapter~11]{mat02} although our
understanding of their complexity is still not complete, even in 2
dimensions.

The current result shows a divergence in the computational complexity
of Tukey and zonoid depth.  In constant dimensions $d\ge 3$ the
fastest algorithms for computing the Tukey depth of a point have
running times of $\Omega(n^{d-1})$ \cite{c05}, whereas the current
result shows that zonoid depth can be computed in $O(n)$ time in any
constant dimension $d$.  When the dimension grows arbitarily large the
situation is even worse.  Computing $T(p,S)$ is NP-hard in general
\cite{c94}, while the result of Bern and Eppstein \cite{be01} yields a
polynomial time algorithm for computing $Z(p,S)$ in any dimension.
Thus, together these results show that zonoid depth is
computationally more tractable than Tukey depth in both large and
small dimensions.

\item Our algorithm makes use of Chan's recent technique for solving
implicit linear programs in small dimensions \cite{c04}.  Interestingly,
this technique was introduced in order to solve a problem related to
Tukey depth, namely the problem of finding a point $p$ that maximizes
$T(p,S)$.  Unfortunately, the resulting algorithm runs in $O(n\log n +
n^{d-1})$ time, limiting its usefulness for dimensions $d\ge
3$.\footnote{In fact, this running time is probably optimal. See Chan
\cite[Section 1.4]{c04} for details.}  Indeed, although Chan's
technique itself does not asymptotically increase the running time as
the dimension $d$ increases, it seems that most applications of the
technique either break down or have quickly increasing running times
as $d$ increases.\footnote{One notable exception is parametric minimum
spanning trees \cite{e03}.}  The current result is therefore an
atypical example that illustrates the full utility of this extremely
powerful technique.
\end{enumerate}

In the following, all points, vectors, and hyperplanes are assumed to
live in $\R^d$ and $\Hy^d$ denotes the set of all hyperplanes in
$\R^d$.  The notation $x_i$ denotes the $i$th coordinate of the point
$x$.  We use the $\cdot$ notation to denote the inner-product of two
points/vectors, i.e., $x\cdot y=\sum_{i=1}^d x_iy_i$.  For a set $S$
of $n$ points and a non-zero vector $r$, $S_1^r,\ldots,S_n^r$ is the
sequence of elements of $S$ ordered by decreasing projections onto
$r$, i.e., $S_i^r\cdot r \ge S_{i+1}^r\cdot r$ for all $1\le r\le
n-1$.

For a point $x$ and a hyperplane $h$, we denote by $x\drop h$ the
$d$th coordinate of the vertical projection of $x$ onto $h$ (the
height of $x$ when dropped onto $h$).  For a set $H$ of $n$
hyperplanes, let $H_i^x$ be the $i$th hyperplane in $H$ encountered by
a downward vertical ray originating at $(x_1,\ldots,x_{d-1},\infty)$.
For ease of notation we use the shorthand $H_{-i}^x=H_{|H|-i+1}^x$.
For $i>|H|$ we use the convention that $H_{i}^x$ (respectively
$H_{-i}^x$) is the ``horizontal hyperplane at infinity''
$\{x:x_d=-\infty\}$ (respectively, $\{x:x_d=+\infty\}$). 

The remainder of this paper is organized as follows: \secref{chan}
reviews Chan's generalized optimization technique.  \secref{dual}
discusses properties of zonoids in primal and dual space.
\secref{decision} presents an algorithm to answer to the decision
problem, given $p$, $S$, and $k$, is $p\in Z_k(S)$?  Finally,
\secref{optimization} describes our algorithm for, given $p$ and $S$,
computing $Z(p,S)$.
 
\section{Chan's Generalized Optimization Technique}
\seclabel{chan}

Chan \cite{c04} used the following theorem to provide an $O(n\log n)$
time algorithm for maximum Tukey depth.\footnote{Actually,
\thmref{chan} applies to LP-type problems \cite{sw92}. Here we only state it's
specialization to linear programming problems.}  In the following, and
throughout the remainder of the paper, we use the shorthand $\cap S$
to denote $\bigcap_{s\in S}s$.

\begin{thm}[Chan 2004]\thmlabel{chan}
Let $\mathcal{H}$ denote the set of all halfspaces in $\R^d$,
let $\mathcal{P}$ denote the set of all possible inputs to some problem, 
let $f:\mathcal{P}\mapsto 2^{\mathcal{H}}$ be any function mapping 
problem inputs to sets of halfspaces,
let $g:\R^d\mapsto \R$ be
any linear objective function, 
and let $D(n)=\Omega(n^{\epsilon})$, for some $\epsilon>0$, be a non-decreasing function of
$n$.  Suppose that $f$ and $g$ satisfy:
\begin{enumerate}
\setcounter{enumi}{-1}
\item Given inputs $P_1,\ldots,P_d\in\mathcal{P}$ each of constant
size, a point $p\in\cap (f(P_1)\cup\cdots\cup f(P_d))$ maximizing
$g(p)$ can be found in constant time.

\item Given a point $p\in\R^d$ and an input $P\in\mathcal{P}$
of size $n$, there exists a $D(n)$ time algorithm to determine whether
$p\in\cap f(P)$.

\item There exists constants $\alpha < 1$ and $r$ such that, for any
input $P\in\mathcal{P}$ of size $n$, it is possible to compute, in
$D(n)$ time, inputs $P_1,\ldots,P_r$, each of size at most
$\ceil{\alpha n}$, and such that $\cap f(P) =
\cap(f(P_1)\cup\cdots\cup f(P_r))$.

\end{enumerate}
Then there exists a randomized $O(D(n))$ expected time algorithm to
compute, for any input $P\in\mathcal{P}$ of size $n$ a point
$p\in\cap f(P)$ that maximizes $g(p)$.
\end{thm}

It is worth noting that the codomain of the function $f$ may contain
infinite sets.  That is, it is acceptable (and common) to have inputs
$P\in\mathcal{P}$ that generate an infinite number of constraints,
i.e., $|f(P)|=\infty$.


\section{Properties of Primal and Dual Zonoids}
\seclabel{dual}

The $k$-zonoid $Z_k(S)$ is a convex polytope.  The extreme-most vertex
of $Z_k(S)$ in direction $x$ can be obtained as a convex combination of the
$\ceil{k}$ extreme-most points of $S$ in direction $x$.  More
precisely,
\begin{equation} 
\argmax_p\{p\cdot x: p\in Z_k(S)\} =
        \left(\sum_{i=1}^{\floor{k}} \frac{1}{k}S_i^x\right) +
          (1-\lfloor k\rfloor/k)S_{\ceil{k}}^x  
          \eqlabel{extreme}
\end{equation}
\cite{be01,gm06}.  Intuitively, we assign the maximum allowable
coefficient ($1/k$) to each of the $\floor{k}$ extreme-most vertices
and the ``leftover'' ($1-\lfloor k\rfloor/k$) is assigned to the next vertex.

We wish to arrive at a situation in which we can apply \thmref{chan}
and this is best done by working in the dual.
Consider the point-hyperplane duality function
$\dual$ given by 
\[
    \dual(x)=\{y\in\R^d : y_d = x_1y_1 +\cdots +x_{d-1}y_{d-1} - x_d \}
\] 
when $x$ is a point in $\R^d$ and
\[
     \dual(X) = \{\dual(x) : x\in X\}
\]
when $S$ is a subset of $\R^d$.  See Edelsbrunner's book \cite{e97}
for properties of this duality. 

Let $H=\dual(S)$.  Then,
under this duality, the \emph{dual $k$-zonoid} $\dual(Z_k(S))$ is the set 
of all hyperplanes in $\R^d$
that do not intersect either of two convex sets $A_k(S)$ and $B_k(S)$.
That is,
\[
     \dual(Z_k(S)) = \{ h\in\Hy^d : h\cap(A_k(S)\cup B_k(S)) = \emptyset \} \enspace ,
\]
where
\begin{equation}
   A_k(S) = \left\{x\in\R^d: x_d \ge 
\left(\sum_{i=1}^{\floor{k}} \frac{1}{k}(x\drop H_i^x)\right) +
          (1-\lfloor k\rfloor/k)(x\drop H_{\ceil{k}}^x) \right\}  \eqlabel{ra}
\end{equation} 
and
\begin{equation}
   B_k(S) = \left\{x\in\R^d: x_d \le 
\left(\sum_{i=1}^{\floor{k}} \frac{1}{k}(x\drop H_{-i}^x)\right) +
          (1-\lfloor k\rfloor/k)(x\drop H_{-\ceil{k}}^x) \right\} \enspace .
\end{equation}
The definitions of $A_k(S)$ and $B_k(S)$ follow from \eqref{extreme}
and the duality $\dual$.
The two sets $A_k(S)$ and $B_k(S)$ are convex, unbounded from above,
respectively, below, and piecewise linear.  Indeed, the linear pieces
of $A_k(S)$ (respectively $B_k(S)$) are in one to one correspondence
with the linear pieces of the $\ceil{k}$-level (respectively
the $(n-\floor{k}+1)$-level) of the hyperplanes in $H$.  Thus,
$A_k(S)$ and $B_k(s)$ are convex polytopes that are implicitly defined
by the hyperplanes in $H$ and it is these implicit ``linear programs''
that will ultimately allow us to apply \thmref{chan}.


\section{The Decision Algorithm}
\seclabel{decision}

Next we consider the following decision problem:  Given a point set
$S$ and an integer $k$, is the origin contained in $Z_k(S)$?  By
translation, a solution to this problem allows us to test if an
arbitrary point $p\in\R^d$ is contained in $Z_k(S)$. One approach to
solving this problem is to compute the intersection of $Z_k(S)$ with
the vertical line $\{x\in\R^d:x_0=x_1=\cdots=x_{d-1}=0\}$ through the origin
and then check if this intersection contains the origin.

Under the duality $\dual$, the above strategy is equivalent to finding
the lowest point on $A_k(S)$ and the highest point on $B_k(S)$ and
checking that each of these points is above, respectively, below, the
hyperplane $\{x\in\R^d:x_d=0\}$.  In the remainder, we focus on
determining the lowest point in $A_k(S)$.  Finding the highest point
in $B_k(S)$ is done in a symmetric manner.  However, before we can
proceed, we need to define a slightly more general problem involving
weights.

Let $S$ be a set of $n$ points in $\R^d$ and let $w:S\mapsto\N$ be a
function assigning positive integer weights to the elements of $S$.
We denote by $S^w$ the multiset in which each element $p\in S$ occurs
$w(p)$ times.  The \emph{$w$-weighted zonoid} $Z_k(S,w)$ is simply the
$k$-zonoid of the multiset $S^w$, i.e., $Z_k(S,w)=Z_k(S^w)$.  As with
standard zonoids, the weighted zonoid $Z_k(S,w)$ dualizes to the set
of all lines that do not intersect either of two convex regions
$A_k(S,w)$ and $B_k(S,w)$, where $A_k(S,w)=A_k(S^w)$ and
$B_k(S,w)=B_k(S^w)$.

This definition of weighted zonoids allows us to naturally define
subproblems.  For a subset $C\subseteq S$, define the \emph{total
weight}
\[
       w(C)=\sum_{p\in C}w(p)
\]
and the \emph{weighted mean}
\[ 
       \mu(C)=\frac{1}{w(C)}\sum_{p\in C} p\times w(p) \enspace .
\]
The \emph{contraction} of
$(S,w)$ by $C$ is obtained by replacing the points of $C$ by their
weighted average, $\mu(C)$.  More precisely, the contraction of
$(S,w)$ by $C$ is the pair $(R,v)$ where 
\[ R = (S\setminus C) \cup \{ \mu(C) \} \] 
and 
\[ v(p) = \left\{\begin{array}{ll} 
        w(p) & \mbox{if $p\in S\setminus C$} \\ 
        w(C) & \mbox{if $p=\mu(C)$} \end{array}\right.
\]

The following lemma shows that contraction results in strictly smaller
zonoids:

\begin{lem}\lemlabel{contraction}
If $(R,v)$ is a contraction of $(S,w)$ by $C$ then $Z_k(R,v)
\subseteq Z_k(S,w)$.
\end{lem}

\begin{proof}
Let $x$ be any point in $Z_k(R,v)$.  Then, by the definition of
zonoids:
\begin{eqnarray*}
    x &=& \sum_{p\in R^v} \lambda_pp \\
      &=& \left( \sum_{p\in (R\setminus \{\mu(C)\})^v} \lambda_pp \right)
          + \left( \sum_{p\in \{\mu(C)\}^v}\lambda_{\mu(C)}p \right) \\
      &=& \left(\sum_{p\in (S\setminus C)^w} \lambda_pp \right)
          + \left(\sum_{p\in C^w}\lambda_{\mu(C)} p\right) \\
      &\in& Z_k(S,w)
\end{eqnarray*}
as required.
\end{proof}

We now have all the tools required to apply \thmref{chan} to solve our
decision problem.

\begin{thm}\thmlabel{decision}
Given a set $S$ of $n$ points in $\R^d$ and a 
function $w:S\mapsto\N$ that is computable in constant
time, the point $x\in A_k(S,w)$ such
that $x_d$ is minimum can be found in $O(n)$ expected
time.
\end{thm}

\begin{proof}
Let $f$ be the function that maps the pair $(S,w)$ onto a set of
halfspaces whose intersection is $A_k(S,w)$ and let the objective
function $g(p)=p_d$.  We need to show that the function $f$ and $g$
satisfy
Conditions~0--2 of \thmref{chan}.

To satisfy Condition 0 of \thmref{chan} we can enumerate all the
linear constraints generated by each of the $d$ subproblems and use
any linear programming algorithm to find a point $x$ that satisfies
all constraints and such that $x_d$ is minimum.  There are only $d$
subproblems, each of constant size, so this step takes constant time,
as required.

To satisfy Condition 1 of \thmref{chan} we observe that testing if
$x\in A_k(S,w)$ simply involves checking if $x$ satisfies \eqref{ra}.
Let $H=\dual(S)$.  This check can be accomplished by using a
$D(n)=O(n)$ time weighted selection algorithm
\cite[Exercise~9-2]{clrs01} to compute the smallest index $t$ and the
hyperplanes $H_{1}^x,\ldots,H_{t}^x$ such that
$\sum_{i=1}^tw(\dual(H_{i}^x)) \ge k$.  Once this is done we need only
check \eqref{ra} which, in the weighted setting, becomes 
\[
     x \ge \left(\sum_{i=1}^{t-1} \frac{1}{k}(x\drop
H_i^x)\times w(\dual(H_i^x))\right) 
   + \frac{1}{k}(x\drop H_t^x) \times \left(k-\sum_{i=1}^{t-1} v(\dual(H_{i}^x)) \right)
\enspace .
\]

To satisfy Condition~2 of \thmref{chan} we make use of \emph{cuttings}
\cite[Section~4.7]{mat02}.  In particular, we use the fact that, in
$O(n)$ time, it is possible to partition $\R^d$ into $r=O(1)$
simplices $\Delta_1,\ldots,\Delta_r$ such that the interior of each
simplex is intersected by at most $n/2$ of the hyperplanes in
$\dual(S)$.  For each simplex $\Delta_i$ we create a subproblem
$(S_i,w_i)$ as follows: Let $C_i\subseteq S$ contain every point $p\in
S$ such that $\dual(p)$ is above the interior of $\Delta_i$.  We first
construct the pair $(T_i,w_i)$ by contracting $(S,w)$ by $C_i$.  Next,
we obtain $S_i$ by removing from $T_i$ every point $p$ such that
$\dual(p)$ is below the interior of $\Delta_i$.  The subproblems
$(S_i,v_i)$ for $1\le i\le r$ that we obtain in this manner are each
of size at most $n/2+2$.

It follows from \lemref{contraction} (the contraction step) 
and the definition of $Z_k(S,w)$ (the deletion step)
that $Z_k(S_i,{w_i})\subseteq Z_k(S,w)$.  In the dual, this means that
$A_k(S_i,{w_i})\supseteq A_k(S,w)$.  To satisfy Condition~2 of
\thmref{chan} we must
show that $\bigcap_{i=1}^r A_k(S_i,{w_i}) = A_k(S,w)$.  To do this,
consider any point $x$ on the boundary of $A_k(S,w)$.  It is
sufficient to show that $x$ is also on the boundary of at least one
region $A_k(S_i,{w_i})$ for $1\le i\le r$.  The point $x$ is defined
by $\ceil{k}$ hyperplanes $h_1,\ldots,h_{\ceil{k}}\in\dual(S^w)$ in
the sense that 
\[
   x_d=\left(\sum_{i=1}^{\floor{k}} \frac{1}{k}(x\drop h_i)\right) 
       + (1-\lfloor k\rfloor/k)(x\drop h_{\ceil{k}}) \enspace .
\]
Let $q=x\drop h_{\ceil{k}}$.  There
is some simplex $\Delta_i$ that contains $q$.  Observe that
each of $h_1,\ldots,h_{\ceil{k}-1}$ is either completely above
the interior of $\Delta_i$ or intersects $\Delta_i$.  Furthermore, 
any hyperplane in
$\dual(S)$ that is completely above $\Delta_i$ is one of
$h_1,\ldots,h_{\ceil{k}-1}$.  Therefore, the subproblem $(S_i,w_i)$
is obtained from $(S,w)$ by contracting
$C_i\subseteq\{\dual(h_1),\ldots,\dual(h_{\ceil{k}-1})\}$ and then deleting 
some subset of
$S\setminus\{\dual(h_1),\ldots,\dual(h_{\ceil{k}-1})\}$.  Let
$I=\dual(S_i^{w_i})$.  Then, every point $x$ in $A_k(S_i,w_i)$ must
satisfy
\begin{eqnarray*}
  x_d & \ge & \left(\sum_{i=1}^{\floor{k}} \frac{1}{k}(x\drop I_i^x)\right) 
       + (1-\lfloor k\rfloor/k)(x\drop I_{\ceil{k}}^x) \\
   & = & \left(\sum_{i=1}^{\floor{k}} \frac{1}{k}(x\drop h_i)\right) 
       + (1-\lfloor k\rfloor/k)(x\drop h_{\ceil{k}}) \enspace .
\end{eqnarray*}
Thus $x$ is on the boundary of $A_k(S_i,w_i)$, as required.  We have
now satisfied all three conditions necessary to apply \thmref{chan},
completing the proof.
\end{proof}

\section{The Optimization Algorithm}
\seclabel{optimization}

In the previous section we showed, given $p$, $S$ and $k$, how to
answer the question:  Is $p\in Z_k(S)$?  In this section we consider
the optimization problem, given $p$ and $S$: What is the maximum value
of $k$ such that $p\in Z_k(S)$?  For this problem, we can apply
\thmref{chan} again, this time on a problem in $\R^{d+1}$.

Consider the point set
\[
    Z(S)=\{p\in\R^{d+1} : (p_1,\ldots,p_d)\in Z_{p_{d+1}}(S)  \} \enspace .
\]
The set $Z(S)$ is a convex polytope in $\R^{d+1}$.
Dualizing $Z(S)$ as before gives two regions
$A(S)$ and $B(S)$.
We are interested in the hyperplane $h_0=\{x\in\R^{d+1}:x_d=0\}$.
In particular, the value $k$ we are searching for is the minimum of
$k_A$ and $k_B$ where
\[
   k_A = \min\{p_{d+1}: p\in h_0\cap A(S)\}
\]
and
\[
   k_B = \min\{p_{d+1}: p\in h_0\cap B(S)\}
\]
In words, we want the minimum value of $k$ such that $A_k(S)$
(respectively, $B_k(S)$)
intersects the hyperplane $h_0=\{x\in\R^d: x_d=0\}$.

\begin{thm}
Given a set $S$ of $n$ points in $\R^d$ and a point $p\in\R^d$, the
maximum value $k$ such that $p\in Z_k(S)$ can be found in $O(n)$
expected time.
\end{thm}

\begin{proof}[Proof Sketch]
The proof is another application of \thmref{chan} to find the values
$k_A$ and $k_B$ described above.  We focus only on finding $k_A$, as
finding $k_B$ is a symmetric problem.  The details are much the same
as in \thmref{decision} so we only sketch them.  As before we
generalize $A(S)$ and $B(S)$ to the weighted setting using multisets
and let $f(S,w)$ be the function that maps $(S,w)$ on to the set of
linear constraints that define $h\cap A(S^w)$.

As before, $S$ satisfies Condition~0 of \thmref{chan} since, for
constant size subproblems we can explictly generate the polytopes
$Z(S_1,w_1),\ldots,Z(S_{d+1},w_{d+1})$, compute their common
intersection with $h_0$ and find a point in the intersection
maximizing the objective function $g(p)=p_{d+1}$. 

The decision problem we must solve to satisfy Condition~1 of
\thmref{chan} is the problem of testing whether a point $p\in
\R^{d+1}$ is contained in $h_0\cap A(S)$.  But this is simply a matter
of checking that $p\in h_0$ and that $(p_1,\ldots,p_d)$ is in
$A_{p_{d+1}}(S)$, a problem for which we described an $O(n)$ time
algorithm in the proof of \thmref{decision}.

The partitioning into subproblems required to satisfy Condition~2 of
\thmref{chan} can be done in exactly the same manner as described in
the proof of \thmref{decision}.  To see that this partitioning works
in the current case we need only observe that the paritioning makes no
use of the value $k$ and the argument used to show its correctness
holds for all values of $k$.  This completes the proof sketch.
\end{proof}

\bibliographystyle{plain}
\bibliography{zonoid}

\end{document}

