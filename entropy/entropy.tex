\documentclass[charterfonts,lotsofwhite]{patmorin}
\usepackage{graphicx}
\usepackage{amsopn}
\input{pat}

\newcommand{\defequals}{\stackrel{\mathrm{def}}{=}}
\newcommand{\boundary}{\partial}
\DeclareMathOperator{\depth}{depth}

\title{\MakeUppercase{Distribution-Sensitive Point-in-Polygon Testing}}
\author{ULB Algorithms \and John Iacono \and Pat Morin}
\date{}

\begin{document}
\maketitle

\section{Introduction}


For a set $X$, itself containing sets, we denote the union of its
elements by $\cup X=\bigcup_{x\in X} x$.

For a distribution $D$ and an event $X$, we denote by $D_{|X}$ the
distribution $D$ conditioned on $X$.  That is, the distribution where 
the probability of an even $Y$ is
$\Pr(Y|X)=\Pr(Y\cap X)/\Pr(X)$.

A \emph{decision problem} $\mathcal{P}$ over a domain $\mathcal{D}$ is
a function $\mathcal{P}:\mathcal{D}\mapsto \{0,1\}$.

A \emph{decision tree} is a full binary tree\footnote{A full binary
tree is a rooted ordered binary tree in which each non-leaf node has
exactly two children.} in which each internal node $n$ is labelled
with a predicate $P_n(\cdot)$ and for which each leaf $\ell$ is
labelled with a boolean value $d(\ell)\in\{0,1\}$. The \emph{search
path} of an input $p$ in a comparison tree $T$ starts at the root of
$T$ and, at each internal node $n$, proceeds to the left child of $n$
if $P_n(p)$ is false and proceeds to the right child of $n$ otherwise.
We denote by $T(p)$ the label of the final (leaf) node in the search
path for $p$.  We say that the decision tree $T$ solves a decision
problem $\mathcal{P}$ over the domain $\mathcal{D}$ if, for every $p\in
\mathcal{D}$, $\mathcal{P}(p)=T(p)$.

When there is a probability measure $D$ over $\mathcal{D}$, we define
the \emph{expected search time} of a decision tree $T$ as the expected
length of the search path for $p$ when $p$ is drawn at random from
$\mathcal{D}$ according to the distribution $D$.  Note that, for each
leaf $\ell$ of $T$ there is a subset of $r(\ell)\subseteq \mathcal{D}$
such that the search path for any $p\in r(\ell)$ ends at $\ell$.
Thus, the expected search time of $T$ can be written as
\[
     \mu_D(T) = \sum_{\ell\in L(T)} \Pr(r(\ell))\times \depth(\ell)
	\enspace ,
\]
where $L(T)$ denotes the leaves of $T$ and $\depth(\ell)$ denotes the
length of the path from the root of $T$ to $\ell$.


\section{Collections of Intervals}

Before considering point-in-polygon testing we warm up by studying a
related one-dimensional problem on a set of intervals.  Our motivation
for studying this problem is that it gives intuition into how to
characterize the complexity of point-in-polygon testing under random
distributions.

Let $I=\{[a_i,b_i]: i=1,\ldots,n\}$ be a set of $n$ disjoint closed
real intervals, with $b_i < a_{i+1}$ for all $1\le i\le n$.  We are
interested in the problem of constructing a decision tree for $I$ that
can answer the decision problem: Given $p\in\R$, is $p\in \cup I$?
The type of decision trees we consider are \emph{comparison trees}.
These are decision trees in which each internal node $v$ is labelled
with a comparison of the form $P_v(x) = [x > n(v)]$ where $n(v)$ is
some real number called the \emph{label} of $v$.\footnote{Here and
throughout, we use Iverson's notation \cite{kXX} where, for a
predicate $X$, $[X]$ evaluates to 1 if $X$ is true and 0 otherwise.}
In particular, we are interested in the case when the query point $p$
is drawn according to some probability distribution $D$.   Throughout
the remainder of this section, all references to probabilities are
implicitly with reference to $D$ and the query domain is the support
of $D$.  We call such a decision tree a membership tester for $(I,D)$.

To study the expected search time of comparison trees for this problem
we need to define a slightly different set of intervals than $I$.  Let
$J$ be the set of intervals obtained after the following two
operations:

\begin{enumerate}

\item Remove from $I$ all intervals $\ell$ such that $\Pr(\ell) = 0$.

\item Repeat the following operation as many times as possible:  If
$\Pr([b_i,a_{i+1}])=0$ then replace the two intervals $(a_i,b_i)$ and
$(a_{i+1},b_{i+1})$ with the single interval $(a_i,b_{i+1})$
\end{enumerate}

\begin{lem}\lemlabel{intervals}
Let $\overline{J}$ be the set of maximal open intervals in
$\R\setminus\cup J$ and let $S=J\cup \overline{J}$.  Then the
expected search time of any membership tester $T$ for $(I,D)$ is at least
\[
    \mu_D(T) \ge H(I,D) \defequals 
	-\sum_{x\in S} \Pr(x)\log \Pr(x) \enspace .
\]
\end{lem}

\begin{proof}
Consider two distinct intervals $i,j\in S$ and suppose $p\in i$ and
$q\in j$.  Note that the search paths for $p$ and $q$ must finish in
different leaves of $T$ since, otherwise, there is some point
$r\in[p,q]$ that is incorrectly classified by $T$, i.e., such
that $T(p)\neq [p\in\cup I]$.  Now, for every interval $i\in S$,
select a point $p_i\in i$ such that the length of the search path for
$p$ is minimum.   Using the standard encoding of paths in a binary
search tree, this gives a code for the intervals of $S$ where the
length of the codeword for $i$ is the depth of $p_i$ in $T$.  In this
way we obtain

\[
   \mu_D(T)
    = \sum_{\ell\in L(T)} \Pr(\ell)\depth(\ell)
   \ge \sum_{i \in S} \Pr(i)\depth(p_i)
   \ge H(I,D)
   \enspace , 
\]
where the second inequality follows from Shannon's source coding
theorem \cite{X}.
\end{proof}

Though not the focus of the current paper, it is worth noting that the
lower bound in \lemref{intervals} can be matched to within an additive
constant by storing the intervals of $J$ and $\overline{J}$ at the
leaves of a biased binary search tree \cite{kXX,mXX}.  In particular,
for every $I$ and $D$ there exists a membership tester for $(I,D)$
whose expected search time is $H(I,D)+O(1)$.

\section{Point In Convex Polygon Testing}

Let $P$ be a convex $n$-gon whose boundary is denoted by $\partial P$
and consider a probability measure $D$ over $\R^2$.  In this section
we are interested in the problem of testing whether a query point $p$,
drawn according to $D$, is contained in $P$.   

In particular, we are interested in algorithms that can be described
as \emph{linear decision trees}.  These are decision trees such that
each internal node $v$ contains a linear inequality $P_v(x,y)=[ax+by
\ge c]$.  Again, we require that, for every $p$ in the support of $D$,
the leaf reached in the search path for $p$ is labelled with a 1 if
and only if $p\in P$.  Geometrically, each internal node of $T$ is
labelled with a directed line and the decision to go to the left or
right child depends on whether p is to the left or right (or on) this
line.  

\subsection{Nice Distributions}

In this section we assume throughout that the support of $D$ is a
convex set whose interior includes $P$.  The following lemma is
crucial in proving the optimality of our data structure:

\begin{lem}\lemlabel{lower-bound-a}
Let $C=P_1\setminus P_2$ where $P_1$ and $P_2$ are closed unbounded
convex subsets of $\R^2$ with $P_2\subseteq P_1$.  Let
$C_1,\ldots,C_k$ be the connected components of $C\setminus\partial
P$.  Then, for any linear decision tree $T$,
\[
	\mu_{D_{|C}}(T) \ge \frac{1}{2}\times H(P,D_{|C},C) 
		= -\frac{1}{2}
			\times \sum_{i=1}^k \Pr(C_i|C)\log \Pr(C_i|C)
		\enspace .
\]
\end{lem}

\begin{proof} 
We first prove a lower bound for  a slightly different type of
decision tree.  A \emph{chord} of $C$ is a line segment whose
endpoints are on the boundary of $C$.  A \emph{chord decision tree}
for $C$ is defined in exactly the same manner as a linear decision
tree except that the internal nodes of the tree are labelled with
chords of $C$.  The chord at a node $v$ partitions $C$ into two parts
and each of these corresponds to one of the children of $v$.  The
following claim relates chord decision trees to linear decision trees
and follows easily from the fact that the intersection of any line
with $C$ consists of at most 2 line segments.

\begin{clm}\clmlabel{two-approx}
Given a linear decision tree $T$ for $(P,D_{|C})$ with expected cost $k$,
there exists a chord decision tree $T_C$ for $(P,D_{|C})$ with expected cost
at most $2k$. 
\end{clm}

Let $T$ be any linear decision tree and let $T_C$ be the chord
decision tree guaranteed by \clmref{two-approx}.  As in the proof of
\lemref{intervals}, the search paths for a point $p\in C_i$ and a
point $q\in C_j$ with $i\neq j$ must finish at different leaves of the
chord decision tree (this uses the fact that the support of $D$ is a
convex set with $P$ in its interior).  Thus, we can assign a code to
$C_1,\ldots,C_k$ such that the length of the codeword for $C_i$ is the
length of the shortest path from the root of $T$ to a leaf
corresponding to some point in $C_i$.  Then we again obtain
\[
  \mu_{D_{|C}}(T_C)
	= \sum_{\ell\in L(T_C)} \Pr(\ell|C)\depth(\ell) 
	\ge -\sum_{i=1}^k \Pr(C_i|C)\log\Pr(C_i|C) \enspace .
\]
Then, from \clmref{two-approx}, $\mu_{D_{|C}}(T) \ge
\frac{1}{2}\mu_{D_{|C}}(T_C)$, completing the proof.
\end{proof}

Next we describe a data structure that uses \lemref{lower-bound-a} to
guide its construction.   The data structure works by creating a
sequence of successively finer approximations $A_0,\ldots,A_k$ to
$\boundary P$.  The initial approximation $A_0$ has, as its outer
boundary, the bounding box of $P$ and its inner boundary is the convex
hull of an uppermost, leftmost, bottommost and rightmost point at
which $P$ intersects its bounding box (see \figref{approx0}). 

\begin{figure}
\begin{center}\includegraphics{approx0}\end{center}
\caption{The approximation $A_0$ to $\boundary P$.}
\end{figure}

In general, the approximation $A_i$ is obtained by selecting a set
$S_i$ of $O(2^i)$ points from $\boundary P$.  The outer approximation
is a polygon containing $P$ whose edges are tangent to $P$ at the
points in $S_i$.  The inner approximation is the convex hull of $S_i$.
We ensure that successive approximations have a containment
relationship, i.e., $A_i\supseteq A_{i+1}$, by choosing our boundary
points so that $S_i\subseteq S_{i+1}$.

More specifically, we obtain the set $S_{i+1}$ from $S_i$ in the
following way:  Let $p_0,\ldots,p_{k-1}$ be the points in $S_i$ as
they occur in conterclockwise order around the boundary of $P$.  The
approximation $A_i$ thus consists of $k$ triangles
$\Delta_0,\ldots,\Delta_{k-1}$ where $\Delta_j$ has one side joining
$p_j$ and $p_{j+1}$ and a third vertex $t_j$ that lies at the
intersection of two lines tangent to $P$ at $p_j$ and
$p_{j+1}$.\footnote{Throughout this discussion, subscripts are
implicitly taken modulo $k$.}  For each triangle $\Delta_j$ for $0\le
j< k$ we add a new point to $S_{i+1}$  
by partitioning $\Delta_j$ into two triangles
$\Delta_{j,0}$ and $\Delta_{j,1}$ by a line $\ell$ through $t_j$ and such
that 
\[  
     \Pr(\Delta_{j,0}) = \Pr(\Delta_{j,1}) = \Pr(\Delta_{j})/2 \enspace .
\]
We then select a new point to add to $S_{i+1}$ at the intersection of
$\ell$ and $\boundary P$ (see \figref{split}).

\begin{figure}
\begin{center}
\begin{tabular}{cc}
\includegraphics{split-a} & \includegraphics{split-b}
\end{tabular}
\end{center}
\caption{Splitting the triangle $\Delta_j$ into two triangles
$\Delta_{j,0}$ and $\Delta_{j,1}$ with a line through $t_j$.}
\figlabel{split}
\end{figure}



\subsection{Working with Entropy}

Here are some useful properties of entropy. Consider a probability
distribution over $\{1,\ldots,n\}$ where $\Pr\{i\}=p_i$ and let
$H=\sum_{i=1}^n p_i\log (1/p_i)$ be the entropy of this distribution.
We partition the probabilities into $\ceil{\log n}$ \emph{groups}
$G_1,\ldots G_{\ceil{\log n}}$ where 
\[
	G_i = \{j : 1/2^{i-1} > p_j \ge 1/2^i \} \enspace 
\]
for $1\le i < \ceil{\log n}$ and $G_{i}=\{j: p_j < 1/2^i\}$ for
$i=\ceil{\log n}$.  Fix some constant $0<\epsilon < 1/2$ and define a
group $G_i$ as \emph{big} if $|G_i|\ge 2^{\epsilon i}$ and
\emph{small} otherwise.  Let $B$ contain the indices of all big groups
and let $S$ contain the indices of all small groups.

First we show that small groups do not contribute much to $H$ and that
most of the entropy comes from big groups:
\begin{eqnarray*}
   H & = & \sum_{j=1}^n p_j\log(1/p_j) \\
   & = & \sum_{i\in B}\sum_{j\in G_i} p_j\log (1/p_j) +
   	\sum_{i\in S}\sum_{j\in G_i} p_j\log (1/p_j) \\
    & \le & \sum_{i\in B}\sum_{j\in G_i} p_j\log (1/p_j) +
	\sum_{i\in S}\sum_{j\in G_i} p_j i \\
    & \le & \sum_{i\in B}\sum_{j\in G_i} p_j\log (1/p_j) +
	\sum_{i\in S}(2^{\epsilon i}/2^i) i \\
    & \le & \sum_{i\in B}\sum_{j\in G_i} p_j\log (1/p_j) +
	\sum_{i\in S}i/(2^{1-\epsilon})^i \\
    & \le & \sum_{i\in B}\sum_{j\in G_i} p_j\log (1/p_j) +
	\sum_{i=1}^\infty i/(2^{1-\epsilon})^i \\
    & = & \sum_{i\in B}\sum_{j\in G_i} p_j\log (1/p_j) + O(1)
\end{eqnarray*}

Next, observe that if $G_i$ is big then
\begin{equation}
    \log\left(\sum_{j\in G_i} p_j \right)  
     \ge \log\left(\sum_{j\in G_i} 1/2^{i} \right)  
      \ge \log\left(2^{\epsilon i}/2^{i} \right)  
	\ge (\epsilon -1)i 
\end{equation}


\begin{lem}
\end{lem}

\section{Conclusion}

\end{document}
