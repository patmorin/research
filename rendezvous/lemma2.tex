\documentclass[lotsofwhite]{patmorin}
\usepackage{amsthm,amsopn}
\usepackage[noend]{algorithmic}

\newcommand{\bigrand}{\textsc{BigRand}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\note}[1]{\enspace\enspace\hfill{\mbox{[#1]}}}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\bias}{bias}
\DeclareMathOperator{\reset}{reset}

\input{pat}

\title{\MakeUppercase{Randomized Rendez-Vous with Limited Memory}}
\author{Evangelos Kranakis \and
	Danny Krizanc \and
	Pat Morin}

\begin{document}
\maketitle
\begin{abstract}
We consider the problem of two identical agents with limited memory, 
starting at different locations on a ring network, who wish to 
rendez-vous.
\end{abstract}

\section{Introduction}

We consider the problem of two identical agents with limited memory,
starting at different locations on a ring network, who wish to
rendez-vous.  Does Danny or Evangelos want to write the intro?



\section{Preliminaries}

We begin with some preliminary results on random walks and probabilistic
counters.

\subsection{The Model}

We are interested in modelling the situation in which the nodes
\subsection{A Lemma on Random Walks}

Let $X_1,X_2,X_3\ldots\in\{-1,+1\}$ be independent random variables with
\[ \Pr\{X_i=-1\}=\Pr\{X_i=+1\}=1/2
\]
and let $S_i=\sum_{j=1}^i X_j$.
If we define the \emph{hitting time} $h_m$ as
\[
   h_m = \min\left\{i:|S_i|= m\right\} \enspace .
\]
The following result is well-known:
\begin{lem}[\cite{X,Y}]\lemlabel{hitting-time}
$\E[h_m] = m^2$.
\end{lem}
Applying Markov's inquality with \lemref{hitting-time} yields the
following useful corollary
\begin{cor}\corlabel{hitting-time}
$\Pr\{\max\{|S_i|:i\in\{1,\ldots,2m^2\}\} \ge m\} \ge 1/2$ .
\end{cor}

Let $Y_1,\ldots,Y_m$ be i.i.d.\
non-negative random variables with finite expectation $r=E[Y_i]$,
independent of $X_1,\ldots,X_m$, and with the
property that 
\[ \Pr\{Y_i \ge \alpha r\} \ge 1/2 \]
for some constant
$\alpha > 0$.

\begin{lem}\lemlabel{walk}
Let $X_1,\ldots,X_m$ and $Y_1,\ldots,Y_m$ be defined as above.  Then
there exists constants $\beta,\kappa>0$ such that
\[
  \Pr\left\{\left|\sum_{i=1}^m X_iY_i\right| \ge \beta r\sqrt{m}\right\} 
     \ge \kappa \enspace .
\]
\end{lem}

\begin{proof}
We will define 4 events $E_1,\ldots,E_4$, such that, if all of these
events occurs then $\left|\sum_{i=1}^m X_iY_i\right| \ge \alpha
r\sqrt{m}/2$.  Each of $E_1,\ldots,E_4$ occurs with probability at
least $1/2$ and all of these events are independent, so the
probability that they all occur is at least $1/16$.  In this way, the 
lemma will be proven for $\beta = \alpha/2$ and $\kappa=1/16$.

Consider the index set $I=\{i\in\{1,\ldots,m\}: Y_i \ge \alpha r\}$.
Let $E_1$ be the event $|I| \ge m/2$.  Then, it follows from the fact
that the median of a $\textrm{binomial}(1/2,m)$ random variable is
$\ceil{m/2}$, that $\Pr\{E_1\} \ge 1/2$.  Define $m' = \max\{i\in I\}$ and
observe that, if $E_1$ occurs, then $m'\ge |I| \ge m/2$.

Let $E_2$ be the event
\begin{equation}
	\sign\left(\sum_{i\in I} X_iY_i\right) = 
	\sign\left(\sum_{i\not\in I} X_iY_i\right) \eqlabel{sign}
\end{equation}
and observe that, again, $\Pr\{E_2\} \ge 1/2$.

Let $I_j=\{i\in I: i\le j\}$ and let $E_3$ be the event
\begin{equation}
	\max\left\{\left|\sum_{i\in I_j} X_i\right|:j\in I\right\} \ge
\sqrt{|I|/2} \enspace .
        \eqlabel{big}
\end{equation}
By \corref{hitting-time} $\Pr\{E_3\}\ge 1/2$.

Finally, let $I^-=\{i\in I:X_i=-1\}$, let $I^+=\{i\in I:X_i=+1\}$ and
assume, wlog, that $|I^+| \ge |I^-|$.  Partition $I_+$ arbitrarily
into two sets $I^+_1$ and $I^+_2$ so that $|I^+_1|=|I^-|$. Let $E_4$ be the event that
\[
     \sum_{i\in I^+_1} Y_i >  \sum_{i\in I^-} Y_i \enspace .
\]
Observe that $E_1,\ldots,E_4$ are all independent and, if they all
occur, then
\begin{eqnarray*}
  \left|\sum_{i=1}^{m'} X_iY_i\right|
   & = & \left|\sum_{i\in I} X_iY_i\right|  
           + \left|\sum_{i\not\in I} X_iY_i\right|  \\
   & \ge & \left|\sum_{i\in I} X_iY_i\right| 
           \note{because of $E_2$}\\
   & = & \left|\sum_{i\in I^+_2} Y_i
       +\sum_{i\in I^+_1} Y_i - \sum_{i\in I^-} Y_i\right| 
           \\
   & \ge & \sum_{i\in I^+_2} Y_i 
           \note{because of $E_4$} \\
   & \ge & \sum_{i\in I^+_2} \alpha r \\
   & = & |I^+_2|\alpha r \\
   & \ge & \alpha r\sqrt{m'/2} 
           \note{because of $E_3$} \\
   & \ge & \alpha r\sqrt{m}/2 
           \note{because of $E_1$} \enspace ,\\
\end{eqnarray*}
as required.
\end{proof}


\subsection{An Approximate Counter}

Consider a random variable $Y$ generated by the following
algorithm:


\noindent
\begin{minipage}{\textwidth}
$\bigrand(t)$
\begin{algorithmic}[1]
\STATE{$Y\gets C \gets 0$}
\WHILE{$C < t$}
  \STATE{$Y\gets Y + 1$}
  \IF {a coin toss comes up heads}
    \STATE{$C\gets C + 1$}
  \ELSE
    \STATE{$C\gets 0$}
  \ENDIF
\ENDWHILE
\RETURN{$Y$}
\end{algorithmic}
\end{minipage}

\begin{lem}\lemlabel{counter}
Let $Y$ be the output of Algorithm $\bigrand(t)$.  Then 
\begin{enumerate}
\item $\E[Y]=2^t(2-1/2^{t-1})$ and
\item $\Pr\{Y \ge \E[Y] / 2\} \ge 1/2$.
\end{enumerate}
\end{lem}

\begin{proof}
To compute the expected value of $Y$ we observe that the algorithm
begins by tossing a sequence of $i-1$ heads and then either
(a)~returning to the initial state if the $i$th coin toss is a tail
or (b)~terminating if $i=2^t$.  The first case occurs with probability
$1/2^i$ and the second case occurs with probability $1/2^t$.
In this way, we obtain the equation
\[
   \E[Y] = \sum_{i=1}^{t} \frac{1}{2^i}\left(i + \E[Y]\right) +
\frac{t}{2^t} \enspace .
\]
Rearranging terms and multiplying by $2^{t}$, we obtain
\[
   \E[Y] = 2^t(2-1/2^{t-1}) \enspace .  
\]

To prove the second part of the lemma, consider the number of times
the counter $C$ is reset to $0$ in Line~7 of the algorithm.  This
number is a geometric($1/2^t$) random variable and its expected value
is therefore $2^t \ge \E[Y]/2$.  Since the number of times Line~7
executes is a lower bound on the number of times the value of $Y$ is
incremented (Line~3), this completes the proof.
\end{proof}

\section{The Rendez-Vous Algorithm}

Consider the following algorithm used by an agent to make a random
walk on a ring.  The agent repeatedly performs the following steps:
(1)~toss a coin to determine a direction
$d\in\{\mbox{clockwise},\mbox{counterclockwise}\}$ then (2)~run
algorithm $\bigrand(2^{k-1})$ replacing each increment of the variable
$Y$ with a step in direction $d$. By using one bit to remember the
direction $d$ and $k-1$ bits to keep track of the counter $C$ in the
$\bigrand$ algorithm, this algorithm can be implemented by an agent
having only $k$ bits of memory.

We call $m$ iterations of the above algorithm a \emph{round}.
Together, \lemref{walk} and \lemref{counter} imply that, during a
round, with probability at least $\kappa$, an agent will travel a
distance of at least $\beta 2^{2^{k-1}}\sqrt{m}$ from their original
location.  Set
\[
   m= \frac{n^2}{\beta^2 2^{2^k}}  
\]
and consider what happens when two agents $A$ and $B$ both execute
this rendez-vous algorithm.  During the first round of $A$'s
executtion, with probability at least $\kappa$, agent $A$ will have
visited agent $B$'s starting location. Furthermore, with probability
at least $1/2$ will not have moved away from $A$, so the paths of
agents $A$ and $B$ will cross, and a rendez-vous will occur, with
probability at least $\kappa/2$.

By \lemref{counter}, the expected number of steps taken for $A$ to
execute the $i$th round is at most
\[
    E[M_i] \le m 2^{2^{k-1}} = \frac{n^2}{\beta^22^{2^{k-1}}}
\]
The variables $M_1,M_2,\cdots$ are independent and the algorithm
terminates when $A$ and $B$ rendez-vous.  If we define $T$ as
the round in which agents $A$ and $B$ rendez-vous then the time to
rendez-vous is bounded by
rendez-vous time is b
\[
   \sum_{i=1}^T M_i \enspace .
\]
Note that the event $T=j$ is independent of $M_{j+1},M_{j+2},\ldots$
so $T$ is a \emph{stopping time} \cite{mXX} for the sequence
$M_1,M_2,\ldots$ so, by Wald's equation
\[
   E\left[\sum_{i=1}^T M_i\right] = E[T]\cdot E[M_1] \le
\frac{2}{\kappa}\cdot \frac{n^2}{\beta^22^{2^{k-1}}}.
\]
This completes the proof of our first theorem.
\begin{thm}
There exists a rendez-vous algorithm in which each agent uses at most
$k$ bits of memory and whose expected rendez-vous time is
$O(\min\{n^2/2^{2^{k-1}},2^{2^{k-1}}\})$.
\end{thm}



\comment{






 the paths of agents $A$ and
$B$ cross and they rendez-vous within the first $m$ iterations of $A$'s
algorithm.

By \lemref{counter}, 
the expected number of steps taken by $A$ during the first $m$
iterations of $A$'s algorithm is less than 
\[
   \mu = 2^{2^{k-1}} m = \frac{n^2}{\beta^2 2^{2^{k-1}}} \enspace ,
\]
and an application of Chernoff's bounds shows that the probability that
the number of steps taken by $A$ is greater than $(1+\epsilon)\mu$ is
at most $\exp(-\epsilon^2 m/3)$.  Thus, with constant probability,
agents $A$ and $B$ successfully rendez-vous during the first $m$
iterations of $A$'s algorithm after $O(n^2/2^{2^{k-1}})$ steps.  If
this fails, then the same statement is true about the next $m$
iterations of $A$'s algorithm, and so on.  The resulting geometric
series gives an upper bound on the expected time it takes for $A$ and
$B$ to rendez-vous:
}




\section{The Lower Bound}

Next we show that the algorithm in \secref{algorithm} is, in some
sense, optimal.

Let $t=2^k$ and observe that, any algorithm that uses $k$ bits of
memory is represented as a finite state machine with $t$ states.  Each
state of this state machine has two outgoing edges, representing the
two possible results of a coin toss and each edge $e$ is labelled with
a value $\ell(e)\in\{+1,-1, 0\}$ depending on whether the agent go
clockwise, counterclockwise, or remains at the current location,
respectively.

We say that an algorithm is \emph{well-behaved} if the directed graph
of the state machine has only one strongly connected component that
contains all nodes.  We are particularly interested in intervals
between consecutive visits to the start state, which we will call
\emph{rounds}.  For an edge $e$ of the
state machine, let $f(e)$ be the expected number of times the edge $e$
is traversed during a round.  The
\emph{reset time} of algorithm $\A$ is then defined as
\[
   \reset(\A) = \sum_e f(e) \enspace ,
\]
which is the expected length of a round.  The following lemma says
that $\mathcal{A}$ resets itself fairly frequently:
\begin{lem}
For a well-behaved algorithm $\A$, $\reset(\A)\le 2^t$.
\end{lem}
\begin{proof}
Easy.
\end{proof}

The \emph{bias} of a well-behaved algorithm $\A$ is defined as
\[
    \bias(\A) = \sum_{e} f(e)\cdot\ell(e)
\]
is the expected sum of the edge labels encoutered during a round.

Algorithms with a non-zero bias are somewhat more difficult to study.
However, observe that, for any algorithm $\mathcal{A}$ we can replace
every edge label $\ell(e)$ with the value $\ell(e)-x$ for any real
number $x$ and obtain an equivalent algorithm in the sense that, if
two agents $A$ and $B$ execute the modified algorithm following the
same sequence of edges then $A$ and $B$ will rendez-vous after exactly
the same number of steps.
In particular, if we replace each edge label $\ell(e)$ with the value
\[
   \ell'(e) = \ell(e) - \frac{\bias(\A)}{\reset(\A)}
\]
then we obtain an algorithm $\A'$ with $\bias(\A')=0$.  Furthermore, since
$|\bias(\A)|\le\reset(\A)$, every edge label $\ell'(e)$ has
$-2 \le \ell'(e)\le 2$.

\noindent
\framebox{
\begin{minipage}{\textwidth}
To get perfectly matching upper and lower bounds we need to prove
this:
\begin{lem}
For a well-behaved algorithm $\A'$ with $\bias(A')=0$,
$\E[|\sum_{e} f(e)\cdot\ell(e)|] \le 2^{t/2}$.
\end{lem}
\end{minipage}
}

If we study the location of $\A'$ after $m$ rounds, we find that this
is given by 
$\sum_{i=1}^m Y_i$
where the $Y_i$ are independent, $\E[Y_i] = \bias(\A') = 0$ and $\E[|Y_i|]\le
2\reset(\A') \le
2^{t+1}$. In particular, we have
\begin{eqnarray*}
  \E\left[\left(\sum_{i=1}^m Y_i\right)^2\right]
   & = & \E\left[\sum_{i=1}^m Y_i^2 
          + 2\sum_{i=1}^m\sum_{j=i+1}^m Y_iY_j\right] \\
   & = & \sum_{i=1}^m \E[Y_i^2] \\
   & \le & 2cm (2^t)^2
\end{eqnarray*}
for some constant $c$.
FIXME: Justify the last inequality.  By applying Jensen's inequality
to the above, we obtain
\[
     \E\left[\left|\sum_{i=1}^m Y_i\right|\right] \le \sqrt{2cm}2^t \enspace .
\]
In particular, for the value 
\[
    m=\frac{1}{16}\frac{n^2}{2c2^{2t}}
\]
we have
\[
     \E\left[\left|\sum_{i=1}^m Y_i\right|\right] \le n/4 \enspace ,
\]
and, using Markov's Inequality,
\[
     \Pr\left\{\left|\sum_{i=1}^m Y_i\right| \ge n/2\right\} \le 1/2 \enspace .
\]

To summarize, with probability at least $1/4$, neither agent $A$ nor
$B$ 


\begin{lem}

\end{lem}

\end{document}

